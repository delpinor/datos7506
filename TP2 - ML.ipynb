{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import emoji\n",
    "from num2words import num2words\n",
    "  \n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, Conv1D\n",
    "from keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GRU, GlobalAveragePooling1D,GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown,words\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.decomposition import PCA\n",
    "import tokenization\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import KeyedVectors\n",
    "import eli5\n",
    "import bert\n",
    "import re, string\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "pd.options.display.float_format = '{:20,.10f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CODE_LOCATION = {'al': 'alabama',\n",
    "  'ak': 'alaska',\n",
    "  'az': 'arizona',\n",
    "  'ar': 'arkansas',\n",
    "  'ca': 'california',\n",
    "  'co': 'colorado',\n",
    "  'ct': 'connecticut',\n",
    "  'de': 'delaware',\n",
    "  'dc': 'district of columbia',\n",
    "  'fl': 'florida',\n",
    "  'ga': 'georgia',\n",
    "  'hi': 'hawaii',\n",
    "  'id': 'idaho',\n",
    "  'il': 'illinois',\n",
    "  'in': 'indiana',\n",
    "  'ia': 'iowa',\n",
    "  'ks': 'kansas',\n",
    "  'ky': 'kentucky',\n",
    "  'la': 'louisiana',\n",
    "  'me': 'maine',\n",
    "  'md': 'maryland',\n",
    "  'ma': 'massachusetts',\n",
    "  'mi': 'michigan',\n",
    "  'mn': 'minnesota',\n",
    "  'ms': 'mississippi',\n",
    "  'mo': 'missouri',\n",
    "  'mt': 'montana',\n",
    "  'ne': 'nebraska',\n",
    "  'nv': 'nevada',\n",
    "  'nh': 'new hampshire',\n",
    "  'nj': 'new jersey',\n",
    "  'nm': 'new mexico',\n",
    "  'ny': 'new york',\n",
    "  'nc': 'north carolina',\n",
    "  'nd': 'north dakota',\n",
    "  'oh': 'ohio',\n",
    "  'ok': 'oklahoma',\n",
    "  'or': 'oregon',\n",
    "  'pa': 'pennsylvania',\n",
    "  'ri': 'rhode island',\n",
    "  'sc': 'south carolina',\n",
    "  'sd': 'south dakota',\n",
    "  'tn': 'tennessee',\n",
    "  'tx': 'texas',\n",
    "  'ut': 'utah',\n",
    "  'vt': 'vermont',\n",
    "  'va': 'virginia',\n",
    "  'wa': 'washington',\n",
    "  'wv': 'west virginia',\n",
    "  'wi': 'wisconsin',\n",
    "  'wy': 'wyoming',\n",
    "  'as': 'american samoa',\n",
    "  'gu': 'guam',\n",
    "  'mh': 'marshall islands',\n",
    "  'fm': 'micronesia',\n",
    "  'mp': 'northern marianas',\n",
    "  'pw': 'palau',\n",
    "  'u.s.a': 'united states',\n",
    "  'usa': 'united states',\n",
    "  '304': 'west virginia',\n",
    "  'd.c': 'district of columbia',\n",
    "  'd.c.': 'district of columbia',                 \n",
    "  'us': 'united states',\n",
    "  'ny': 'new york',\n",
    "  'nyc': 'new york',\n",
    "  'uk': 'united kingdom',\n",
    "  'u.k': 'united kingdom',\n",
    "  'u.k.': 'united kingdom',\n",
    "  'bc': 'british columbia',\n",
    "  'ab': 'alberta',\n",
    "  'vi': 'virgin islands'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Imprimir metricas de las predicciones\n",
    "def mostrar_metricas(y_test, y_pred):\n",
    "    print('Reporte de clasificación: \\n', classification_report(y_test, y_pred))\n",
    "    print('Matriz de confusión: \\n',confusion_matrix (y_test, y_pred))\n",
    "    print('ROC: \\n', metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    # Obtener y reformar la matriz de datos de \n",
    "    matrix = confusion_matrix (y_test, y_pred) \n",
    "    matrix = matrix.astype ('float') / matrix.sum (axis = 1) [:, np.newaxis] \n",
    "\n",
    "    # Build the plot\n",
    "    plt.figure()\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('Matriz de confusión')\n",
    "    plt.show()\n",
    "    \n",
    "def grafico_pie(df, titulo, valores, etiquetas):\n",
    "    fig = px.pie(df, values=valores, names=etiquetas)\n",
    "    fig.update_layout(title_text=titulo,\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "    \n",
    "def grafico_distr(df, columna, titulo, xtitulo, ytitulo):\n",
    "    x1 = df.loc[df['target'] == 1][columna]\n",
    "    x2 = df.loc[df['target'] == 0][columna]\n",
    "    group_labels = ['Verdadero', 'Falso']\n",
    "    colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n",
    "    fig = ff.create_distplot([x1, x2], group_labels,colors=colors)\n",
    "    fig.update_layout(title_text=titulo,\n",
    "                      xaxis_title=xtitulo,\n",
    "                      yaxis_title=ytitulo,\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def resultados(pred, test_df):\n",
    "    res_df=pd.DataFrame(test_df['id'])\n",
    "    res_df['target']=pred\n",
    "    res_df.to_csv('data/submission.csv', index=False)\n",
    "    \n",
    "def dict_vocabulario(x):\n",
    "    tweets = x.apply(lambda s: s.split()).values\n",
    "    vocab = {}\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1 #Si ya existe en el diccionario\n",
    "            except KeyError:\n",
    "                vocab[word] = 1  #Si no existe\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def porcentaje_cobertura(x, embeddings):\n",
    "    vocab = dict_vocabulario(x)\n",
    "    cubiertos = {}\n",
    "    no_cubiertos = {}\n",
    "    cant_cubiertos = 0\n",
    "    cant_no_cubiertos = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            cubiertos[word] = embeddings[word]\n",
    "            cant_cubiertos += vocab[word]\n",
    "        except:\n",
    "            no_cubiertos[word] = vocab[word]\n",
    "            cant_no_cubiertos += vocab[word]\n",
    "\n",
    "    palabras_cubiertas_pct = len(cubiertos) / len(vocab)\n",
    "    texto_cubierto_pct = (cant_cubiertos/ (cant_cubiertos + cant_no_cubiertos))\n",
    "    return no_cubiertos, palabras_cubiertas_pct, texto_cubierto_pct\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "#punctuation = list(string.punctuation)\n",
    "punctuation = '!\"$%&\\()*+,-/:;<=>?[\\\\]“”^_`#{|}~’'\n",
    "\n",
    "\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "\n",
    "def unir_texto(text):\n",
    "    return ( ' '.join(text))\n",
    "\n",
    "def eliminar_palabras_con(text, con):\n",
    "    palabras = []\n",
    "    for word in text.split():\n",
    "        if con not in word:\n",
    "            palabras.append(word)\n",
    "    return unir_texto(palabras)\n",
    "\n",
    "\n",
    "# Tipo de palabra\n",
    "#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Lematizar palabras\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Elimino stops words \n",
    "def eliminar_sw(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip() not in stops: \n",
    "            #pos = pos_tag([i.strip()])\n",
    "            #word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "            final_text.append(i.strip())\n",
    "    return unir_texto(final_text)\n",
    "\n",
    "def lematizar(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        pos = pos_tag([i.strip()])\n",
    "        word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "        final_text.append(word)\n",
    "    return unir_texto(final_text)\n",
    "\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "def eliminar_nums(text):\n",
    "    cadena = []\n",
    "    for x in text:\n",
    "         if x not in string.digits:\n",
    "                cadena.append(x)\n",
    "    return  ''.join(cadena)\n",
    "\n",
    "def eliminar_punct(text):\n",
    "    cadena = []\n",
    "    for x in text:\n",
    "         if x not in punctuation:\n",
    "                cadena.append(x)\n",
    "    return  ''.join(cadena)\n",
    "        \n",
    "def eliminar_espacios_multiples(text):\n",
    "    text = re.sub(r\"\\s+\",\" \", text, flags = re.I)\n",
    "    return text.strip()\n",
    "\n",
    "def eliminar_palabras_con_numeros(text):\n",
    "    return re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "\n",
    "def convertir_location_code(text):\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if word in CODE_LOCATION:\n",
    "            word = CODE_LOCATION[word]\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def reemplazar_chars_primer(text):\n",
    "    text = text.replace(' yobe ',' yobe niger ') \n",
    "    text = text.replace('mediterran...','mediterranean') \n",
    "    text = text.replace('&amp',' and ')\n",
    "    text = text.replace('&gt;&gt',' see ')\n",
    "    text = text.replace('&gt',' ')\n",
    "    text = text.replace('&lt',' ')\n",
    "    text = text.replace(' mph ',' miles per hour ')\n",
    "    text = text.replace('rcmp','royal canadian mounted police')\n",
    "    text = text.replace(' tch','trans canada highway')\n",
    "    text = text.replace('lmfao','laughing my fucking ass off')\n",
    "    text = text.replace('pkwy','park way')\n",
    "    text = text.replace('hwy','high way')\n",
    "    text = text.replace(' gov ',' government ')\n",
    "    text = text.replace('govt','government')\n",
    "    text = text.replace('gov\\'t','government')\n",
    "    text = text.replace('friend50','friend')\n",
    "    text = text.replace('offr','officer')\n",
    "    text = text.replace('pkk','kurdistan workers party')\n",
    "    text = text.replace('v deo','video')\n",
    "    text = text.replace(' rly','really')\n",
    "    text = text.replace('i\\'m','i am')\n",
    "    text = text.replace(' dont ',' do not ')\n",
    "    text = text.replace('don\\'t','do not')\n",
    "    text = text.replace(' don t','do not')\n",
    "    \n",
    "\n",
    "    \n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('i\\'ve','i have')\n",
    "    text = text.replace('there\\'s ','there is ')\n",
    "    text = text.replace('i\\'ll ','i will ')\n",
    "    text = text.replace('doesn\\'t','does not')\n",
    "    text = text.replace('i\\'d','i had')\n",
    "    text = text.replace('didn\\'t','did not')\n",
    "    text = text.replace('we\\'re','we are')\n",
    "    text = text.replace('they\\'re','they are')\n",
    "    text = text.replace('isn\\'t','is not')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('let\\'s','let us')\n",
    "    text = text.replace('ain\\'t','am not')\n",
    "    text = text.replace('that\\'s','that is')\n",
    "    text = text.replace('won\\'t','will not')\n",
    "    text = text.replace('wasn\\'t','was not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('haven\\'t','have not')\n",
    "    text = text.replace('here s','here is')\n",
    "    text = text.replace('aren\\'t','are not')\n",
    "    text = text.replace('you\\'ll','you will')\n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('shouldn\\'t','should not')\n",
    "    text = text.replace('should\\'ve','should have')\n",
    "\n",
    "    text = text.replace('here\\'s','here is it')\n",
    "    text = text.replace('she\\'s','she is')\n",
    "    text = text.replace('we\\'ve','we have')\n",
    "    text = text.replace('you\\'ve','you have')\n",
    "    text = text.replace('who\\'s','who is')\n",
    "    text = text.replace('y\\'all','you all')\n",
    "    text = text.replace('wouldn\\'t','would not')\n",
    "    text = text.replace('they\\'ve','they have')\n",
    "    text = text.replace('weren\\'t','were not')\n",
    "    text = text.replace('would\\'ve','would have')\n",
    "    text = text.replace('you\\'d','you would')\n",
    "    text = text.replace('they\\'ll','they will')\n",
    "    text = text.replace('we\\'d','we would')\n",
    "    text = text.replace('how\\'re','how are')\n",
    "    text = text.replace('they\\'d','they would')\n",
    "    text = text.replace('we\\'ll','we shall')\n",
    "    text = text.replace('it\\'ll','it will')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('can\\'t','can not')\n",
    "    text = text.replace('can t ','can not ')\n",
    "    text = text.replace(' he\\'s',' he is')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace(' u ',' you ')\n",
    " \n",
    "\n",
    "    text = text.replace('1st','first')\n",
    "    text = text.replace('2nd','second')\n",
    "    text = text.replace('3rd','third')\n",
    "    text = text.replace('4th','fourth')\n",
    "    text = text.replace('5th','fifth')\n",
    "    text = text.replace('6th','sixth')\n",
    "    text = text.replace('7th','seventh')\n",
    "    text = text.replace('8th','eighth')\n",
    "    text = text.replace('9th','ninth')\n",
    "    text = text.replace('10th','tenth')\n",
    "\n",
    "    text = text.replace('u.s.','united states')\n",
    "    text = text.replace('u.s','united states')\n",
    "    text = text.replace('d.c.','district of columbia')\n",
    "    text = text.replace('b.c.','british columbia')\n",
    "\n",
    "    text = text.replace('bioterror','bio teror')\n",
    "    text = text.replace('mh370','malaysia airlines flight 370') #Ver de eliminar\n",
    "    text = text.replace('\\'the','the')\n",
    "    text = text.replace('legionnaires\\'','legionnaires')\n",
    "   \n",
    "    text = text.replace('didnt','did not')\n",
    "    text = text.replace('\\'i', 'i')\n",
    "    text = text.replace('confirmed\\'', 'confirmed')\n",
    "    text = text.replace('\\'conclusively', 'conclusively')\n",
    "    text = text.replace('\\'we', 'we')\n",
    "    text = text.replace('\\'it', 'it')\n",
    "    text = text.replace('bestnaijamade', 'best naija made')\n",
    "    text = text.replace('water\\'', 'water')\n",
    "    text = text.replace('china\\'s', 'china')\n",
    "    text = text.replace('neighbour\\'s', 'neighbour')\n",
    "    text = text.replace('crematoria\\'', 'crematoria')\n",
    "    text = text.replace('officeroad', 'office road')\n",
    "    text = text.replace('wwi', 'world war i')\n",
    "    text = text.replace('wwii', 'world war ii')\n",
    "    text = text.replace('ww1', 'world war i')\n",
    "    text = text.replace('ww2', 'world war ii')\n",
    "    text = text.replace('2k15', '2015')\n",
    "    text = text.replace(' www ', ' world war ')\n",
    "    text = text.replace('usgs', 'united states geological survey')\n",
    "    text = text.replace('km', ' kilometers')\n",
    "    text = text.replace('clutch asf','clutch as fuck')  \n",
    "    text = text.replace('\\'if', 'if')\n",
    "    text = text.replace('\\'save', 'save')\n",
    "    text = text.replace('100%', 'one hundred percent')\n",
    "    text = text.replace('iger\\'s', 'iger')\n",
    "    text = text.replace('charity.\\'', 'charity')\n",
    "    text = text.replace('\\'suicide', 'suicide')\n",
    "    text = text.replace('\\'there', 'there')\n",
    "    text = text.replace('typhoon-devastated', 'typhoon devastated')\n",
    "    text = text.replace('11-year-old', 'eleven year old')\n",
    "    text = text.replace('sensor-senso', 'sensor')\n",
    "    text = text.replace('c-130', 'lockheed hercules')\n",
    "    text = text.replace('self-image?', 'self image')\n",
    "    text = text.replace('i-77', 'interstate 77')       #Ver de eliminar los numeros\n",
    "    text = text.replace('3-d', 'three dimensions')\n",
    "    text = text.replace('cleared:incident', 'cleared incident')\n",
    "    text = text.replace('tomorrow\\'s', 'tomorrow')\n",
    "    text = text.replace('h370', 'malaysia airlines flight 370') #Ver de eliminar los numeros\n",
    "    text = text.replace('chief\\'s', 'chief')\n",
    "    text = text.replace('\\'when', 'when')\n",
    "    text = text.replace('soudelor\\'s', 'soudelor')\n",
    "    text = text.replace('jupiter\\'s', 'jupiter')\n",
    "    text = text.replace('w/o', 'without')\n",
    "    text = text.replace('hostageand2', 'hostage and')\n",
    "    text = text.replace('women\\'s', 'women')\n",
    "    text = text.replace('california\\'s', 'california')\n",
    "    text = text.replace('1fourth', 'fourth')\n",
    "    text = text.replace('150-foot', 'foot')\n",
    "    text = text.replace('someone\\'s', 'someone')\n",
    "    text = text.replace('harm/kid', 'harm kid')\n",
    "    text = text.replace('non compliant', 'non compliant')\n",
    "    text = text.replace('demonstratio...', 'demonstration')\n",
    "    text = text.replace('\\'people', 'people')\n",
    "    text = text.replace('disaster\\'', 'disaster')\n",
    "    text = text.replace('meat-loving', 'meat loving')\n",
    "    text = text.replace('stand-user?', 'stand user')\n",
    "    text = text.replace('injury:i-495', 'injury interstate 495')\n",
    "    text = text.replace('collision-no', 'collision no')\n",
    "    text = text.replace('explosion-proof', 'explosion proof')\n",
    "    text = text.replace('triple-digit', 'triple digit')\n",
    "    text = text.replace('lulgzimbestpicts', 'lul g zim best picts')\n",
    "    text = text.replace('0-day', 'zero day')\n",
    "    text = text.replace('connector-connecto', 'connector connecto')\n",
    "    text = text.replace('2-united', 'two united')\n",
    "    text = text.replace('collision-1141', 'collision')\n",
    "    text = text.replace('h370.', 'malaysia airlines flight 370')\n",
    "    text = text.replace('hatchet-wielding', 'hatchet wielding')\n",
    "    text = text.replace('3-alarm', 'three alarm')\n",
    "    text = text.replace('i-65', 'interstate 65')\n",
    "    text = text.replace('18-wheeler', '18 wheeler')    \n",
    "    text = text.replace('six-meter', 'six meter')\n",
    "    text = text.replace('soloquiero', 'solo quiero')\n",
    "    text = text.replace('everyday', 'every day')\n",
    "    text = text.replace('gbbo', 'the great british bake off')\n",
    "    text = text.replace('misfortunebut', 'misfortune but')\n",
    "    text = text.replace('53inch 300w', '')\n",
    "    text = text.replace('4X4', '')\n",
    "    text = text.replace('16yr','sixteen years')\n",
    "    text = text.replace('fvck','fuck')\n",
    "    text = text.replace('p.m.','post meridiem')\n",
    "    text = text.replace(' pm ',' post meridiem ')\n",
    "    text = text.replace('a.m.','ante meridiem')\n",
    "    text = text.replace(' a.m ',' ante meridiem ')\n",
    "    text = text.replace(' rn ',' right now ')\n",
    "    text = text.replace(' da ',' the ')\n",
    "    text = text.replace(' min ',' minutes ')\n",
    "    text = text.replace('\\'s', '')\n",
    "    text = text.replace('10:00','ten hours')\n",
    "    text = text.replace(' u ',' you ')\n",
    "    text = text.replace('prebreak','pre break')\n",
    "    text = text.replace('soudelor','hanna')\n",
    "    text = text.replace('bayelsa','bayelsa state nigerian')\n",
    "    text = text.replace('marians','CNMI')\n",
    "    text = text.replace('udhampur','indian union territory of jammu and kashmir')\n",
    "    text = text.replace('utc2015','utc 2015')\n",
    "    text = text.replace('time2015','time 2015')\n",
    "    text = text.replace('utc2015','utc 2015')\n",
    "    text = text.replace('time2015','time 2015')\n",
    "    text = text.replace('20150805','2015 08 05')\n",
    "    text = text.replace('8615','2015 08 06')\n",
    "    text = text.replace('trfc','traffic')\n",
    "    text = text.replace('beyhive','beyonce fan')\n",
    "    text = text.replace('o784','')\n",
    "    text = text.replace('abstorm','absolute storm')\n",
    "    text = text.replace('animalrescue','animal rescue')\n",
    "    text = text.replace('icemoon','ice moon')\n",
    "    text = text.replace('runion','reunion')\n",
    "    text = text.replace('hiroshimanagasaki','hiroshima nagasaki')\n",
    "    text = text.replace('ww1 2 ','world war ')\n",
    "    text = text.replace('measuresarrestpastornganga','measures arrest pastor nganga')\n",
    "    text = text.replace('warmbodies','warm bodies')\n",
    "    text = text.replace('full re \\'','full read')\n",
    "    text = text.replace('full rea \\'','full read')\n",
    "    text = text.replace('linkury','linkury malware')\n",
    "    text = text.replace('neileastwood77','neil eastwood')\n",
    "    text = text.replace('mhtw4fnet','')\n",
    "    text = text.replace('kisii','kisii kenya')\n",
    "    text = text.replace('yazidishingalgenocide','yazidi shingal genocide')\n",
    "    text = text.replace('sinjar','sinjar irak')\n",
    "    text = text.replace('okwx','oklahoma weather')\n",
    "    text = text.replace('rohingya','rohingya myanmar')    \n",
    "    text = text.replace('chicagoarea','chicago area')\n",
    "    text = text.replace('socialnews','social news')\n",
    "    text = text.replace('wheavenly','heavenly') \n",
    "    text = text.replace(' www ',' ')\n",
    "    text = text.replace('it\\'d','it would')\n",
    "    text = text.replace('buildings\\'we','building\\'we')\n",
    "    text = text.replace('couldn\\'t','could not')\n",
    "    text = text.replace('r\\'lyeh','fictional lost city')\n",
    "    text = text.replace('could\\'ve','could have')\n",
    "    text = text.replace('don\\'t','do not')\n",
    "    text = text.replace('that\\'d','that would')\n",
    "    text = text.replace('u\\'d','you would')\n",
    "    text = text.replace('he\\'d','he had')\n",
    "    text = text.replace('he\\'ll','he will')\n",
    "    text = text.replace('o\\'clock','hours')\n",
    "    text = text.replace('cont\\'d','continued')\n",
    "    text = text.replace('sittwe','sittwe myanmar')\n",
    "    text = text.replace('twia','texas windstorm insurance association')\n",
    "    text = text.replace('mansehra','mansehra pakistan')\n",
    "    text = text.replace('97georgia','georgia state route 97')\n",
    "    text = text.replace('bb17','big brother 17') \n",
    "    text = text.replace('beforeitsnews','before it is news')\n",
    "    text = text.replace('myfitnesspa','my fitness')\n",
    "    text = text.replace('p b ban temporary300','pbban temporary')\n",
    "    text = text.replace(' xp ',' experience ')\n",
    "    text = text.replace('crosssectarian','cross sectarian')\n",
    "    text = text.replace(' r amag','ra magazine')\n",
    "    text = text.replace('shantae','shantae game')\n",
    "    text = text.replace('tubestrike','tube strike')\n",
    "    text = text.replace('humanconsumption','human consumption')\n",
    "    text = text.replace('abbswinston','abbs winston')\n",
    "    text = text.replace('gunsense','gun sense')\n",
    "    text = text.replace('godslove','gods love')\n",
    "    text = text.replace('worldnews','world news')\n",
    "    text = text.replace('abuseddesolateandlost','abused desolate and lost')\n",
    "    text = text.replace('f o x', 'fox')\n",
    "    text = text.replace('rock\\'n', 'rock and')\n",
    "    text = text.replace('w/', 'with')\n",
    "    text = text.replace('...', ' ') #En este orden\n",
    "    text = text.replace('..', ' ')  #En este orden\n",
    "    text = text.replace('c afire', 'california fire')  \n",
    "    text = text.replace(' st. ', ' saint ')\n",
    "    text = text.replace('nowplaying', 'now playing')\n",
    "    text = text.replace('listenlive', 'listen live')\n",
    "    text = text.replace('sydtraffic', 'sydney traffic')\n",
    "    text = text.replace('trafficnetwork', 'traffic network')\n",
    "    text = text.replace(': ', ' ')\n",
    "    return text\n",
    "\n",
    "def eliminar_palabras_especiales(text):\n",
    "    text = eliminar_palabras_con(text, 'http')\n",
    "    text = eliminar_palabras_con(text, '@')\n",
    "    \n",
    " \n",
    "    #text = eliminar_palabras_con(text, '#')\n",
    "    \n",
    "    '''\n",
    "    text = eliminar_palabras_con(text, 'mmm')\n",
    "    text = eliminar_palabras_con(text, 'mhm')\n",
    "    text = eliminar_palabras_con(text, 'ww')\n",
    "    text = eliminar_palabras_con(text, 'jsj')\n",
    "    text = eliminar_palabras_con(text, 'haha')\n",
    "    text = eliminar_palabras_con(text, 'hah')\n",
    "    text = eliminar_palabras_con(text, 'ooh')\n",
    "    text = eliminar_palabras_con(text, 'hhh')\n",
    "    text = eliminar_palabras_con(text, 'ahh')   \n",
    "    '''\n",
    "    return text\n",
    "def limpiar_char_especiales(text):\n",
    "    text = re.sub(r'[^\\/\\&\\.\\-\\'\\sA-Za-z0-9]', '', text) #Las que se ingnoran despues se limpian\n",
    "    return text\n",
    "\n",
    "def limpiar_comillas_simples(text):\n",
    "    cadena = []\n",
    "    end_char = '\\''\n",
    "    for word in text.split():\n",
    "        if word.startswith(end_char):\n",
    "            word = word[len(end_char):]\n",
    "        if word.endswith(end_char):\n",
    "            word = word[:len(word) - len(end_char)]\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "\n",
    "def eliminar_cortas(text):\n",
    "    excepto = ['i', 'a', '1', '2', '3', '4', '5',\n",
    "              '6', '7', '8', '9', '0']\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if (len(word) == 1) & (word not in excepto):\n",
    "            word = ''\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def numero_a_texto(text):\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            word = num2words(word)\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def split_hashtag(text):\n",
    "    cadena = []\n",
    "    for w in text.split():\n",
    "        p = w.replace('#', '')\n",
    "        cond = ((not p.isupper()) & (not p.islower()))\n",
    "        if (w.startswith('#') & cond):\n",
    "            p = ' '.join(re.findall('[A-Z][^A-Z]*', p))\n",
    "        cadena.append(p)\n",
    "    return ' '.join(cadena)\n",
    "    \n",
    "def reemplazo_inicial(text):\n",
    "    text = text.replace('%20', ' ')\n",
    "    text = text.replace('Rly', 'really')\n",
    "    text = text.replace('RT ', 'rt ')\n",
    "    text = text.replace(' RT ', ' rt ')\n",
    "    text = text.replace('Û', '\\'')\n",
    "    text = text.replace('UTC', ' utc ')\n",
    "    return text\n",
    "\n",
    " #Algunos tweets como por ejmplo los que contienen la palabra funtenna que no está vectorizado\n",
    " #se repiten con tendencia a ser falsos. 19/5 por ahí conviene entrenarlos con target 0. Ver otros...\n",
    "def cambiar_valor_target(df):\n",
    "    #Tweet casi siempre falso 14/19\n",
    "    cond = (df.text_clean == 'hot funtenna hijacking computers to send data as sound waves black hat 2015 pre break best')\n",
    "    df.loc[cond,'target'] = 0\n",
    "    \n",
    "def formato_inicial(df):\n",
    "    df['location'].fillna(value=' ', inplace=True)\n",
    "    df['keyword'].fillna(value=' ', inplace=True)\n",
    "    \n",
    "    #Minúsculas\n",
    "    df['keyword_clean'] = df.keyword.str.replace('%20', ' ')\n",
    "    df['location_clean'] = df.location.str.replace('%20', ' ')\n",
    "    df['text_clean'] = df.text.apply(reemplazo_inicial)\n",
    "    \n",
    "    df['keyword'] = df.keyword_clean.str.lower()\n",
    "    #Antes de poner en minúsculas al texto hago split de los hashtags\n",
    "    df['text_clean'] = df.text_clean.apply(split_hashtag)   \n",
    "    df['text_clean'] = df.text_clean.str.lower()\n",
    "    df['location_clean'] = df.location_clean.str.lower()\n",
    "\n",
    "    \n",
    "    #Sobre location\n",
    "    df['location_clean'] = df.location_clean.str.lower()\n",
    "    df['location_clean'] = df.location_clean.apply(convertir_location_code)\n",
    "    df['location_clean'] = df.location_clean.apply(limpiar_char_especiales)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_punct)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_palabras_con_numeros)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_cortas)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_palabras_especiales)\n",
    "\n",
    "\n",
    "    \n",
    "    #Sobre text_clean\n",
    "    df['text_clean'] = df.text_clean.apply(eliminar_palabras_especiales) #Links, @user, etc\n",
    "    df['text_clean'] = df.text_clean.apply(limpiar_char_especiales)\n",
    "    df['text_clean'] = df.text_clean.apply(reemplazar_chars_primer) #i'm por i am, etc.\n",
    "    df['text_clean'] = df.text_clean.apply(limpiar_comillas_simples) #Al inicio y final de palabras\n",
    "    \n",
    "    #Estos caracteres no se eliminaron con limpiar_char_especiales ya que requerían trantamiento.\n",
    "    df['text_clean'] = df.text_clean.str.replace('\\'', ' ') #Algunas palabras están separadas por\n",
    "    df['text_clean'] = df.text_clean.str.replace('-', ' ') #Algunas palabras están separadas por: war-zone \n",
    "    df['text_clean'] = df.text_clean.str.replace('/', ' ') #Algunas palabras están separadas por: days/weeks \n",
    "    df['text_clean'] = df.text_clean.str.replace('.', '') #Puntos al final de las palabras\n",
    "\n",
    "\n",
    "   # df['text_clean'] = df.text_clean.apply(numero_a_texto)\n",
    "   # df['text_clean'] = df.text_clean.str.replace('-', ' ') # Esto porque lo anterior enumera: twenty-five\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_punct)\n",
    "    #df['text_clean'] = df.text_clean.apply(eliminar_palabras_con_numeros)\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_sw) \n",
    "   # df['text_clean'] = df.text_clean.apply(lematizar) \n",
    "    df['text_clean'] = df.text_clean.apply(eliminar_espacios_multiples)\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_cortas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_df[train_df.target == 0]['location'].str.split(expand=True).stack().value_counts().to_csv('data/sample0.csv')\n",
    "#train_df[train_df.target == 1]['location'].value_counts().to_csv('data/sample1.csv')\n",
    "#cols= ['word_lenght']\n",
    "#df = pd.get_dummies(train_df, columns=cols, drop_first=True)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('data/test.csv', encoding='utf-8')\n",
    "#train_df = train_df.sample(frac=1)\n",
    "formato_inicial(train_df)\n",
    "cambiar_valor_target(train_df)\n",
    "formato_inicial(test_df)\n",
    "train_df.to_csv('data/train_fttd.csv', index=False)\n",
    "test_df.to_csv('data/test_fttd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>location_clean</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>1555</td>\n",
       "      <td>bomb</td>\n",
       "      <td></td>\n",
       "      <td>Hiroshima marks 70 years since bomb http://t.co/3u6MDLk7dI</td>\n",
       "      <td>1</td>\n",
       "      <td>bomb</td>\n",
       "      <td></td>\n",
       "      <td>hiroshima marks 70 years since bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>9965</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>I'm at Baan Thai / Tsunami Sushi in Washington DC https://t.co/Udp10FRXrL</td>\n",
       "      <td>0</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>washington district of columbia</td>\n",
       "      <td>i am at baan thai tsunami sushi in washington dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>4295</td>\n",
       "      <td>drowning</td>\n",
       "      <td></td>\n",
       "      <td>@Homukami Only URs and SRs matter Rs you'll be drowning in. Tho you're already drowning in Ns lol.</td>\n",
       "      <td>0</td>\n",
       "      <td>drowning</td>\n",
       "      <td></td>\n",
       "      <td>only urs and srs matter rs you will be drowning in tho you are already drowning in ns lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>9600</td>\n",
       "      <td>thunder</td>\n",
       "      <td>Macon, GA</td>\n",
       "      <td>#thunder outside my house this afternoon #gawx ??????????????????</td>\n",
       "      <td>1</td>\n",
       "      <td>thunder</td>\n",
       "      <td>macon georgia</td>\n",
       "      <td>thunder outside my house this afternoon gawx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1701</td>\n",
       "      <td>bridge collapse</td>\n",
       "      <td>California</td>\n",
       "      <td>#computers #gadgets Two giant cranes holding a bridge collapse into nearby homes http://t.co/UZIWgZRynY #slingnews</td>\n",
       "      <td>1</td>\n",
       "      <td>bridge collapse</td>\n",
       "      <td>california</td>\n",
       "      <td>computers gadgets two giant cranes holding a bridge collapse into nearby homes slingnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>3848</td>\n",
       "      <td>detonation</td>\n",
       "      <td></td>\n",
       "      <td>Ignition Knock (Detonation) Sensor-Senso Standard fits 03-08 Mazda 6 3.0L-V6 http://t.co/c8UXkIzwM6 http://t.co/SNxgH9R16u</td>\n",
       "      <td>0</td>\n",
       "      <td>detonation</td>\n",
       "      <td></td>\n",
       "      <td>ignition knock detonation sensor standard fits 03 08 mazda 6 30l v6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>6837</td>\n",
       "      <td>loud bang</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>daviesmutia: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass.</td>\n",
       "      <td>1</td>\n",
       "      <td>loud bang</td>\n",
       "      <td>kenya</td>\n",
       "      <td>daviesmutia breaking news unconfirmed i just heard a loud bang nearby in what appears to be a blast of wind from my neighbour ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>3760</td>\n",
       "      <td>destruction</td>\n",
       "      <td>Silesia, Poland</td>\n",
       "      <td>@LT3dave so many specs so much fan service so much lore destruction</td>\n",
       "      <td>0</td>\n",
       "      <td>destruction</td>\n",
       "      <td>silesia poland</td>\n",
       "      <td>so many specs so much fan service so much lore destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>8476</td>\n",
       "      <td>screamed</td>\n",
       "      <td>1D | 5SOS | AG</td>\n",
       "      <td>I JUST SCREAMED IN 57 LANGUAGES THIS IS SO GOOD https://t.co/ldjet9tfMk</td>\n",
       "      <td>0</td>\n",
       "      <td>screamed</td>\n",
       "      <td>ag</td>\n",
       "      <td>i just screamed in 57 languages this is so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1065</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>Gages Lake, IL</td>\n",
       "      <td>@beckyfeigin I defs will when it stops bleeding!</td>\n",
       "      <td>1</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>gages lake illinois</td>\n",
       "      <td>i defs will when it stops bleeding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword         location  \\\n",
       "1076  1555             bomb                    \n",
       "6947  9965          tsunami   Washington, DC   \n",
       "2988  4295         drowning                    \n",
       "6702  9600          thunder        Macon, GA   \n",
       "1181  1701  bridge collapse       California   \n",
       "2682  3848       detonation                    \n",
       "4804  6837        loud bang            Kenya   \n",
       "2620  3760      destruction  Silesia, Poland   \n",
       "5935  8476         screamed   1D | 5SOS | AG   \n",
       "735   1065         bleeding   Gages Lake, IL   \n",
       "\n",
       "                                                                                                                                          text  \\\n",
       "1076                                                                                Hiroshima marks 70 years since bomb http://t.co/3u6MDLk7dI   \n",
       "6947                                                                 I'm at Baan Thai / Tsunami Sushi in Washington DC https://t.co/Udp10FRXrL   \n",
       "2988                                        @Homukami Only URs and SRs matter Rs you'll be drowning in. Tho you're already drowning in Ns lol.   \n",
       "6702                                                                         #thunder outside my house this afternoon #gawx ??????????????????   \n",
       "1181                        #computers #gadgets Two giant cranes holding a bridge collapse into nearby homes http://t.co/UZIWgZRynY #slingnews   \n",
       "2682                Ignition Knock (Detonation) Sensor-Senso Standard fits 03-08 Mazda 6 3.0L-V6 http://t.co/c8UXkIzwM6 http://t.co/SNxgH9R16u   \n",
       "4804  daviesmutia: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass.   \n",
       "2620                                                                       @LT3dave so many specs so much fan service so much lore destruction   \n",
       "5935                                                                   I JUST SCREAMED IN 57 LANGUAGES THIS IS SO GOOD https://t.co/ldjet9tfMk   \n",
       "735                                                                                           @beckyfeigin I defs will when it stops bleeding!   \n",
       "\n",
       "      target    keyword_clean                   location_clean  \\\n",
       "1076       1             bomb                                    \n",
       "6947       0          tsunami  washington district of columbia   \n",
       "2988       0         drowning                                    \n",
       "6702       1          thunder                    macon georgia   \n",
       "1181       1  bridge collapse                       california   \n",
       "2682       0       detonation                                    \n",
       "4804       1        loud bang                            kenya   \n",
       "2620       0      destruction                   silesia poland   \n",
       "5935       0         screamed                               ag   \n",
       "735        1         bleeding              gages lake illinois   \n",
       "\n",
       "                                                                                                                             text_clean  \n",
       "1076                                                                                                hiroshima marks 70 years since bomb  \n",
       "6947                                                                                   i am at baan thai tsunami sushi in washington dc  \n",
       "2988                                          only urs and srs matter rs you will be drowning in tho you are already drowning in ns lol  \n",
       "6702                                                                                       thunder outside my house this afternoon gawx  \n",
       "1181                                           computers gadgets two giant cranes holding a bridge collapse into nearby homes slingnews  \n",
       "2682                                                                ignition knock detonation sensor standard fits 03 08 mazda 6 30l v6  \n",
       "4804  daviesmutia breaking news unconfirmed i just heard a loud bang nearby in what appears to be a blast of wind from my neighbour ass  \n",
       "2620                                                                         so many specs so much fan service so much lore destruction  \n",
       "5935                                                                                    i just screamed in 57 languages this is so good  \n",
       "735                                                                                                  i defs will when it stops bleeding  "
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df[train_df.text_clean.str.contains('\\'')]['text_clean'].str.split(expand=True).stack().value_counts().to_csv('data/sample.csv')\n",
    "#train_df.sample(2)\n",
    "#train_df[train_df.location_clean.str.contains('')]['location_clean'].value_counts().to_csv('data/location.csv')\n",
    "#train_df[train_df.text_clean.str.contains('')]['text_clean'].str.split(expand=True).stack().value_counts().to_csv('data/textss.csv')\n",
    "train_df[train_df.text_clean.str.contains('')].sample(10)\n",
    "#test_df[test_df.text.str.contains('enna')]\n",
    "#train_df.sample(5)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grafico_distr(train_df, 'cant_palabras', 'Gráfico de distr. de cantidad de palabras en text',  'Cantidad', '')\n",
    "#grafico_distr(train_df, 'cant_caracteres','Gráfico de dist. de cantidad de caracteres en text', 'Cantidad', '')\n",
    "#grafico_distr(train_df, 'cant_stopwords_pct','Gráfico de distr. porcentual de stopwords en text', 'Porcentaje', '')\n",
    "#grafico_distr(train_df, 'cant_mayus_pct', 'Gráfico de distr. porcentual de mayúsculas en text','Porcentaje', '')\n",
    "#grafico_pie(train_df, 'Porcentaje de tweets que contienen link', 'ref_noticias', 'target')\n",
    "#grafico_distr(train_df, 'cant_numeros', 'Gráfico de distr. porcentual de mayúsculas en text','Cantidad', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de la matriz de pesos(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens únicos(blending):  14705\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "# Cantidad máxima de palabras en text_clean\n",
    "MAX_SEQUENCE_LENGTH = train_df['text_clean'].str.split().str.len().max() \n",
    "\n",
    "#Maximo de palabras\n",
    "MAX_NUM_WORDS = 50000\n",
    "\n",
    "# Matriz de salida\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "VALIDATION_SPLIT = 0.1  \n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "t = test_df['text_clean']\n",
    "\n",
    "#Cantidad de toquens\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Más una columna para el index\n",
    "print('Tokens únicos(blending): ', vocab_size)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_datos_blending(X, y):\n",
    "    #Para entrenar los resultados de los clasificadores\n",
    "    train, test = train_test_split(train_df, test_size=0.1)\n",
    "    X = train['text_clean']\n",
    "    y = train['target']\n",
    "    \n",
    "    X_t = test['text_clean']\n",
    "    y_t = test['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = carga_datos_train(X, y)\n",
    "    \n",
    "\n",
    "    #Texo a sentencias\n",
    "    X_t = tokenizer.texts_to_sequences(X_t.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_t = pad_sequences(X_t, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    # 0/1\n",
    "    y_t = y_t.values\n",
    "    return X_train, X_test, y_train, y_test, X_t, y_t\n",
    "\n",
    "def carga_datos_test(X_train, y_train, X_test):\n",
    "\n",
    "    \n",
    "    #Texo a sentencias\n",
    "    X_train = tokenizer.texts_to_sequences(X_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    # [0 1]\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def carga_datos_train(X, y):\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=12)\n",
    "        \n",
    "\n",
    "    #Texo a secuencias\n",
    "    X_train = tokenizer.texts_to_sequences(X_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    # [0 1]\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    y_test = pd.get_dummies(y_test).values\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://nlp.stanford.edu/projects/glove/\n",
    "#https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "#https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "\n",
    "#EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin'    \n",
    "#EMBEDDING_FILE = 'data/wiki-news-300d-1M.vec' \n",
    "EMBEDDING_FILE = 'data/glove.840B.300d.txt' #Best\n",
    "#EMBEDDING_FILE = 'data/glove.6B.300d.txt' \n",
    "#EMBEDDING_FILE = 'data/glove.twitter.27B.200d.txt'\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def embeddings_matrix(w2vec=False):\n",
    "    not_found_vect = []\n",
    "    if w2vec == True:\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True) #para word2vec\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    # Matriz de pesos\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            if w2vec == True:\n",
    "                embedding_vector = embeddings_index.wv[word]\n",
    "            else:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[index] = embedding_vector\n",
    "        except:\n",
    "            not_found_vect.append(word)\n",
    "    return embedding_matrix, embeddings_index, not_found_vect\n",
    "            \n",
    "#Create embedding matrix\n",
    "embedding_matrix, embeddings_index, not_found_vect = embeddings_matrix(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras que no se encuentran en los vectores(ver!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pct palabras cubiertas: 0.8820809248554913\n",
      "Pct texto cubierto: 0.9810534641809302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14705, 300)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c = porcentaje_cobertura(X,embeddings_index)\n",
    "print('Pct palabras cubiertas:', b)\n",
    "print('Pct texto cubierto:', c)\n",
    "#print('Palabras no cubiertas:', a)\n",
    "serie = pd.Series(a, index=a.keys())\n",
    "palabras_df = serie.to_frame().reset_index()\n",
    "palabras_df.sort_values(by=0, ascending=False).to_csv('data/palabras_no_tw.csv')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El mejor algoritmo(submiteado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 29, 300)           4308600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 29, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_53 (Bidirectio (None, 29, 64)            85248     \n",
      "_________________________________________________________________\n",
      "bidirectional_54 (Bidirectio (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,460,025\n",
      "Trainable params: 151,425\n",
      "Non-trainable params: 4,308,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "215/215 [==============================] - 13s 60ms/step - loss: 0.5982 - accuracy: 0.7103 - val_loss: 0.4530 - val_accuracy: 0.8018 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      " 49/215 [=====>........................] - ETA: 10s - loss: 0.4838 - accuracy: 0.7793"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=1e-6, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=([reduce_lr, early_stop]))\n",
    "\n",
    "pred_lstm_bi = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_lstm_bi, axis=1)\n",
    "resultados(pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 34, 300)           4411500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 34, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 34, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 34, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 32, 128)           38528     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,590,686\n",
      "Trainable params: 179,186\n",
      "Non-trainable params: 4,411,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/211\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 0.6703 - accuracy: 0.5965 - val_loss: 0.6521 - val_accuracy: 0.6165 - lr: 1.0000e-04\n",
      "Epoch 2/211\n",
      "41/41 [==============================] - 3s 72ms/step - loss: 0.6209 - accuracy: 0.6888 - val_loss: 0.5982 - val_accuracy: 0.7303 - lr: 1.0000e-04\n",
      "Epoch 3/211\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.5691 - accuracy: 0.7390 - val_loss: 0.5425 - val_accuracy: 0.7373 - lr: 1.0000e-04\n",
      "Epoch 4/211\n",
      "41/41 [==============================] - 5s 114ms/step - loss: 0.5253 - accuracy: 0.7579 - val_loss: 0.5040 - val_accuracy: 0.7601 - lr: 1.0000e-04\n",
      "Epoch 5/211\n",
      "41/41 [==============================] - 4s 110ms/step - loss: 0.4966 - accuracy: 0.7711 - val_loss: 0.4756 - val_accuracy: 0.7793 - lr: 1.0000e-04\n",
      "Epoch 6/211\n",
      "41/41 [==============================] - 4s 104ms/step - loss: 0.4805 - accuracy: 0.7805 - val_loss: 0.4542 - val_accuracy: 0.8056 - lr: 1.0000e-04\n",
      "Epoch 7/211\n",
      "41/41 [==============================] - 4s 105ms/step - loss: 0.4650 - accuracy: 0.7908 - val_loss: 0.4393 - val_accuracy: 0.8109 - lr: 1.0000e-04\n",
      "Epoch 8/211\n",
      "41/41 [==============================] - 4s 100ms/step - loss: 0.4489 - accuracy: 0.8046 - val_loss: 0.4282 - val_accuracy: 0.8126 - lr: 1.0000e-04\n",
      "Epoch 9/211\n",
      "41/41 [==============================] - 5s 125ms/step - loss: 0.4443 - accuracy: 0.8065 - val_loss: 0.4210 - val_accuracy: 0.8109 - lr: 1.0000e-04\n",
      "Epoch 10/211\n",
      "41/41 [==============================] - 8s 197ms/step - loss: 0.4381 - accuracy: 0.8021 - val_loss: 0.4170 - val_accuracy: 0.8126 - lr: 1.0000e-04\n",
      "Epoch 11/211\n",
      "41/41 [==============================] - 4s 108ms/step - loss: 0.4355 - accuracy: 0.8079 - val_loss: 0.4143 - val_accuracy: 0.8126 - lr: 1.0000e-04\n",
      "Epoch 12/211\n",
      "41/41 [==============================] - 3s 84ms/step - loss: 0.4270 - accuracy: 0.8130 - val_loss: 0.4095 - val_accuracy: 0.8144 - lr: 1.0000e-04\n",
      "Epoch 13/211\n",
      "41/41 [==============================] - 3s 82ms/step - loss: 0.4255 - accuracy: 0.8169 - val_loss: 0.4080 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
      "Epoch 14/211\n",
      "41/41 [==============================] - 3s 82ms/step - loss: 0.4227 - accuracy: 0.8151 - val_loss: 0.4057 - val_accuracy: 0.8179 - lr: 1.0000e-04\n",
      "Epoch 15/211\n",
      "41/41 [==============================] - 3s 76ms/step - loss: 0.4185 - accuracy: 0.8163 - val_loss: 0.4034 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
      "Epoch 16/211\n",
      "41/41 [==============================] - 4s 98ms/step - loss: 0.4127 - accuracy: 0.8237 - val_loss: 0.4028 - val_accuracy: 0.8231 - lr: 1.0000e-04\n",
      "Epoch 17/211\n",
      "41/41 [==============================] - 5s 120ms/step - loss: 0.4132 - accuracy: 0.8209 - val_loss: 0.4007 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
      "Epoch 18/211\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.4082 - accuracy: 0.8235 - val_loss: 0.4033 - val_accuracy: 0.8214 - lr: 1.0000e-04\n",
      "Epoch 19/211\n",
      "41/41 [==============================] - 4s 98ms/step - loss: 0.4114 - accuracy: 0.8239 - val_loss: 0.4002 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
      "Epoch 20/211\n",
      "41/41 [==============================] - 4s 104ms/step - loss: 0.4062 - accuracy: 0.8243 - val_loss: 0.3992 - val_accuracy: 0.8266 - lr: 1.0000e-04\n",
      "Epoch 21/211\n",
      "41/41 [==============================] - 5s 111ms/step - loss: 0.4088 - accuracy: 0.8237 - val_loss: 0.3986 - val_accuracy: 0.8214 - lr: 1.0000e-04\n",
      "Epoch 22/211\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.4022 - accuracy: 0.8254 - val_loss: 0.3989 - val_accuracy: 0.8266 - lr: 1.0000e-04\n",
      "Epoch 23/211\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.3921 - accuracy: 0.8368"
     ]
    }
   ],
   "source": [
    "        \n",
    "X_train, X_test, y_train, y_test = carga_datos_train(X,y)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 3, activation='relu'))        \n",
    "#model.add(Conv1D(256, 3, activation='relu'))        \n",
    "#model.add(Conv1D(512, 3, activation='relu'))        \n",
    "model.add(GlobalMaxPool1D())\n",
    "#model.add(Dropout(0.2))        \n",
    "#model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))        \n",
    "model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 211\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=([reduce_lr, early_stop]))\n",
    "\n",
    "\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_t, y_t = carga_datos_blending(X, y)\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 1 - BI-LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100 #best 9\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm_bi = model.predict(X_t)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 2 - LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm = model.predict(X_t)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 3 - CONV-1D\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df = pd.DataFrame(pred_lstm)\n",
    "lstm_bi_df = pd.DataFrame(pred_lstm_bi)\n",
    "conv_df = pd.DataFrame(pred_conv)\n",
    "\n",
    "\n",
    "val_df = pd.concat([lstm_df, lstm_bi_df, conv_df],axis=1)\n",
    "\n",
    "\n",
    "#Entrenamiento\n",
    "LR = LogisticRegression(C=1.2, n_jobs=-1)\n",
    "LR.fit(val_df, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y, X_test = carga_datos_test(X, y, t)\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 1 - BI-LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100 #best 9\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm_bi = model.predict(X_test)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 2 - LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm = model.predict(X_test)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 3 - CONV-1D\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados de las predicciones\n",
    "lstm_df = pd.DataFrame(pred_lstm)\n",
    "lstm_bi_df = pd.DataFrame(pred_lstm_bi)\n",
    "conv_df = pd.DataFrame(pred_conv)\n",
    "\n",
    "val_df = pd.concat([lstm_df, lstm_bi_df, conv_df],axis=1)\n",
    "\n",
    "#Métricas\n",
    "y_pred = LR.predict(val_df)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "resultados(y_pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM -> TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "pred_lstm = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_lstm, axis=1)\n",
    "resultados(pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED CONVOLUCIONAL 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "\n",
    "# Agrego unua red convolucional 1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(64,  return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_conv, axis=1)\n",
    "resultados(pred, test_df)\n",
    "\n",
    "'''\n",
    "\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "MLP = MLPClassifier(activation='relu', solver='adam',random_state=1)\n",
    "\n",
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "#Predicciones\n",
    "y_pred = MLP.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.771 (0.016831)\n",
      "Reporte de clasificación: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1109\n",
      "           1       0.79      0.68      0.73       795\n",
      "\n",
      "    accuracy                           0.79      1904\n",
      "   macro avg       0.79      0.78      0.78      1904\n",
      "weighted avg       0.79      0.79      0.79      1904\n",
      "\n",
      "Matriz de confusión: \n",
      " [[970 139]\n",
      " [256 539]]\n",
      "ROC: \n",
      " 0.7763246394564768\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAugAAAI6CAYAAAB1rL20AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzde1RVdfrH8c8R8AKooJg3tNAEFbE0zCzvoGGjpVnNdLPUMbO7mmVNZTaV5Zh5zcxMtMyx8lJeMhUvqZMCWaOhaBoqiKgIqAjKxf37w985IwHC2Wz0KO/XrLOW7e93f/dzDsxaz3l49nfbDMMwBAAAAMAlVLrSAQAAAAD4HxJ0AAAAwIWQoAMAAAAuhAQdAAAAcCEk6AAAAIALIUEHAAAAXAgJOgAAAOBCSNABAAAAF0KCDgCoEEaMGKGgoCCNGzfuSocCAJdEgg7AJUydOlVBQUEaPXr0lQ5FQUFBCgoKUlJS0pUOxeUcPXpUL7/8sjp16qSWLVtesZ/Z4sWLFRQUpEcffbRU85cuXaoVK1aoU6dOeumll8o5OgAoG/crHQAA5z366KOKjo6WJF1//fVavXr1Jec/8sgjiomJkSQFBARo1apVlsSxbds2RUdHq0WLFgoPD7dkTbiunJwcDRgwQAcOHJC3t7datmwpDw8P3XDDDVc6tEtKTEzUW2+9pSZNmujDDz+Um5vblQ4JAC6JBB24yh08eFCxsbEKDQ0tcvzQoUOKjY0tl2tHR0dr2rRp6tevX5kTdF9fXwUEBKhOnToWRQerbdmyRQcOHNB1112nFStWqEaNGlcslurVqysgIED169e/5Lz8/HyNGjVK7u7u+vjjj1W9evXLFCEAmEeCDlzFmjZtqv3792vx4sXFJuiLFy+WYRiOua7qkUce0SOPPHKlw8Al7Nu3T5LUtm3bK5qcS1KPHj3Uo0ePEucdOHBAd9xxh1566SVdf/31lyEyACg7etCBq1jPnj3l6empVatWKTs7u9D4+fPntXTpUrm5uemee+65AhHiWnLu3DlJUrVq1a5wJKXXtGlTPfvss2rbtu2VDgUASo0KOnAV8/T0VEREhBYvXqwffvhBffv2LTD+008/6ciRI+rateslW0dSUlL0ww8/6Mcff9TBgwd17Ngxubu764YbblB4eLgGDBggb2/vAucEBQU5/r1kyRItWbKkwPiePXskXajgv/LKK7r11ls1d+5cLViwQEuWLFFCQoIyMzMVFRUlf39/TZ061dEu89577znW6d69uw4fPlziZ2Ffp7R++OEHRUZGKj4+Xm5ubgoKCtLAgQNL1aoTFxenefPmKSYmRsePH1eVKlUUFBSk/v37q2/fvqpUyfnah2EYioqK0qJFi7Rz505lZGSoRo0a8vf3V5cuXXT//ffruuuuK3BOenq65syZo3Xr1ikpKUk2m03+/v7q0aOHBg4cWGQ7h/3+hXHjxql79+6aPn26oqKidOzYMfn6+qpLly56/vnnC/y+2H82dn/+eds/+4vXvvfeewtdOykpSWFhYZL+9/thl5OTo/nz52vlypX6448/dPbsWdWoUUN+fn4KDQ3Vfffdp+DgYMf8i3+vPv/880LXysrK0hdffKFVq1bpwIEDys/PV/369dWlSxcNHjy40GcpSaNHj9aSJUv0zDPPaMiQIfrkk0+0YsUKJScny8vLS7fddpteeOEFl++5B3D1I0EHrnL9+/fX4sWLtWjRokIJ+uLFiyVJ9957r86cOVPsGnPnztVnn32mqlWrys/PT4GBgcrIyFB8fLzi4uK0YsUKffnll6pZs6bjnLZt2+rIkSM6cuSIateuXWL7gGEYev7557V69WrVr19fAQEBpdolpVWrVqpbt26RY2lpaTpw4ECJa/zZ5MmT9dFHH0mSatWqpQYNGmjfvn16+umn9eqrr17y3E8//VQTJkyQYRjy8vJSkyZNlJGRodjYWMXGxioqKkpTpkxx6kbEs2fPasSIEYqKipIk+fj4KCgoSKdOndKuXbv03//+V/Xr1y+Q9O7bt0+DBg3S0aNH5ebmphtvvFGGYWjfvn3au3evli5dqjlz5hT7c0lJSVHfvn11/PhxNWnSRA0bNtShQ4f09ddfa+vWrVq6dKnjS1n9+vUv+fOuUqVKqd9rUfLz8zV48GDHjc8NGzZUQECATp48qYMHD2rv3r2qUaNGgQT9Uo4ePapBgwZp3759stlsatKkiapUqaLff/9dkZGRWrp0qT755BPddNNNRZ6fmZmpv/71r9qzZ4+aNGmi66+/XgkJCfr+++/1008/afHixWrYsGGZ3jMAXJIB4KrzyCOPGIGBgcbMmTMNwzCMHj16GEFBQcahQ4ccc06ePGmEhIQYt956q3Hu3Dlj0aJFRmBgoHHnnXcWWu8///mPsW3bNiMvL6/A8eTkZOPJJ580AgMDjddff73QeVOmTDECAwONl19+udhY7ddt0aKF0a5dO2PTpk2OsdzcXCM3N7fUa13s9OnTRu/evY3AwEDjqaeeMvLz80t13pYtW4zAwEAjMDDQ+OSTTxzn5ebmGlOmTDGCg4Md44mJiQXOXbFihREYGGiEhoYaS5YsKXDN//73v0aPHj2MwMBAY9q0aaWKxe7ll192rPv9998XWDc7O9tYsmSJERMT4zh27tw548477zQCAwON+++/3zh8+LBj7ODBg8bdd99tBAYGGnfffXehn6n9dyc4ONgYNGiQcfToUcdYXFyccfvttxuBgYHG5MmTC8VZ0s/IvvaiRYuKHE9MTHR8thdbs2aNERgYaHTq1MnYvXt3gbHc3Fxjw4YNxsaNGwsct/9ePfLII8XG0bNnT2PPnj2O48ePHzcGDBjguNapU6cKnGf/OQQHBxv9+vUzDhw44Bg7dOiQ4zN/6aWXinx/AGAVetCBa0C/fv1kGEaBtoMVK1bo3Llz6tOnjypXrnzJ8zt06KBbb721UNW3fv36mjhxojw8PLRs2TLl5+ebjjE/P1+vv/66Onbs6Djm7u4ud3fn/5CXn5+v4cOHa+/evWrVqpUmTJhQ6raSmTNnSpLCw8M1ZMgQx3nu7u569tlndeuttxZ5Xl5eniZMmCBJevfddwu1srRu3VoTJ06UzWZTZGSkcnJyShVPfHy84+c2ZcoURUREFFi3atWq6tu3b4GbgFeuXKmEhAR5eHhoypQpatCggWOscePGmjRpktzc3BQfH6+1a9cWeV1vb299+OGHBVo9WrZsqb///e+SpPXr15cqfiv88ccfkqSIiAg1b968wJi7u7u6dOmizp07l2qt2NhYRyX+X//6lwIDAx1jfn5+mjJliry9vXX06FF9/fXXRa5hs9k0adKkAn8laNSokUaMGCHp8n42AComEnTgGmBPFpcuXSrDMCT9r73lvvvuK9UamZmZ+uqrr/TKK69o8ODBeuihh/Tggw9q0KBBstlsysrKMtVOYufl5aVevXqZPv9ib7/9tn788Uc1aNBAH3/8calvWszKynLsB1/cA24ee+yxIo//97//1eHDh1WnTp1idw9p1aqVGjRooFOnTikuLq5UMa1Zs0aS1KZNG3Xo0KFU52zcuFGS1KtXL9WrV6/QeEBAgLp37y5J2rBhQ5Fr9O7du8idWG6++WZJF7bvvFzsXzD+85//KC0trUxr2d/vLbfcotatWxcar1mzpuP/E8V9Nh07dlTjxo0LHbd/NidPnlRGRkaZ4gSAS6EHHbgG1K9fXx06dNCWLVu0detW1alTRzt27FDLli0LVSSLEhMTo+eff14nTpy45LyyJCUBAQGmquV/FhkZqS+//FLe3t76+OOPndo3/dChQ46/AjRr1qzIOTfeeGORx+Pj4yVd6Bd/8MEHi72G/TM6cuSI2rRpU2JMe/fulaRSzbVLSEiQVPx7kKTAwECtWbPGMffPAgICijzu5+cnSZe8Z8Fq4eHhCggI0O+//64uXbqoffv2Cg0NVZs2bdSmTZsS/wJ0sdJ+NtL/Kvd/VtxNoPbPRrrw+fj4+JQ6LgBwBgk6cI249957tWXLFi1evNiRSBS1k8afZWZm6rnnnlNaWpo6dOigJ554QkFBQapRo4Y8PDwkSV27dtWRI0eUl5dnOj5PT0/T59qtX79e77//vtzd3TVp0qQCO8mURmZmpiSpUqVKqlWrVpFzLk7CLnbq1ClJ0unTp7V9+/YSr3X27FmnYnLmATr25PlSX07sY8Ul2sX91cFms5U6DqtUrVpVX375paZPn64VK1Zo06ZN2rRpk6QLrTgPPPCAnnvuuVL9pcSKz6a439WLW4/sf6kCgPJAgg5cI3r06KEaNWpozZo18vT0lIeHh3r37l3ieRs3blRaWprq16+vjz/+WFWrVi0wbhiGTp48WV5hl9ru3bs1YsQInT9/XmPGjFGnTp2cXsO+K8n58+eVlpam2rVrF5qTmppa5Ln2pK1du3b64osvnL52STGdPn261Od4eXlJko4fP17sHPuYfe7lVFzyWtRe/Xa1atXS66+/rtdee0379u3T9u3btXnzZq1bt06fffaZjhw5okmTJpV4bVf/bACgNOhBB64RVapUUa9evZSdna0TJ06oe/fu8vX1LfE8+1aHISEhhZJz6UILRlZWVpHnXq5q69GjRzV06FBlZWVp0KBB+tvf/mZqncaNGztuhLU/FfPPijtub4v4/fffdf78eVPXL4r9rwC//PJLqc9p0qSJI5bi2Ftn7HMvB/uXmOJapUpzD4PNZlOzZs3017/+VVOnTtX06dMlSd9//73S09NLPN+Zz6Zp06YlrgcAVwIJOnAN+dvf/qYOHTqoQ4cOevjhh0t1jj0pL67iOHv27BLPvVRltKyysrI0bNgwHT16VD169NCoUaNMr+Xp6enYDaW4Kvi8efOKPH7LLbfouuuuU0ZGhr755hvTMfxZz549ZbPZ9Msvv2jbtm2lOqdLly6SLiStKSkphcYPHjyodevWSbrQnnS52Hc9Ke7LxoIFC5xe8+IngB49erTE+fbP5ueff9aOHTsKjZ86dUqLFi2SdHk/GwBwBgk6cA1p2bKlIiMjFRkZqfbt25fqnHbt2km6kFQtXLjQcTwnJ0eTJk3SsmXLHL3of2ZPyHbs2FEuNxWeP39eL774ouLi4hQSEqJ//etfpp7SebEnnnhCkrR69Wp99tlnjmp4fn6+pk+fXmySXLlyZb300kuSpH/+85+KjIws1Gd+5swZ/fDDD/rHP/5R6ngCAwPVr18/SdJzzz2nNWvWFGgROXfunL799lvFxsY6jvXq1UsBAQHKzc3V888/r+TkZMdYYmKiXnjhBeXn56t58+aOJ3deDvadY9avX68VK1YUeA8TJ050bH/4Z3PmzNGsWbMKPTE2OztbU6dOlXShR780T/AMDQ11bJU5atSoApX0EydO6IUXXtDp06dVt27dUu9wBACXGz3oQAXXsmVL3XPPPfr222/1xhtvaNq0abruuut08OBBnT59Wi+88IK+/vrrQsmTJN1xxx3y8/NTcnKyunbtqoCAAMdTJYt6/LqzkpOTHU/XtLe3FGfKlCml2tGlY8eOGjp0qGbOnKn3339fn376qRo0aKCkpCSlp6fr1Vdf1bvvvlvkuX369FFaWprGjx+vcePGaeLEiY73nJ6erqSkJJ0/f97pp0yOGTNGGRkZWrdunZ555hn5+PioUaNGOnXqlJKTk5Wbm6tx48Y5qv+VK1fW1KlTNWjQIP36668KDw8v8CRRewzOPtG0rG677Tb17NlTq1ev1ogRI/T++++rTp06SkhIUF5ent544w29/vrrhc5LTk7WvHnzNGHCBNWpU0d169ZVbm6uEhMTlZWVJXd3d7311ltFtmAVZcKECY4nifbp00dNmzZV5cqV9fvvvys3N1c+Pj6aOnWqUzfmAsDlRIIOQOPGjVOzZs20aNEiJSUl6dy5c2rZsqUGDBig8PDwYh/o4unpqcjISE2dOlXbt29XXFxcmXZ6uZT9+/dfcvzcuXOlXmvEiBFq0aKF5s6dq/j4eCUkJKh58+YaOHCgwsPDi03QpQv7pHfs2FHz58/X1q1bdejQIeXk5MjHx0ehoaHq3LlzsfukF6dq1ar66KOPtGrVKi1evFhxcXGKj49XzZo1FRwcrK5duxa6KbZZs2b67rvvNGfOHEVFRengwYOy2Wy68cYbFR4eroEDBxa5z3l5++CDD/Tpp5/q22+/1eHDh5WTk6M77rhDTz/9tOOG2D978MEHVatWLW3btk2HDh1yfMmoV6+eQkND9dhjj5Vqu1C7unXr6uuvv9bnn3+uH374QQkJCcrPz1fDhg3VpUsXDR48WHXr1rXqLQOA5WwGe0UBAAAALoMedAAAAMCFkKADAAAALoQEHQAAAHAhJOgAAACACyFBBwAAAFwICToAAADgQkjQAQAAABfCg4oAAABgOVsP/3Jb21iTVG5ruwIS9BKU5y8XgIrJWJOks/lZVzoMANeYqm6eVzoEWIQWFwAAAMCFUEEHAACA9Wy2Kx3BVYsEHQAAANajT8M0PjoAAADAhVBBBwAAgPVocTGNCjoAAADgQqigAwAAwHoU0E2jgg4AAAC4ECroAAAAsB496KZRQQcAAABcCBV0AAAAWI8ysGkk6AAAALAeLS6m8d0GAAAAcCFU0AEAAGA9CuimUUEHAAAAXAgVdAAAAFivEiV0s6igAwAAAC6ECjoAAACsRwHdNCroAAAAgAuhgg4AAADrsQ+6aSToAAAAsB75uWm0uAAAAAAuhAo6AAAArMc2i6ZRQQcAAABcCBV0AAAAWI8CumlU0AEAAAAXQgUdAAAA1mObRdOooAMAAAAuhAo6AAAArMcuLqaRoAMAAMB65Oem0eICAAAAuBAq6AAAALAeN4maRgUdAAAAcCFU0AEAAGA9CuimUUEHAAAAXAgVdAAAAFiPbRZNI0EHAABAhZKTk6M5c+bou+++U2Jiojw9PRUaGqphw4YpODjYqbVOnTql2bNnKyoqSomJicrPz1e9evXUoUMHPfHEE2rUqJHT8dkMwzCcPqsCsfXwv9IhALjGGGuSdDY/60qHAeAaU9XN80qHUIDt8aByW9uI3GP63JycHA0ePFjR0dGqXbu22rVrp+PHj+vnn3+Wh4eHZsyYoU6dOpVqrdTUVP3tb39TYmKiatWqpZtuuknu7u767bffdOTIEXl5eSkyMlKtW7d2KkYq6AAAALCei26zOGvWLEVHRyskJESRkZHy9vaWJC1fvlwjR47UqFGjtHbtWsfxS/noo4+UmJiojh07aurUqfL0vPAlKS8vT2PHjtVXX32ld955RwsXLnQqRm4SBQAAQIWQl5enefPmSZLGjBlTIAnv3bu3unTpovT0dC1atKhU68XExEiSnnjiCUdyLknu7u569tlnJUk7d+6Usw0rJOgAAACwXqVyfJm0fft2ZWRkyN/fXyEhIYXG77rrLklSVFRUqdbz8PAocU7NmjVlc/KvCSToAAAAqBB2794tScXeCNqyZUtJ0p49petxt/eqf/LJJ8rOznYcz8vL09SpUyVJ999/v9Nx0oMOAAAA67lgD3pycrIkqV69ekWO249nZGTozJkz8vLyuuR6Q4YM0S+//KLNmzere/fuuummm+Th4aGdO3cqIyNDgwcP1vPPP+90nCToAAAAuKqEhYVdcry4FpWsrAs7aFWrVq3I8Yv7yEuToHt7e2vWrFl666239M0332j9+vWOseDgYN10001yc3O75BpFocUFAAAA1rOV48tFJCcn67777tPq1av1z3/+Uz/++KNiYmI0a9YsZWVl6bnnntO0adOcXpcKOgAAAK4qpb2J88/sFfKL+8UvZq+wSyqxei5JL7/8svbu3avJkycrIiLCcbxz584KCAhQnz59NGPGDPXu3Vs33HBDqeOkgg4AAADr2Wzl9zKpQYMGkqSUlJQix+3HfXx8SkzQjxw5oujoaHl4eKhHjx6Fxhs1aqTWrVsrLy9P0dHRTsVJBR0AAADWc8EycIsWLSRJcXFxRY7v2rVLkhQUVPJTUO3JvJeXV7F95jVq1JB04aZTZ7jgRwcAAABYr23btvLx8VFSUpJ27txZaHzlypWSSr4JVZLq1Kkj6ULyffDgwULjeXl5joTf39/fqThJ0AEAAGA9F2xxcXd314ABAyRJY8eOVWZmpmNs+fLl2rhxo3x9fdW/f3/H8R07digiIqJAj7l0Iem275v+2muvKT093TGWm5ur999/X4cPH1b16tXVsWNH5+J0+p0BAAAAV6khQ4Zo69atio6OVs+ePdWuXTulpqYqNjZWHh4eGj9+vLy9vR3zs7OzlZCQUORab7/9th5//HHHWq1bt1bVqlUVFxenI0eOyMPDQ2+//baj1aW0qKADAADAei66zWLlypU1e/ZsDR8+XD4+Plq3bp327dunsLAwLVy4UJ07dy71WsHBwfruu+/06KOPys/PTzExMdq4caNsNpvuueceffPNN4Uq76VhMwzDcPqsCsTWw7meIQAoibEmSWfzs0qeCABOqOrmWfKky8j2VHC5rW18VPRNntcKWlwAAABgvUou9EShqwwtLgAAAIALoYIOAAAA65Vht5WKjgQdAAAA1iM/N40WFwAAAMCFUEEHAACA5Wy0uJhGBR0AAABwIVTQAQAAYDkq6OZRQQcAAABcCBV0AAAAWI4CunlU0AEAAAAXQgUdAAAAlqtECd00KugAAACAC6GCDgAAAMuxi4t5JOgAAACwHAm6ebS4AAAAAC6ECjoAAAAsRwXdPCroAAAAgAuhgg4AAADLUUA3jwo6AAAA4EKooAMAAMBy9KCbRwUdAAAAcCFU0AEAAGA5KujmkaADAADAcjaRoJtFiwsAAADgQqigAwAAwHK0uJhHBR0AAABwIVTQAQAAYDkK6OZRQQcAAABcCBV0AAAAWK4SJXTTqKADAAAALoQKOgAAACzHLi7mkaADAADAciTo5tHiAgAAALgQKugAAACwHAV086igAwAAAC6ECjoAAAAsRw+6eVTQAQAAABdCBR0AAACWo4JuHhV0AAAAwIVQQQcAAIDlqKCbR4IOAAAAy5Ggm0eLCwAAAOBCqKADAADAchTQzaOCDgAAALgQKugAAACwnCv3oOfk5GjOnDn67rvvlJiYKE9PT4WGhmrYsGEKDg4u9TpBQUElzrHZbIqPj3cqPhJ0AAAAVBg5OTkaPHiwoqOjVbt2bXXr1k3Hjx/XmjVrtGHDBs2YMUOdOnUq1Vr9+vUrduyXX37RgQMH1K5dO6djJEEHAACA5Vy1gj5r1ixFR0crJCREkZGR8vb2liQtX75cI0eO1KhRo7R27VrH8Ut57733ih278847JV06iS8OPegAAACoEPLy8jRv3jxJ0pgxYwok4b1791aXLl2Unp6uRYsWlek627dv14EDB+Tp6elI1J1Bgg4AAADLVbLZyu1l1vbt25WRkSF/f3+FhIQUGr/rrrskSVFRUaavIUlLly6VJPXo0UNeXl5On0+LCwAAACznih0uu3fvlqRibwRt2bKlJGnPnj2mr5GTk6Pvv/9ekrn2FokKOgAAACqI5ORkSVK9evWKHLcfz8jI0JkzZ0xdIyoqSqdOnVKDBg102223mVqDCjoAAAAsV543iYaFhV1yvLgWlaysLElStWrVihz39PR0/PvMmTOm2lPs7S1333236c+ACjoAAABggdTUVG3evFmS1LdvX9PrUEEHAACA5Wwqvwq62Zs47RXy7OzsIsftFXZJpqrny5cvV15entq0aaOAgABTMUpXaQU9OztbqampSk1NLfYDBsrqqbsfU8LnPyl7xT5tnbJM7YJuvuT85/sNVvxnG5W1fJ8OzY/WxCfHqIpHFcd4wuc/yViTVOg17dm3y/utAHAh//5yoXqF36V2N7fXw399VDt3/Fbs3H2/79eI50eqV/hduqllG30xb36hOV/9+yvd1/cB3d6uo25v11GPPjhAm3/cXJ5vAbhqNWjQQJKUkpJS5Lj9uI+Pj6kEfcmSJZLKVj2XrpIKelZWlhYtWqSoqCjFx8fr5MmTBcZr1qyp5s2bKzw8XPfee2+B/iHAjAe69NHEoW/oySmvaNvuX/TCvX/XD+O+UNCgLjqecaLQ/Ae79dV7f39Fgya8qP/silWgfxNFjpoowzA0cuZbkqR2z/xFbpXcHOe0uiFIa8f/W19vXHHZ3heAK2vV9z9owvsf6LUx/1BI61aa//mXGvbEU/p2xVLVrl2r0PyzZ8/K399fPe7soQnvfVDkmtfVravnhz+rxtc3liFp2dJlev6Z4Vq46N+6sVnTcn5HQPFc8UFFLVq0kCTFxcUVOb5r1y5JUlBQkNNrx8fHKz4+XlWqVHFs12iWy1fQt2zZovDwcL377rvaunWrMjIyZBhGgVdGRoa2bt2qd955Rz169NCWLVuudNi4yo3o/4Rmfb9AkT98pd2HfteTk0cr69xZDbrzb0XOvz04VFviYrVg/VIdPJqkNT//qAXrv9Wtzf9XdU89maaj6ccdr963hWvf4QPauOOny/W2AFxhn0d+oXvvv1d9771HTW9sqtfG/ENVq1bV0sVLi5zfKiRYI0YNV6+7IlS5skeRc7p266JOXTrp+huu1w03XK9nX3hGnp6e2rFjR3m+FeCq1LZtW/n4+CgpKUk7d+4sNL5y5UpJJd+EWhT7zaFhYWGqUaNGmeJ06QR9165dGjp0qNLS0tS5c2e99957WrZsmWJiYhQXF6e4uDjFxMRo2bJleu+999SpUyedOHFCTz75pGOfS8BZHu4euiUwRGu3b3IcMwxDa7dvUoeWbYs85z9xsbqlWYijDSagXmPddWt3rYxeV+w1Hgm7V5/98G/r3wAAl5Sbk6vdu3brttvaO45VqlRJt3Vorx2/WpNM5+fn6/uVq5Sdna2bbmptyZqAWTabrdxeZrm7u2vAgAGSpLFjxyozM9Mxtnz5cm3cuFG+vr7q37+/4/iOHTsUERGhiIiIYtfNz8/XsmXLJJW9vUVy8RaXGTNmKD8/XxMnTiz2TwXVq1dX9erV1axZM/Xt21fLly/Xiy++qI8++khTp069zBHjWuBXs5bc3dx1NP14geNH01PVvNGNRZ6zYP1S+chEENIAACAASURBVNWspc0fLpbNZpOHu4dmLJuncQumFTm/7+13yse7hiJXf215/ABcU3pGuvLz81Xbr2ArS+3atZXwx4Eyrf373t/16IOPKScnR56e1fThlA/U9EbaW3BluWCHiyRpyJAh2rp1q6Kjo9WzZ0+1a9dOqampio2NlYeHh8aPHy9vb2/H/OzsbCUkJFxyzc2bNys1NVV16tRRx44dyxyjS1fQY2NjdcsttzjVx9O7d2+FhoYqNja2HCMDCurSuoNeffAZPTX1H2o7rJf6vfl3/aV9mF57+Pki5w/u9Td9H71eR04cvcyRArgW3XDDDfpq8b/1xb/n6f6/3q/XX31D+/ftv9JhAS6pcuXKmj17toYPHy4fHx+tW7dO+/btU1hYmBYuXKjOnTs7vab95tA+ffrIzc2thNklc+kKelZWlurUqeP0eX5+fkX2FQGlkXoyTXn5earrW/B3r66vn1LSjxV5zj8ff1Gfr12s2d8vkCT9diBeXlU99ckL7+udL6fIMAzH3MbXNVR4m066d+yQ8nsTAFyOr4+v3NzcdCI1rcDxEydOyM+vdpnW9qjsocbXN5YktQxuqbjf4jT/8wV6Y+xrZVoXKAtXvEnUrnLlynryySf15JNPlji3ffv22rNnzyXnTJo0SZMmTbIqPNeuoDdq1EgxMTEF9qQsSWZmpmJiYtSoUaNyjAzXsty8XP28d6fC2vzvT1Q2m01hbTrqp13bizzHs0o1nTfOFziWfz7fce7FBt75Vx3LSNWKbeb2cAVwdfKo7KEWLVto29ZtjmPnz5/Xtq3Ran2ztf3i5w1Dubk5lq4J4PJx6QS9T58+Sk1N1cCBAx3b3lxKXFycBg0apLS0NN19992XIUJcqyYu+kRD7npQA3rcp+aNb9SM58bJq2o1zflhoSRp7kuT9O6g0Y75y7au1bDej+qvXe/WDfUaKbxtJ/3zsVFatnWNzp//X+Jus9k08M4HNHfNN44EHkDF8ejjj2jxN0v03dLv9Mf+P/T22HeVnZ2tvv3ukST9Y/RrmjxximN+bk6u4nfvUfzuPcrNzdWxo8cUv3uPDh085JgzeeIU/Rz7sw4fTtbve3/X5IlTFBsdq7t6l22bN6CsXPEm0auFS7e4DBo0SJs3b1ZMTIz69+8vf39/BQcHq169eqpWrZqkC437KSkpiouLU1JSkgzDUPv27TVo0KArHD2uZl9tXKY6PrX11mMvqp5vHf26f5ciXn1UxzJSJV1oU7m4Yv72/MkyDENvP/6SGvrV0/GTJ7Rs6xr947PxBdYNb9tJ19f112er2L0FqIgiet2p9LR0fTR1hlJTTyioeZA+mjldtf+/xSXlSIoqVfpf7ezY8eP6a///be86d848zZ0zT6HtbtHsuZ9KktLS0vTa6Nd1/HiqvKt7KzCwmWbM+kgdbr/t8r45AJaxGRc3x7qg3NxcffLJJ5o3b16BBxTZvz1dHH7NmjX12GOPaciQIfLwKHq/WGfZevhbsg4A2BlrknQ2v/StewBQGlXdXOtBjYETi9+WsKz2jlhVbmu7ApeuoEuSh4eHnn76aT355JPavn27du/ereTkZEdfuqenpxo0aKAWLVqobdu2ltw5CwAAAFwpLp+g27m5ualdu3Zq167dlQ4FAAAAJagAreLl5qpJ0AEAAHD1qAg3c5YXl97FBQAAAKhoqKADAADAclTQzaOCDgAAALgQKugAAACwHBV086igAwAAAC6ECjoAAAAsRwHdPCroAAAAgAuhgg4AAADL0YNuHgk6AAAALEeCbh4tLgAAAIALoYIOAAAAy1FBN48KOgAAAOBCqKADAADAchTQzaOCDgAAALgQKugAAACwHD3o5lFBBwAAAFwIFXQAAABYjwq6aVTQAQAAABdCBR0AAACWowfdPBJ0AAAAWI783DxaXAAAAAAXQgUdAAAAlqPFxTwq6AAAAIALoYIOAAAAy1FBN48KOgAAAOBCqKADAADAclTQzaOCDgAAALgQKugAAACwHAV080jQAQAAYDlaXMyjxQUAAABwIVTQAQAAYDkq6OZRQQcAAABcCBV0AAAAWI4KunlU0AEAAAAXQgUdAAAAlqOCbh4VdAAAAMCFUEEHAACA5Sigm0eCDgAAAMvR4mIeCToAAAAqlJycHM2ZM0ffffedEhMT5enpqdDQUA0bNkzBwcFOr3f+/Hl98803+vbbb7Vv3z5lZWXJz89PrVq10mOPPabQ0FCn1iNBBwAAgOVctYKek5OjwYMHKzo6WrVr11a3bt10/PhxrVmzRhs2bNCMGTPUqVOnUq+XmZmpoUOHKjY2Vr6+vmrTpo2qVKmi5ORkrV+/Xi1atCBBBwAAAIoza9YsRUdHKyQkRJGRkfL29pYkLV++XCNHjtSoUaO0du1ax/GSjBw5UrGxsRo0aJCGDx+uypUrO8YyMjKUnp7udIzs4gIAAADL2Wy2cnuZlZeXp3nz5kmSxowZUyAJ7927t7p06aL09HQtWrSoVOutXbtWGzZsUFhYmF5++eUCybkk+fj4KCAgwOk4SdABAABQIWzfvl0ZGRny9/dXSEhIofG77rpLkhQVFVWq9RYsWCBJevzxxy2LUaLFBQAAAOXAFVvQd+/eLUnF3gjasmVLSdKePXtKXCsvL0+xsbFyc3PTzTffrP379+v777/XsWPH5OvrqzvuuEO33nqrqThJ0AEAAFAhJCcnS5Lq1atX5Lj9eEZGhs6cOSMvL69i10pMTNTZs2fl5+enzz//XB988IHy8/Md4x9//LG6du2qiRMnXnKdopCgAwAAwHLluYtLWFjYJceLa1HJysqSJFWrVq3IcU9PT8e/S0rQT548KelCMj9+/Hj17dtXQ4cOVZ06dRQbG6sxY8Zow4YNevPNN/Wvf/3rkvH+GT3oAAAAsJ7NVn4vF3D+/HlJF1pdbr31Vr3//vtq0qSJqlevrm7dumn69Omy2WxatmyZDh065NTaVNABAABwVSntTZx/Zq+QZ2dnFzlur7BLKrEt5eJq+wMPPFBoPCQkRMHBwfrtt98UHR2txo0blzpOKugAAACwnCtus9igQQNJUkpKSpHj9uM+Pj4lJugNGzZ0/Nvf37/IOfbjqampTsVJgg4AAIAKoUWLFpKkuLi4Isd37dolSQoKCipxrerVqzuq4vZ+9D/LyMiQVLDaXhok6AAAALBcJVv5vcxq27atfHx8lJSUpJ07dxYaX7lypaSSb0K1s8/bunVrobFTp045Ev7itnUsDgk6AAAAKgR3d3cNGDBAkjR27FhlZmY6xpYvX66NGzfK19dX/fv3dxzfsWOHIiIiFBERUWi9xx57TFWrVtWXX35ZIEnPycnR2LFjderUKTVv3lxt27Z1Lk5n3xgAAABQkvLcZrEshgwZoq1btyo6Olo9e/ZUu3btlJqaqtjYWHl4eGj8+PHy9vZ2zM/OzlZCQkKRa9WvX1/vvPOOXnrpJQ0cOFA33XST/Pz8tHPnTqWkpMjPz08TJ050+rOggg4AAIAKo3Llypo9e7aGDx8uHx8frVu3Tvv27VNYWJgWLlyozp07O7Ve7969tWDBAnXr1k0HDhzQhg0b5ObmpocffliLFy9W06ZNnY7RZhiG4fRZFYitR9F35QKAWcaaJJ3Nzyp5IgA4oaqbczcilreeix8vt7VX3xtZbmu7AlpcAAAAYDlXbXG5GtDiAgAAALgQKugAAACwHFVg8/jsAAAAABdCBR0AAACWq0QPumlU0AEAAAAXQgUdAAAAlmMXF/OooAMAAAAuhAo6AAAALEcPunkk6AAAALAcLS7m0eICAAAAuBAq6AAAALAcVWDz+OwAAAAAF0IFHQAAAJbjJlHzqKADAAAALoQKOgAAACzHLi7mUUEHAAAAXAgVdAAAAFiOHnTzTFfQw8LCNHz48FLNHTFihMLDw81eCgAAAFcZWzm+rnWmE/TDhw/r2LFjpZp7/PhxHT582OylAAAAgArjsrS45OXlqVIl2t0BAAAqClpczCv3rDk3N1cHDx5UzZo1y/tSAAAAwFWv1BX0mJgYbdu2rcCxI0eOaNq0acWec/bsWcXGxio9PV2dO3c2HyUAAACuKlTQzSt1gr5t2zZNmzatwJ6WR44c0fTp0y95nmEYqlatmp588knzUQIAAAAVRKkT9ObNm6tfv36O/16yZIlq166tTp06FXtOtWrV1LhxY0VERKhevXplixQAAABXDR5UZF6pE/Tw8PACWyUuWbJE119/vcaNG1cugQEAAAAVkeldXKKiolSlShUrYwEAAMA1gh5080wn6A0bNrQyDgAAAFxDSM/NK/M+6AcPHtTcuXP1008/KSUlRefOndOuXbsc419//bWOHj2qgQMHysvLq6yXAwAAAK5pZUrQV65cqVdffVXnzp2TYRiSCt8QcPLkSU2fPl1NmzZVr169ynI5AAAAXCVocTHP9IOK4uPj9dJLLyknJ0cPP/ywPv/8cwUHBxead+edd8owDEVFRZUpUAAAAKAiMF1B//TTT5Wfn69XXnlFAwYMkKQibxpt1KiRatWqpZ07d5qPEgAAAFcVKujmma6gR0dHy8vLy5GcX0q9evV07Ngxs5cCAAAAKgzTFfS0tDQFBgaWaq6bm5vy8vLMXgoAAABXGR5UZJ7pCrq3t7dOnDhRqrnJycny9fU1eykAAACgwjCdoAcFBenYsWPav3//Jef9/PPPOnHihFq3bm32UgAAALjKVLLZyu11rTOdoN99990yDENvvvmmMjMzi5yTlpamN954QzabTXfffbfpIAEAAICKwnQPer9+/bR48WLFxMTonnvu0V/+8hdHy8uSJUu0Z88eLV26VBkZGbrjjjvUs2dPy4IGAACAa7v269zlx2bYnzBkwqlTpzRy5Eht2rSpyBsBDMPQHXfcoUmTJql69eplCvRKsfXwv9IhALjGGGuSdDY/60qHAeAaU9XN80qHUMCw9cPLbe0Z3T4st7VdQZmeJFqjRg3NmjVLP/30k1auXKn4+HidOnVKnp6eCgwMVK9evdS1a1eLQgUAAACufWVK0O06dOigDh06WLEUAAAArgEV4WbO8mL6JlEAAAAA1rOkgg4AAABcjAcVmWc6QR8wYECp57q5ucnb21v+/v4KDQ1V165d5ebmZvbSAAAAwDXLdIIeHR0t6X/fjoraDObPYzabTZGRkWrUqJE++OADhYSEmL08AAAAXJgr91Hn5ORozpw5+u6775SYmChPT0+FhoZq2LBhCg4OLvU6ixcv1iuvvFLseEBAgFatWuV0fKYT9HHjxikpKUkzZ85UlSpVFB4erhYtWsjLy0tnzpxRfHy81q5dq3Pnzmno0KHy9fXV/v37tWrVKh06dEhDhgzRt99+q7p165oNAQAAAHBKTk6OBg8erOjoaNWuXVvdunXT8ePHtWbNGm3YsEEzZsxQp06dnFqzefPmatGiRaHjderUMRWj6QT99ttvV9++fXXzzTdrypQpqlWrVqE5aWlpeu655zR//nwtWbJEDz/8sEaMGKFhw4YpNjZWc+bM0ejRo82GAAAAABflqj3os2bNUnR0tEJCQhQZGSlvb29J0vLlyzVy5EiNGjVKa9eudRwvjfDwcD377LOWxWj6rw9TpkxRZmamJk2aVGRyLkm1atXShx9+qNOnT2vKlCmSJG9vb7377ruSpE2bNpm9PAAAAFxYJZut3F5m5eXlad68eZKkMWPGFEjCe/furS5duig9PV2LFi0q8/svC9MJ+qZNm9SsWTP5+fldcl6dOnUUGBiozZs3O441atRIjRs3VnJystnLAwAAAE7Zvn27MjIy5O/vX+S9kHfddZckKSoq6nKHVoDpFpeMjAzVqFGjVHNzcnKUkZFR4JiPj49SUlLMXh4AAAAuzBUfVLR7925JKvZG0JYtW0qS9uzZ49S6cXFxGj9+vE6fPi1fX1+1adNGnTt3Nr1roekEvW7dutq/f7/27t2rwMDAYuft3btX+/fvl7+/f4HjaWlp8vHxMXt5AAAAwCn27o169eoVOW4/npGRoTNnzsjLy6tU665fv17r168vcOyGG27Q5MmT1bx5c6fjNJ2g9+zZU7Nnz9awYcM0YcIEtWnTptCcX3/9VS+++KIk6c4773QcP3r0qBITE9W+fXuzlwcAAIALK8+bRMPCwi45XlyLSlZWliSpWrVqRY57eno6/l2aBL1OnTp65pln1L17dzVq1Eh5eXnavXu3PvzwQ+3cuVOPP/64li5dWuwXguKYTtCHDRumDRs2aP/+/XrooYcUEBBQaJvFP/74Q4Zh6MYbb9SwYcMc5y5cuFCS1LFjR7OXv2yMNUlXOgQA16Cqbp4lTwIAuLROnToV2pLxjjvuUPv27TVgwAD9/PPPmjlzpsaMGePUuqYTdG9vb33xxRd68803tXr1av3xxx/6448/Csyx2Wzq1auX3njjjQLfQIYOHaq///3vqlq1qtnLXzabU67sTQIArj0d64Up6MOIKx0GgGvMnuHOPxCnPFVS+VXQzd7Eaa+QZ2dnFzlur7BLKnV7S1Hc3d01ZMgQ/fzzz9q4caPz55u+siRfX19NnjxZiYmJ2rx5sxISEpSVlSVPT08FBASoY8eOatSoUaHzqlSpUpbLAgAAAE5r0KCBJBW7UYn9uI+PT5kSdOlCD7okHTt2zOlzTSfoS5culXRhO5pGjRrpwQcfNLsUAAAArjGu+KAi+9M+4+LiihzftWuXJCkoKKjM1zp16pSkgn3tpWV6H/RXXnlF06ZNU+XKlc0uAQAAgGuUKz6oqG3btvLx8VFSUpJ27txZaHzlypWSSr4JtTRWrbrQctSqVSunzzWdoPv4+BT7BFEAAADA1bi7u2vAgAGSpLFjxyozM9Mxtnz5cm3cuFG+vr7q37+/4/iOHTsUERGhiIiC9w5lZ2dr9uzZSk9PL3D8/Pnzmj9/vubOnStJevTRR52P0+kz/l9wcLB27twpwzBc8k8YAAAAuHJs5XiTaFkMGTJEW7duVXR0tHr27Kl27dopNTVVsbGx8vDw0Pjx4+Xt7e2Yn52drYSEhELr5Obmavz48Zo0aZJatWql+vXrKysrS3v27FFycrJsNpueffZZdevWzekYTVfQH3vsMZ08edLx7QAAAABwdZUrV9bs2bM1fPhw+fj4aN26ddq3b5/CwsK0cOFCde7cuVTrVK1aVcOGDVNoaKhSUlIUFRWlLVu2yGazqXfv3vryyy/1zDPPmIrRZhiGYepMSXPnztWECRN077336r777lOzZs2uiq0TncE2iwCsxjaLAMqDq22z+I+tr5Xb2u/c9na5re0KTLe42O+ClaSvvvpKX3311SXn22w2x52xAAAAAIpmOkF3tvBehkI9AAAArjJl2W2lojOdoJt9ghMAAACA4plO0Bs2bGhlHAAAALiG2MzvRVLhmU7QAQAAgOLQ4mIeX20AAAAAF1LmCvq5c+e0bt067d69WxkZGcrNzS1yns1m07vvvlvWywEAAOAqwIMszStTgr5hwwaNHj1aJ0+edByz79Zy8Q/F/rRREnQAAADg0kwn6Hv27NGzzz6r8+fPq3fv3oqNjVVKSoqeeuopZWRk6Ndff9WuXbtUtWpVPfTQQ/L09LQybgAAALgwm6igm2U6Qf/ss8+Ul5en119/XQ899JAeeughpaSk6LnnnnPM+emnnzRy5Eht3bpVCxYssCRgAAAA4Fpm+ibRmJgYeXp66v777y92TocOHfThhx9q165d+uSTT8xeCgAAAFeZSjZbub2udaYT9NTUVDVo0EAeHh6SJDc3N0lSTk5OgXnt27eXv7+/Vq1aVYYwAQAAgIrBdIJerVo1R3IuSV5eXpKko0ePFppbo0YNJScnm70UAAAArjI2m63cXtc60wn6ddddp+PHjzv+OyAgQNKF1peLnT59WgkJCapUiS3XAQAAKopK5fi/a12p3+HSpUu1adMmx3+3atVKaWlpOnXqlCSpc+fOMgxDEyZM0I8//qisrCwdPHhQL774os6ePaubb77Z+ugBAACAa0ypd3EZPXq0QkND1alTJ0lSt27dtGTJEm3cuFF9+vRRhw4ddPvtt+s///mPhg4d6jjPMAy5u7vrqaeesj56AAAAuKSK0IpSXpz6G4H9IUTShQR92bJluv322x3Hpk2bpgceeEDVqlWTYRgyDEPNmzfXzJkzdcstt1gXNQAAAHCNMr0PuoeHh5o1a1bgmKenp9566y2NGTNGaWlpqlatmry9vcscJAAAAK4uVNDNM52gX4qbm5vq1KlTHksDAAAA17RySdABAABQsVUSFXSznErQT5w4oaVLl5q+WN++fU2fCwAAAFQETiXoBw8e1CuvvGLqQjabjQQdAACggqAH3TynEvSLd3FxVlnOBQAAwNWlEgm6aU4l6Lfccovmz59fXrEAAAAAFR43iQIAAMByNm4SNc2pBxUBAAAAKF9U0AEAAGC5SjbqwGbxyQEAAAAuhAo6AAAALMc2i+aVOkGPj48vzzgAAAAAiAo6AAAAygG7uJhHgg4AAADL8aAi87hJFAAAAHAhVNABAABgOVpczKOCDgAAALgQKugAAACwHD3o5lFBBwAAAFwIFXQAAABYzmajDmwWnxwAAADgQqigAwAAwHLs4mIeCToAAAAsx02i5tHiAgAAALgQKugAAACwnI0KumlU0AEAAAAXQgUdAAAAlqvETaKmUUEHAABAhZKTk6OZM2fqL3/5i1q3bq3bbrtNzzzzjOLi4sq89tSpUxUUFKSgoCAtWLDA1BpU0AEAAGA5V+1Bz8nJ0eDBgxUdHa3atWurW7duOn78uNasWaMNGzZoxowZ6tSpk6m19+zZo5kzZ8pms8kwDNMxUkEHAABAhTFr1ixFR0crJCREq1ev1uTJk/Xll1/qgw8+UG5urkaNGqXMzEyn183Pz9err74qHx8fde/evUwxkqADAADAcjZbpXJ7mZWXl6d58+ZJksaMGSNvb2/HWO/evdWlSxelp6dr0aJFTq/92Wef6bffftNrr72mGjVqmI5RIkEHAABAOagkW7m9zNq+fbsyMjLk7++vkJCQQuN33XWXJCkqKsqpdRMSEjR16lSFhYUpIiLCdHx2JOgAAACoEHbv3i1JCg4OLnK8ZcuWki70kpeWYRh67bXX5OHhoTFjxpQ9SJGgAwAAoBzYbLZye5mVnJwsSapXr16R4/bjGRkZOnPmTKnWnD9/vmJjYzVixAjVrVvXdGwXYxcXAAAAXFXCwsIuOV5ci0pWVpYkqVq1akWOe3p6Ov595swZeXl5XfI6hw8f1gcffKA2bdrooYceuuRcZ5CgAwAAwHK2CvCgojfeeEO5ubl6++23Ld1WkgQdAAAAVxVnb+K0s1fIs7Ozixy3V9gllVg9X7RokTZv3qynn35aN954o6l4ikOCDgAAAMu54oOKGjRoIElKSUkpctx+3MfHp8QE3f4lYcuWLYqJiSkw9scff0iSIiMjtXLlSrVt21bDhw8vdZwk6AAAAKgQWrRoIUmKi4srcnzXrl2SpKCgoFKv+euvvxY7duDAAR04cEDVq1d3IkoSdAAAAJSDsuxXXl7atm0rHx8fJSUlaefOnYX2Ql+5cqWkkm9ClaSPPvqo2LHRo0dryZIlevPNN/Xggw86HSfbLAIAAKBCcHd314ABAyRJY8eOVWZmpmNs+fLl2rhxo3x9fdW/f3/H8R07digiIsKSBxCVOs7LdiUAAABUGDaba9aBhwwZoq1btyo6Olo9e/ZUu3btlJqaqtjYWHl4eGj8+PHy9vZ2zM/OzlZCQsJljdE1PzkAAABc1Wzl+L+yqFy5smbPnq3hw4fLx8dH69at0759+xQWFqaFCxeqc+fOFn0C5tkMwzCudBCubHOKuW18AKA4HeuFKejDy/enUgAVw57hq650CAV8tf+Lclv7gaaPlNvaroAWFwAAAFjOFbdZvFrQ4gIAAAC4ECroAAAAsFxZe8UrMiroAAAAgAuhgg4AAADL0YNuHhV0AAAAwIVQQQcAAIDlKtGDbhoJOgAAACxHi4t5tLgAAAAALoQKOgAAACxnow5sGp8cAAAA4EKooAMAAMBy9KCbRwUdAAAAcCFU0AEAAGA5G9ssmkYFHQAAAHAhVNABAABguUr0oJtGgg4AAADL0eJiHi0uAAAAgAuhgg4AAADLsc2ieVTQAQAAABdCBR0AAACWs1EHNo1PDgAAAHAhVNABAABgOXrQzaOCDgAAALgQKugAAACwXCX2QTeNBB0AAACWo8XFPFpcAAAAABdCBR0AAACWs9HiYhoVdAAAAMCFUEEHAACA5ehBN48KOgAAAOBCqKADAADAcjbqwKbxyQEAAAAuhAo6AAAALFeJHnTTSNABAABgObZZNI8WFwAAAMCFUEEHAACA5dhm0Twq6AAAAIALoYIOAAAAy9GDbh4JOlCMdUs2atW/1+hk2ik1auqvh55/QE1a3FDk3J9//EUrvvhBxw4fV35evur6X6eeD4Tp9jvbS5Ly8vK15NPvtHNrnI4fSVU1r2pqeUuQ+g/tK18/n8v4rgBcaQ/d1EeDb7lPdbx8FX/8D/1z/UfaeXRvsfOrV/HS8NsfV49md8inircOnz6mdzfM1I8HYiRJlWyV9Oxtj+juFt3l5+WrY5kntGTXWn207cvL9ZYAWIwEHShC9LpYLZy+SI+OeFBNWt6gNV+v04cvTtU7X7ypGr7VC833qu6l3o9EqF7junL3cNd/f9qpOe9/rhq+1dXq1pbKOZujQ3sT1WdALzW60V9nTmdpwdSvNfXVj/XGJ6Mv+/v7v/buParqMu3/+GcjoG5AIDyCJmJJakyKylTjIWVSM2omrXFcjpZjakYe8lTW8zzazFP+zOmxsZU6HglrNTN5ihDFA0JNHhjT8YSiFCgioqBoHJTT/v3BsEfiIHu7kS/6frlaC7/39773tfda6bUvr/v+AmgYBL3gOwAAHvRJREFUT3Xpr7n9J2jero90+EKyXgz+tVYPf1dDI17W5cKrVe53cXLW2uELlFOQq2nR/6usvBz5erTWtRt51nsm9H5Box55Wm/EfqCUnDN6uM2DWjB4hn68ka91//ryTr49oBJ60O131/agL126VG+99VZDh4FGavvf49Q/7BfqO+wx+fq305iZo+TazFX/iNlT7f0P9eyi4P495OvfTq39WunJ5wepfYCfTh/9XpJkdm+umf83VX0G9VLb+9uoc/dOGj3tNzqTfFY5WZfv5FsD0IDGBQ/X349t08akHfr+8lnN2/mRrpfc0IiHh1R7/4iHB8uzmbvCv3pHB88nKeNalv6ZcVTJ2anWe3r6dtOu7/cpITVRGdeyFHv6H/rHmYP6WdvAO/W2ADjYXZugJyQkaNOmTQ0dBhqhkuISnTl1Vl17/ecvNycnJ3Xr9ZC+P55ay8xyFotFSd+d1IX0LHX52QM13leYf10mk0lm9+YOiRuAsbk4Oat7mwe15+wh6zWLLNpz9pB6tuta7ZxBAY/qX5kn9T+DwvXtxM/11ZjlmtRnpJxM//nr+9D5JD3aoYf8vfwkSYEtO6mXb3drCwzQUJzq8dfdjhYX4Cd+vJqnstIytfBuUel6C28PZZ7NqnFeQV6hZj3/lkqKimVq4qTfTf+tuvep/i/d4hvFWv+XTQoJ7a3mbiTowL3Au3kLOTs1UU5BbqXrOQW5CvDuUO2cDp7t9GiHNvrq5G5N3Pzfut/LV/MGvSbnJs76eN9nkqQV//y73JuatfWllSotK1MTJyct/vYTfXVyd72/J6A2tLjYz/AJ+vnz5+2aV1RU5OBIgNo1MzfVvFVzdaPwhk4cTNbflm5QK9+Weqhnl0r3lZSUatn8VbJYpDEzfttA0QJoDEwmk3IKcvXfO/+sMkuZjl9MURv3lhrf+3lrgv5Ul/565qFBmhmzUCk5Z9S1dWfNHTBJF/NztDlpZwO/AwD2MHyCPmjQILu+gVksFr65wS4enu5yauKka1euVbp+7cqP8ryvRQ2zyttg2rRvLUm6/8EOyjxzQTGfxVZK0EtKSrV83irlZF3W7MXTqJ4D95ArhddUUlYqH3Plk5t8zF7KLrhS7ZxL+ZdVUlaqMkuZ9doPl8+qtdt9cnFyVnFZieb0f1kr/vl3xZxKkCSdykmTr0drTeozkgQdDcrIxywWFRVp7dq1ioqKUnp6usxms3r37q3Jkyere/fudV5nz5492rJli5KSkpSVlaVr166pWbNmeuCBBxQWFqaRI0fKxcXF5vgMn6BX8PHxsen+3NxclZaW1lM0uJs5uzirY5f7deK7ZAX36yFJKisr04mDyRr03IA6r1NWZlFJcYn19xXJeVbGRc35cLrcPd0dHjsA4youK9HxrNN6rEMP7fp+r6TyBOaxDj306eGvqp1z8HySwgIHyiSTLLJIkvy9/XQxL0fFZeV/vjRzbirLTQm8JJVayihSATUoKirS+PHjlZiYKB8fHw0cOFCXLl3Sjh07FB8fr2XLlqlfv351Wmvbtm1av369/P391bVrV3l6eio7O1sHDx7UoUOHtGXLFkVERKhp06Y2xWj4BN3X11eZmZnauHGjWrduXed5I0eO1JEjR+oxMtzNBv9mkFYviJT/Qx3V6aGO2rl+t24U3tAvnnpMkrTq3Qh5t/LSiIm/liRt+XSb/AM7qrVfKxUXFevo/uPat32/fjdjlKR/t7X8z0qdOXVW0/7fqyorLdPVnPIj1dxauMnZxfD/KwJwgLUHN2rhkFk6dvG0jlxI1os9n1Nzl2baeHy7JGnhkFnKysvR/327VpL0+eFo/e6RZ/T2E6/o039FqaO3nyb1+W2l4xN3/7Bfr4T8Vud/vFTe4tKqs8YFP6cN/14TaChG/ZK4cuVKJSYmKigoSBEREXJ3Ly+YRUdHa+bMmZo9e7Z27txpvV6b0aNHa8qUKWrVqlWl61lZWRo3bpwOHjyoyMhITZgwwaYYDZ8VBAUFKTMzU8ePH7cpQQduR8ig3voxN0+b10Tr2uVr6vBAe72+6DVri8vli1dkcvrPLvIb14v06eK/6sqlXLk0dVG7+9vo5f96SSGDekuSci/l6l/fln9hnD/+vUqvNfvD6VX61AHcnbae+lr3NffU1MfGqJXZWycu/aCXN/2XdeNoO4/WKrNYrPdfyMvW+E3/pbkDJipqzDJl5WUr8tBmrTzwhfWe/929VNMeH6t5g8LlY/bSxbwc/e3oVmuPOoD/KCkpUWRkpCRp3rx5lZLwsLAwRUVFKSEhQRs2bNCLL754y/UCA6s/zrRNmzaaOHGi3njjDe3du/fuTNBjY2N19OhRDRw4sM7zLDf9AQfYI3T4Ewod/kS1Y3P+/Hql3w9/+VkNf/nZGtdq2c5HqxOWOjI8AI3UZ4e/0mc1tLSMXT+nyrV/ZZ7QyL++Xs3d5fKLC/Vewl/0XsJfHBYj4AhG7EE/ePCgcnNz1b59ewUFBVUZHzZsmBISErRr1646Jei1qeg9d3V1tXmu4RP0xx9/XKGhoTKbzTbNCw8P1+XLPAAGAAAA5U6cOCFJNW4E7datmyQpOTn5tl7nypUrWr16tSRpwIC671+rYPgEvVu3bvr4449tnmfPhwEAAADHMGIFveL47rZt21Y7XnE9NzdX+fn5cnNzq9O6hw4d0t/+9jeVlZVZN4kWFhbqhRde0G9+8xub4zR8gg4AAIBGqB43iYaGhtY6vmvXrmqvFxQUSJKaN6/+mOObOzZsSdDPnj1b5Qn2Y8eO1bRp09SkSZM6rXGzu/9ZqQAAAEA9+tWvfqXk5GQdO3ZM27dv1+uvv67169frueee05kzZ2xejwo6AAAAHK4+W1xqqpDfSkWFvLCwsNrxigq7pDpXz2/m4uKijh076pVXXlG7du00Z84czZ8/X2vXrrVpHSroAAAAuCf4+vpKki5cuFDteMV1Ly8vuxL0mw0bNkyurq7au3dvpcS/LkjQAQAA4HAmk6ne/rNX165dJUnHjx+vdjwpKUlSzeeb28LFxUUeHh6yWCy6cuWKTXNJ0AEAAHBPCA4OlpeXl86dO6ejR49WGY+JiZF0602odZGSkqKcnByZzeYqTxq9FRJ0AAAAOJypHn/Zy9nZWWPHjpUkvfPOO8rLy7OORUdHKyEhQd7e3hoxYoT1+pEjRzR06FANHTq00loFBQWKjIystEaF5ORkzZo1S5L07LPP2vywIjaJAgAA4J4xYcIE7du3T4mJiRo8eLD69Omj7OxsHThwQC4uLnr//ffl7u5uvb+wsFCpqalV1ikpKdG7776rRYsWqVu3bvL19VVJSYkyMjKUlJQki8WikJAQzZlT9QnBt0KCDgAAAIcz4oOKJMnV1VWrV6/WmjVrFBUVpbi4OJnNZoWGhio8PLzGp4z+lNls1ty5c5WYmKhTp07p1KlTKi4ulpeXl/r376+wsDCFhYXJycn2hhWTxWKx2DzrHvKPC/Yd4wMANenbNlSBi4fe+kYAsEHy69saOoRKDl/+Z72t/ch9feptbSOgBx0AAAAwEFpcAAAA4HBGbXFpDKigAwAAAAZCBR0AAAAORwXdflTQAQAAAAOhgg4AAACHM5mooNuLCjoAAABgIFTQAQAA4HD0oNuPCjoAAABgIFTQAQAA4HD0oNuPBB0AAAAOR4uL/WhxAQAAAAyECjoAAAAcjgq6/aigAwAAAAZCBR0AAAAOxyZR+1FBBwAAAAyECjoAAAAcjh50+1FBBwAAAAyECjoAAAAcjgq6/UjQAQAA4HBsErUfLS4AAACAgVBBBwAAQD2ggm4vKugAAACAgVBBBwAAgMPRg24/KugAAACAgVBBBwAAgMNxzKL9qKADAAAABkIFHQAAAA5HBd1+JOgAAABwODaJ2o8WFwAAAMBAqKADAADA4WhxsR8VdAAAAMBAqKADAADA4aig248KOgAAAGAgVNABAADgcJziYj8q6AAAAICBUEEHAACAw9GDbj8SdAAAADgcLS72o8UFAAAAMBAq6AAAAHA4WlzsRwUdAAAAMBAq6AAAAKgHVNDtRQUdAAAAMBAq6AAAAHA46uf2I0EHAADAPaWoqEhr165VVFSU0tPTZTab1bt3b02ePFndu3ev8zrHjh1TfHy8vv32W6WkpKigoEDe3t4KDg7WSy+9pODgYLviI0EHAACAwxn1HPSioiKNHz9eiYmJ8vHx0cCBA3Xp0iXt2LFD8fHxWrZsmfr163fLdUpKSjRixAhJkoeHhx555BF5eHgoJSVFsbGx2rFjh9566y2NGTPG5hhJ0AEAAFAPjJmgr1y5UomJiQoKClJERITc3d0lSdHR0Zo5c6Zmz56tnTt3Wq/X5uGHH9akSZM0cOBAubi4WK9//vnnmj9/vhYsWKDHH39cnTt3tilGNokCAADgnlBSUqLIyEhJ0rx58yol4WFhYRowYICuXLmiDRs23HItZ2dnbdiwQYMHD66UnEvSqFGj1LdvX5WWlmrr1q02x0mCDgAAAIcz1eN/9jp48KByc3PVvn17BQUFVRkfNmyYJGnXrl238SrlAgMDJUkXL160eS4JOgAAAO4JJ06ckKQaN4J269ZNkpScnHzbr3X27FlJUsuWLW2eS4IOAACAemC8Gvr58+clSW3btq12vOJ6bm6u8vPz7X6d1NRUxcfHS5JCQ0Ntns8mUQAAADQqt0p6a2pRKSgokCQ1b9682nGz2Wz9OT8/X25ubjbHVlRUpDfeeEPFxcUKCwuz6djGCiToAAAAcDijHrNY3+bNm6fDhw/L399f8+bNs2sNEnQAAAA0KvZu4qyokBcWFlY7XlFhl2RX9XzRokXauHGj2rZtqzVr1qhFixZ2xUkPOgAAAO4Jvr6+kqQLFy5UO15x3cvLy+YEffny5Vq1apXuu+8+rVmzRn5+fnbHSQUdAAAADmcy4IOKunbtKkk6fvx4teNJSUmS/nNEYl2tW7dOixcvloeHh1avXm3zg4l+igo6AAAA7gnBwcHy8vLSuXPndPTo0SrjMTExkmw7eWXTpk169913ZTabtWLFCutRjbeDBB0AAAAOZ6rHX/ZydnbW2LFjJUnvvPOO8vLyrGPR0dFKSEiQt7e3RowYYb1+5MgRDR06VEOHDq2y3vbt2/X222/L1dVVS5cuVXBwsN2xVYrTIasAAAAAjcCECRO0b98+JSYmavDgwerTp4+ys7N14MABubi46P3335e7u7v1/sLCQqWmplZZJycnRzNmzFBpaan8/f315Zdf6ssvv6xyX0BAgCZOnGhTjCToAAAAuGe4urpq9erVWrNmjaKiohQXFyez2azQ0FCFh4fX+dzywsJCFRcXS5K+//57ff/999XeFxISYnOCbrJYLBabZtxj/nHBvmN8AKAmfduGKnBx1X8qBYDbkfz6toYOoZLs69WflOIILZtV/yTQuwUVdAAAADjcvfqgIkdgkygAAABgICToAAAAgIHQ4gIAAACHM+KDihoLKugAAACAgVBBBwAAQD2ggm4vKugAAACAgVBBBwAAgMNRP7cfFXQAAADAQKigAwAAwOF4UJH9qKADAAAABkIFHQAAAPWACrq9SNABAADgcKTn9qPFBQAAADAQKugAAACoB9TQ7UUFHQAAADAQKugAAABwOI5ZtB8VdAAAAMBASNABAAAAAyFBBwAAAAyEHnQAAAA4nIlTXOxGBR0AAAAwECroAAAAqAdU0O1Fgg4AAACHIz23Hy0uAAAAgIFQQQcAAIDD8aAi+1FBBwAAAAyECjoAAADqARV0e1FBBwAAAAyECjoAAAAcjvq5/aigAwAAAAZCBR0AAAD1gBq6vUjQAQAA4HAcs2g/WlwAAAAAAyFBBwAAAAyEBB0AAAAwEHrQAQAA4HAmNonazWSxWCwNHQQAAACAcrS4AAAAAAZCgg4AAAAYCAk6AAAAYCAk6AAAAICBkKADAAAABkKCDgAAABgICToAAABgICToAAAAgIGQoAMAAAAGQoIOAAAAGAgJOgAAAGAgJOgAAACAgZCgAwAAAAbi3NABAI1ZUVGR1q5dq6ioKKWnp8tsNqt3796aPHmyunfv3tDhAWiEjh8/rj179ujo0aM6duyYMjIyJEm7du1S+/btGzg6AHcCCTpgp6KiIo0fP16JiYny8fHRwIEDdenSJe3YsUPx8fFatmyZ+vXr19BhAmhkPv74Y+3atauhwwDQgEjQATutXLlSiYmJCgoKUkREhNzd3SVJ0dHRmjlzpmbPnq2dO3darwNAXfTo0UNdunTRww8/rKCgIA0fPlzZ2dkNHRaAO4gEHbBDSUmJIiMjJUnz5s2rlISHhYUpKipKCQkJ2rBhg1588cWGChNAIzRx4sSGDgFAA2OTKGCHgwcPKjc3V+3bt1dQUFCV8WHDhkkS/0wNAABsRoIO2OHEiROSVONG0G7dukmSkpOT71hMAADg7kCCDtjh/PnzkqS2bdtWO15xPTc3V/n5+XcsLgAA0PiRoAN2KCgokCQ1b9682nGz2Wz9mQQdAADYggQdAAAAMBASdMAOFRXywsLCascrKuyS5ObmdkdiAgAAdwcSdMAOvr6+kqQLFy5UO15x3cvLiwQdAADYhAQdsEPXrl0llT+SuzpJSUmSpMDAwDsWEwAAuDuQoAN2CA4OlpeXl86dO6ejR49WGY+JiZEkhYaG3unQAABAI0eCDtjB2dlZY8eOlSS98847ysvLs45FR0crISFB3t7eGjFiREOFCAAAGimTxWKxNHQQQGNUVFSk8ePHKzExUT4+PurTp4+ys7N14MABubi4aOnSperfv39DhwmgkYmPj9fSpUutv09KSlJxcbG6du0qV1dXSdKAAQMUHh7eUCECqGfODR0A0Fi5urpq9erVWrNmjaKiohQXFyez2azQ0FCFh4fX+JRRAKjN5cuXdfjw4SrXK55gLEkBAQF3MiQAdxgVdAAAAMBA6EEHAAAADIQEHQAAADAQEnQAAADAQEjQAQAAAAMhQQcAAAAMhAQdAAAAMBASdAAAAMBASNABAFYzZsxQYGCgFixY0NChAMA9iwQdAG7D/v37FRgYqEGDBlUZGzNmjAIDA7Vx48YGiEzauHGjAgMDNWbMmDrdv3nzZm3ZskX9+vXTnDlz6jk6AEBNnBs6AAC42ZgxY5SYmFjpmpOTkzw8PBQQEKDQ0FCNHj1aZrO5gSK8O6Wnp+sPf/iDAgICtHjxYjVp0qShQwKAexYJOgBDateundq1aydJKikpUXp6ug4dOqRDhw5p/fr1ioyMVJs2bRo4ytq1a9dOnTp1koeHR4O8voeHhzp16mT9HGtSWlqq2bNny9nZWcuXL2+weAEA5UjQARjSiBEjNGXKlErXYmNj9eabbyotLU3z58/XsmXLGii6unn//fcb9PWffPJJPfnkk7e8Ly0tTb/4xS80Z84cdezY8Q5EBgCoDT3oABqNIUOGaPLkyZKk+Ph4Xb16tYEjujt07txZU6ZMUXBwcEOHAgAQFXQAjcxjjz0mSSorK9OZM2dUWFiosWPHys/PT3FxcYqOjtZf//pXnTp1SlevXlVkZKR+/vOfSypv5di8ebOioqJ08uRJ5efny9vbWyEhIZowYYIeeuihal+zuLhYERER2rx5s86ePSsPDw/17t1b4eHhtcZa0U+/YMECDR8+vMr4tWvX9Omnn2r37t1KS0vT9evX1apVKwUGBmrIkCH69a9/XWXOxYsXFRkZqW+++Ubp6ekqLS1VmzZt1L17dz3zzDOVNqtu3LhRc+fOVUhIiNatW1dlrYKCAn366afatm2b0tLSVFpaqnbt2mnAgAEaP368WrduXWXOm2++qU2bNum1117ThAkTtGLFCm3ZskXnz5+Xm5ubHn30UU2fPl3+/v61fjYAgJqRoANoVCwWS41j7733nj755BO1bNlS999/v7KysqxjV69e1auvvqoDBw5Iklq3bi1fX1+dOXNG0dHRio2N1cKFC/X0009XWrOoqEiTJk3Snj17JEnt27eXp6en4uPjlZCQcMskvSbHjh3TK6+8okuXLkmSOnbsKA8PD2VmZiouLk5xcXFVEvSvv/5ar7/+uvLy8uTk5KROnTqpWbNmysjIUExMjA4fPlztaTLVycrK0u9//3ulpKTIZDIpICBATZs21enTp61fRlasWKFHHnmk2vl5eXkaOXKkkpOTFRAQoI4dOyo1NVVbt27V3r17tXHjRvn5+dn12QDAvY4EHUCjsm/fPknlJ7t07NhRJ0+elCRduHBBn3/+uRYtWqRnnnlGJpNJFotFxcXFkqRZs2bpwIED6tWrl+bPn68uXbpIKq/ER0ZGauHChZo7d666deumTp06WV9v6dKl2rNnj9zc3LRkyRL17dtXUnnC/8Ybb2jJkiU2v4fs7GxNmjRJ2dnZCgkJ0R//+MdKFeeMjAytX7++0pyUlBRNnTpVhYWFGjJkiN5+++1Km2RTUlIUFxdX5xhmzZqllJQU+fv766OPPrJ+HtnZ2Zo5c6b27dunKVOmaMuWLdVuGv3ss8/UpUsXxcbGWvvW09PTNWHCBKWmpmrJkiVauHChLR8LAODf6EEH0GjExsZaN4Y+8cQT8vT0tI6VlpYqPDxczz77rEwmkyTJZDLJ1dVVe/bs0ddffy1fX18tX77cmoxK5Yn+Sy+9pNGjR+vGjRv65JNPrGMFBQXW1pBp06ZZk3NJ8vT01AcffGDXcY+rVq1Sdna2OnXqpJUrV1ZpB/Hz89O0adMqXfvzn/+swsJChYSE6MMPP6xygs0DDzygiRMn1un1Dxw4YD3KctGiRZU+j5YtW2rJkiVyd3dXVlaWvvjii2rXMJlM+vDDDyttKu3QoYNmzJghSdq9e3edYgEAVEWCDsCQNmzYoFGjRmnUqFF64YUX9Oijj2rq1KkqKCiQv7+/5s+fX2XOCy+8UO1aMTExkqSnn35aLVq0qPaewYMHS5L27t1rvfbdd98pLy9PzZo1q3ZtNzc3Pf/887a+NW3fvl2SNG7cODVr1uyW99+4cUPx8fGSpEmTJsnJ6fb+6K5Yq1evXvrZz35WZdzT09P6viru/am+ffvq/vvvr3K9R48eksr/hSE3N/e24gSAexUtLgAMKTMzU5mZmZLKq9zu7u7q2bNnjQ8q8vb2lo+PT7VrVbTB7NixQ999912199y4cUNSeatMhR9++EFSeUW7pkr5gw8+aMO7Ku/dzsjIkCT17NmzTnPS0tJUVFRk05zapKamSqo99oqqesVn8FM1bQJt2bKl9ef8/Hx5eXnZGSUA3LtI0AEY0muvvVblHPTa1NZqcu3aNUnliW5aWlqt61y/ft36c35+viTVmPjfaqw6FWtKqrGa/1N5eXmSpCZNmsjNzc2m16sthlatWtV4T8XYzfHerKbP++bqfm0begEANSNBB3DXq0gm33vvPY0YMaLO8yqS4ZycnBrvqW2stjWl8i8Obdu2veUcd3d3SeV99vn5+bedpFfMrzhBpjoVY474QgAAsA096ADuehXtGsnJyTbNCwgIkFR+qkphYWG195w+fdqmNd3d3a3HDx46dKhOczp16qSmTZvaNKc2Fe+rtthPnTolqfwhRgCAO4sEHcBd76mnnpIkffnll8rOzq7zvF69esnNzU3Xr1+vcuyhVN7+sWHDBpvjGTJkiCQpIiLC2vteG1dXVz3xxBOSpBUrVtx268iAAQMklW+CPXLkSJXxa9euWd9XxesCAO4cEnQAd72BAweqb9++ys3N1dixY60PK7pZenq6Vq5cWelYQbPZrDFjxkgqP+aw4mFFUnkSO3v27Bp7tGvz8ssvq2XLlvrhhx80ceJEnTlzptJ4RkZGlfPVp02bpubNm2v//v2aMWOGLl68WGk8JSVFK1asqNPr9+7dWyEhIZKk2bNnV6qk5+TkaPr06frxxx/Vpk0bu06pAQDcHnrQAdwTFi9erGnTpmnPnj0aPXq0fHx85Ovrq7KyMmVmZury5cuSyjen3uzVV1/VoUOHtH//fo0bN04dOnSQp6enUlJSJElTp07VBx98YFMsPj4+Wr58uSZPnqx9+/Zp8ODB8vf3l7u7uy5cuGCt8k+dOtU6p3PnzlqyZImmT5+umJgYbdu2zfr0z4yMDOXm5srPz6/OZ6H/6U9/sj5J9JlnnlHnzp3l6uqq06dPq7i4WF5eXvroo4+qfUgRAKB+kaADuCe0aNFCq1ev1vbt2xUVFaUjR47o5MmTatKkiVq3bq3HH39cgwYNsrZ/VGjatKlWrVqliIgIbdq0SefOnVN+fr769++v1157ze6zvoOCghQdHa1169YpLi5OaWlpyszMVKtWrfTLX/7S2gZzs/79+2vr1q1au3atvvnmG2VkZMhkMqlVq1bq27evnn322Tq/fps2bfTFF19o3bp1io2NVWpqqkpLS+Xn56cBAwZo/PjxVR6GBAC4M0wWzsECAAAADIMedAAAAMBASNABAAAAAyFBBwAAAAyEBB0AAAAwEBJ0AAAAwEBI0AEAAAADIUEHAAAADIQEHQAAADAQEnQAAADAQEjQAQAAAAMhQQcAAAAMhAQdAAAAMBASdAAAAMBASNABAAAAA/n/dUbFeKYA2lEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "LGBM = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(LGBM, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accu: %.3f (%.6f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "#Fit\n",
    "LGBM = LGBMClassifier()\n",
    "LGBM.fit(X_train, y_train)\n",
    "\n",
    "#Predicciones\n",
    "y_pred = LGBM.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "#Entrenamiento\n",
    "alg = XGBClassifier(learning_rate=0.1, n_estimators=20, max_depth=5,\n",
    "                    min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,\n",
    "                    objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "alg.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "#Predicciones\n",
    "y_pred = alg.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Random Forest Classifier - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train\n",
    "RFC = RandomForestClassifier(n_estimators=500, max_depth=131, class_weight = 'balanced',\n",
    "                             criterion='entropy', max_features='auto', n_jobs=-1)\n",
    "\n",
    "\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = RFC.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n",
    "#Features importances\n",
    "#features = np.array(X.columns)\n",
    "importances = RFC.feature_importances_\n",
    "\n",
    "'''\n",
    "\n",
    "#Buscando parámetros 'buenos'\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 400],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,5,7,9,11, 13],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Grid Search train\n",
    "GSCV = GridSearchCV(estimator=RFC, param_grid=param_grid, cv= 5)\n",
    "GSCV.fit(X_train, y_train)\n",
    "\n",
    "# Best paramns\n",
    "print(GSCV.best_params_)\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regresión Logística - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "#Entrenamiento\n",
    "\n",
    "LR = LogisticRegression(C=2, n_jobs=-1)\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Métricas\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "resultados(y_pred, test_df)\n",
    "\n",
    "#Feature estimator\n",
    "eli5.show_weights(estimator=LR,feature_names= list(count_vect.get_feature_names()),top=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KNN - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "#Train\n",
    "KNN = KNeighborsClassifier(n_neighbors = 79, metric='minkowski')\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "#Métricas\n",
    "y_pred = KNN.predict(X_test)\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n",
    "\n",
    "#Buscando un K 'bueno'\n",
    "k_range = range(1,100, 3)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for k in k_range:\n",
    "    KNN = KNeighborsClassifier(n_neighbors = k, metric='minkowski')\n",
    "    KNN.fit(X_train, y_train)\n",
    "    scores.append(KNN.score(X_test, y_test))\n",
    "   # accuracy = metrics.accuracy_score(y_test, y_pred) #Accuracy\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multinomial NB - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = MNB.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
