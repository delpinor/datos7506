{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import emoji\n",
    "from num2words import num2words\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, Conv1D\n",
    "from keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GRU, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown,words\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.decomposition import PCA\n",
    "import tokenization\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import KeyedVectors\n",
    "import eli5\n",
    "import bert\n",
    "import re, string\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "pd.options.display.float_format = '{:20,.10f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CODE_LOCATION = {'al': 'alabama',\n",
    "  'ak': 'alaska',\n",
    "  'az': 'arizona',\n",
    "  'ar': 'arkansas',\n",
    "  'ca': 'california',\n",
    "  'co': 'colorado',\n",
    "  'ct': 'connecticut',\n",
    "  'de': 'delaware',\n",
    "  'dc': 'district of columbia',\n",
    "  'fl': 'florida',\n",
    "  'ga': 'georgia',\n",
    "  'hi': 'hawaii',\n",
    "  'id': 'idaho',\n",
    "  'il': 'illinois',\n",
    "  'in': 'indiana',\n",
    "  'ia': 'iowa',\n",
    "  'ks': 'kansas',\n",
    "  'ky': 'kentucky',\n",
    "  'la': 'louisiana',\n",
    "  'me': 'maine',\n",
    "  'md': 'maryland',\n",
    "  'ma': 'massachusetts',\n",
    "  'mi': 'michigan',\n",
    "  'mn': 'minnesota',\n",
    "  'ms': 'mississippi',\n",
    "  'mo': 'missouri',\n",
    "  'mt': 'montana',\n",
    "  'ne': 'nebraska',\n",
    "  'nv': 'nevada',\n",
    "  'nh': 'new hampshire',\n",
    "  'nj': 'new jersey',\n",
    "  'nm': 'new mexico',\n",
    "  'ny': 'new york',\n",
    "  'nc': 'north carolina',\n",
    "  'nd': 'north dakota',\n",
    "  'oh': 'ohio',\n",
    "  'ok': 'oklahoma',\n",
    "  'or': 'oregon',\n",
    "  'pa': 'pennsylvania',\n",
    "  'ri': 'rhode island',\n",
    "  'sc': 'south carolina',\n",
    "  'sd': 'south dakota',\n",
    "  'tn': 'tennessee',\n",
    "  'tx': 'texas',\n",
    "  'ut': 'utah',\n",
    "  'vt': 'vermont',\n",
    "  'va': 'virginia',\n",
    "  'wa': 'washington',\n",
    "  'wv': 'west virginia',\n",
    "  'wi': 'wisconsin',\n",
    "  'wy': 'wyoming',\n",
    "  'as': 'american samoa',\n",
    "  'gu': 'guam',\n",
    "  'mh': 'marshall islands',\n",
    "  'fm': 'micronesia',\n",
    "  'mp': 'northern marianas',\n",
    "  'pw': 'palau',\n",
    "  'u.s.a': 'united states',\n",
    "  'usa': 'united states',\n",
    "  '304': 'west virginia',\n",
    "  'd.c': 'district of columbia',\n",
    "  'd.c.': 'district of columbia',                 \n",
    "  'us': 'united states',\n",
    "  'ny': 'new york',\n",
    "  'nyc': 'new york',\n",
    "  'uk': 'united kingdom',\n",
    "  'u.k': 'united kingdom',\n",
    "  'u.k.': 'united kingdom',\n",
    "  'bc': 'british columbia',\n",
    "  'ab': 'alberta',\n",
    "  'vi': 'virgin islands'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Imprimir metricas de las predicciones\n",
    "def mostrar_metricas(y_test, y_pred):\n",
    "    print('Reporte de clasificación: \\n', classification_report(y_test, y_pred))\n",
    "    print('Matriz de confusión: \\n',confusion_matrix (y_test, y_pred))\n",
    "    print('ROC: \\n', metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    # Obtener y reformar la matriz de datos de \n",
    "    matrix = confusion_matrix (y_test, y_pred) \n",
    "    matrix = matrix.astype ('float') / matrix.sum (axis = 1) [:, np.newaxis] \n",
    "\n",
    "    # Build the plot\n",
    "    plt.figure()\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('Matriz de confusión')\n",
    "    plt.show()\n",
    "    \n",
    "def grafico_pie(df, titulo, valores, etiquetas):\n",
    "    fig = px.pie(df, values=valores, names=etiquetas)\n",
    "    fig.update_layout(title_text=titulo,\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "    \n",
    "def grafico_distr(df, columna, titulo, xtitulo, ytitulo):\n",
    "    x1 = df.loc[df['target'] == 1][columna]\n",
    "    x2 = df.loc[df['target'] == 0][columna]\n",
    "    group_labels = ['Verdadero', 'Falso']\n",
    "    colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n",
    "    fig = ff.create_distplot([x1, x2], group_labels,colors=colors)\n",
    "    fig.update_layout(title_text=titulo,\n",
    "                      xaxis_title=xtitulo,\n",
    "                      yaxis_title=ytitulo,\n",
    "                      template=\"plotly_white\")\n",
    "    fig.show()\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def resultados(pred, test_df):\n",
    "    res_df=pd.DataFrame(test_df['id'])\n",
    "    res_df['target']=pred\n",
    "    res_df.to_csv('data/submission.csv', index=False)\n",
    "    \n",
    "def dict_vocabulario(x):\n",
    "    tweets = x.apply(lambda s: s.split()).values\n",
    "    vocab = {}\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1 #Si ya existe en el diccionario\n",
    "            except KeyError:\n",
    "                vocab[word] = 1  #Si no existe\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def porcentaje_cobertura(x, embeddings):\n",
    "    vocab = dict_vocabulario(x)\n",
    "    cubiertos = {}\n",
    "    no_cubiertos = {}\n",
    "    cant_cubiertos = 0\n",
    "    cant_no_cubiertos = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            cubiertos[word] = embeddings[word]\n",
    "            cant_cubiertos += vocab[word]\n",
    "        except:\n",
    "            no_cubiertos[word] = vocab[word]\n",
    "            cant_no_cubiertos += vocab[word]\n",
    "\n",
    "    palabras_cubiertas_pct = len(cubiertos) / len(vocab)\n",
    "    texto_cubierto_pct = (cant_cubiertos/ (cant_cubiertos + cant_no_cubiertos))\n",
    "    return no_cubiertos, palabras_cubiertas_pct, texto_cubierto_pct\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "#punctuation = list(string.punctuation)\n",
    "punctuation = '!\"$%&\\()*+,-/:;<=>?[\\\\]“”^_`#{|}~’'\n",
    "\n",
    "\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "\n",
    "def unir_texto(text):\n",
    "    return ( ' '.join(text))\n",
    "\n",
    "def eliminar_palabras_con(text, con):\n",
    "    palabras = []\n",
    "    for word in text.split():\n",
    "        if con not in word:\n",
    "            palabras.append(word)\n",
    "    return unir_texto(palabras)\n",
    "\n",
    "\n",
    "# Tipo de palabra\n",
    "#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Lematizar palabras\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Elimino stops words \n",
    "def eliminar_sw(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip() not in stops: \n",
    "            #pos = pos_tag([i.strip()])\n",
    "            #word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "            final_text.append(i.strip())\n",
    "    return unir_texto(final_text)\n",
    "\n",
    "def lematizar(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        pos = pos_tag([i.strip()])\n",
    "        word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "        final_text.append(word)\n",
    "    return unir_texto(final_text)\n",
    "\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "def eliminar_nums(text):\n",
    "    cadena = []\n",
    "    for x in text:\n",
    "         if x not in string.digits:\n",
    "                cadena.append(x)\n",
    "    return  ''.join(cadena)\n",
    "\n",
    "def eliminar_punct(text):\n",
    "    cadena = []\n",
    "    for x in text:\n",
    "         if x not in punctuation:\n",
    "                cadena.append(x)\n",
    "    return  ''.join(cadena)\n",
    "        \n",
    "def eliminar_espacios_multiples(text):\n",
    "    text = re.sub(r\"\\s+\",\" \", text, flags = re.I)\n",
    "    return text.strip()\n",
    "\n",
    "def eliminar_palabras_con_numeros(text):\n",
    "    return re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "\n",
    "def convertir_location_code(text):\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if word in CODE_LOCATION:\n",
    "            word = CODE_LOCATION[word]\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def reemplazar_chars_primer(text):\n",
    "    text = text.replace('mediterran...','mediterranean') \n",
    "    text = text.replace('&amp;','and')\n",
    "    text = text.replace('&gt;&gt;',' view ')\n",
    "    text = text.replace('&gt;',' ')\n",
    "    text = text.replace('&lt;',' ')\n",
    "    text = text.replace('mph','miles per hour')\n",
    "    text = text.replace('rcmp','royal canadian mounted police')\n",
    "    text = text.replace(' tch','trans canada highway')\n",
    "    text = text.replace('lmfao','laughing my fucking ass off')\n",
    "    text = text.replace('pkwy','park way')\n",
    "    text = text.replace('hwy','high way')\n",
    "    text = text.replace(' gov ',' government ')\n",
    "    text = text.replace('govt','government')\n",
    "    text = text.replace('gov\\'t','government')\n",
    "\n",
    "    \n",
    "    text = text.replace('û',' ')\n",
    "    text = text.replace('÷',' ')\n",
    "    text = text.replace('û',' ')\n",
    "\n",
    "    text = text.replace('friend50','friend')\n",
    "    text = text.replace('offr','officer')\n",
    "    text = text.replace('pkk','kurdistan workers party')\n",
    "    text = text.replace('v deo','video')\n",
    "    text = text.replace(' rly','really')\n",
    "    text = text.replace('i\\'m','i am')\n",
    "    text = text.replace(' dont ',' do not ')\n",
    "    text = text.replace('don\\'t','do not')\n",
    "    text = text.replace(' don t','do not')\n",
    "   \n",
    "    \n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('i\\'ve','i have')\n",
    "    text = text.replace('there\\'s ','there is ')\n",
    "    text = text.replace('i\\'ll ','i will ')\n",
    "    text = text.replace('doesn\\'t','does not')\n",
    "    text = text.replace('i\\'d','i had')\n",
    "    text = text.replace('didn\\'t','did not')\n",
    "    text = text.replace('we\\'re','we are')\n",
    "    text = text.replace('they\\'re','they are')\n",
    "    text = text.replace('isn\\'t','is not')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('let\\'s','let us')\n",
    "    text = text.replace('ain\\'t','am not')\n",
    "    text = text.replace('that\\'s','that is')\n",
    "    text = text.replace('won\\'t','will not')\n",
    "    text = text.replace('wasn\\'t','was not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('haven\\'t','have not')\n",
    "    text = text.replace('here s','here is')\n",
    "    text = text.replace('aren\\'t','are not')\n",
    "    text = text.replace('you\\'ll','you will')\n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('shouldn\\'t','should not')\n",
    "    text = text.replace('should\\'ve','should have')\n",
    "\n",
    "    text = text.replace('here\\'s','here is it')\n",
    "    text = text.replace('she\\'s','she is')\n",
    "    text = text.replace('we\\'ve','we have')\n",
    "    text = text.replace('you\\'ve','you have')\n",
    "    text = text.replace('who\\'s','who is')\n",
    "    text = text.replace('y\\'all','you all')\n",
    "    text = text.replace('wouldn\\'t','would not')\n",
    "    text = text.replace('they\\'ve','they have')\n",
    "    text = text.replace('weren\\'t','were not')\n",
    "    text = text.replace('would\\'ve','would have')\n",
    "    text = text.replace('you\\'d','you would')\n",
    "    text = text.replace('they\\'ll','they will')\n",
    "    text = text.replace('we\\'d','we would')\n",
    "    text = text.replace('they\\'d','they would')\n",
    "    text = text.replace('we\\'ll','we shall')\n",
    "    text = text.replace('it\\'ll','it will')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('can\\'t','can not')\n",
    "    text = text.replace('can t ','can not ')\n",
    "    text = text.replace(' he\\'s',' hi is')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace(' u ',' you ')\n",
    " \n",
    "\n",
    "    text = text.replace('1st','first')\n",
    "    text = text.replace('2nd','second')\n",
    "    text = text.replace('3rd','third')\n",
    "    text = text.replace('4th','fourth')\n",
    "    text = text.replace('5th','fifth')\n",
    "    text = text.replace('6th','sixth')\n",
    "    text = text.replace('7th','seventh')\n",
    "    text = text.replace('8th','eighth')\n",
    "    text = text.replace('9th','ninth')\n",
    "    text = text.replace('10th','tenth')\n",
    "\n",
    "    text = text.replace('u.s.','united states')\n",
    "    text = text.replace('u.s','united states')\n",
    "    text = text.replace('d.c.','district of columbia')\n",
    "    text = text.replace('b.c.','british columbia')\n",
    "\n",
    "    text = text.replace('bioterror','bio teror')\n",
    "    text = text.replace('mh370','malaysia airlines flight 370') #Ver de eliminar\n",
    "    text = text.replace('\\'the','the')\n",
    "    text = text.replace('legionnaires\\'','legionnaires')\n",
    "    text = text.replace(':)','smile')\n",
    "    text = text.replace('(:','smile')\n",
    "\n",
    "\n",
    "    text = text.replace(':(','sad')\n",
    "    text = text.replace('\\'i', 'i')\n",
    "    text = text.replace('confirmed\\'', 'confirmed')\n",
    "    text = text.replace('\\'conclusively', 'conclusively')\n",
    "    text = text.replace('\\'we', 'we')\n",
    "    text = text.replace('\\'it', 'it')\n",
    "    text = text.replace('bestnaijamade', 'best naija made')\n",
    "    text = text.replace('water\\'', 'water')\n",
    "    text = text.replace('china\\'s', 'china')\n",
    "    text = text.replace('neighbour\\'s', 'neighbour')\n",
    "    text = text.replace('crematoria\\'', 'crematoria')\n",
    "    text = text.replace('officeroad', 'office road')\n",
    "    text = text.replace('wwi', 'world war i')\n",
    "    text = text.replace('wwii', 'world war ii')\n",
    "    text = text.replace('ww1', 'world war i')\n",
    "    text = text.replace('ww2', 'world war ii')\n",
    "    text = text.replace('2k15', '2015')\n",
    "    text = text.replace(' www ', ' world war ')\n",
    "    text = text.replace('usgs', 'united states geological survey')\n",
    "    text = text.replace('utc', 'utc ')\n",
    "    text = text.replace('km', ' kilometers')\n",
    "  \n",
    "    text = text.replace('\\'if', 'if')\n",
    "    text = text.replace('\\'save', 'save')\n",
    "    text = text.replace('100%', 'one hundred percent')\n",
    "    text = text.replace('iger\\'s', 'iger')\n",
    "    text = text.replace('charity.\\'', 'charity')\n",
    "    text = text.replace('\\'suicide', 'suicide')\n",
    "    text = text.replace('\\'there', 'there')\n",
    "    text = text.replace('typhoon-devastated', 'typhoon devastated')\n",
    "    text = text.replace('11-year-old', 'eleven year old')\n",
    "    text = text.replace('sensor-senso', 'sensor')\n",
    "    text = text.replace('c-130', 'lockheed hercules')\n",
    "    text = text.replace('self-image?', 'self image')\n",
    "    text = text.replace('i-77', 'interstate 77')       #Ver de eliminar los numeros\n",
    "    text = text.replace('3-d', 'three dimensions')\n",
    "    text = text.replace('cleared:incident', 'cleared incident')\n",
    "    text = text.replace('tomorrow\\'s', 'tomorrow')\n",
    "    text = text.replace('h370', 'malaysia airlines flight 370') #Ver de eliminar los numeros\n",
    "    text = text.replace('chief\\'s', 'chief')\n",
    "    text = text.replace('\\'when', 'when')\n",
    "    text = text.replace('soudelor\\'s', 'soudelor')\n",
    "    text = text.replace('jupiter\\'s', 'jupiter')\n",
    "    text = text.replace('w/o', 'without')\n",
    "    text = text.replace('hostageand2', 'hostage and')\n",
    "    text = text.replace('women\\'s', 'women')\n",
    "    text = text.replace('california\\'s', 'california')\n",
    "    text = text.replace('1fourth', 'fourth')\n",
    "    text = text.replace('150-foot', 'foot')\n",
    "    text = text.replace('someone\\'s', 'someone')\n",
    "    text = text.replace('harm/kid', 'harm kid')\n",
    "    text = text.replace('non compliant', 'non compliant')\n",
    "    text = text.replace('demonstratio...', 'demonstration')\n",
    "    text = text.replace('\\'people', 'people')\n",
    "    text = text.replace('disaster\\'', 'disaster')\n",
    "    text = text.replace('meat-loving', 'meat loving')\n",
    "    text = text.replace('stand-user?', 'stand user')\n",
    "    text = text.replace('injury:i-495', 'injury interstate 495')\n",
    "    text = text.replace('collision-no', 'collision no')\n",
    "    text = text.replace('explosion-proof', 'explosion proof')\n",
    "    text = text.replace('triple-digit', 'triple digit')\n",
    "    text = text.replace('lulgzimbestpicts', 'lul g zim best picts')\n",
    "    text = text.replace('0-day', 'zero day')\n",
    "    text = text.replace('connector-connecto', 'connector connecto')\n",
    "    text = text.replace('2-united', 'two united')\n",
    "    text = text.replace('collision-1141', 'collision')\n",
    "    text = text.replace('h370.', 'malaysia airlines flight 370')\n",
    "    text = text.replace('hatchet-wielding', 'hatchet wielding')\n",
    "    text = text.replace('3-alarm', 'three alarm')\n",
    "    text = text.replace('i-65', 'interstate 65')\n",
    "    text = text.replace('18-wheeler', '18 wheeler')    \n",
    "    text = text.replace('six meter', 'six meter')\n",
    "    text = text.replace('soloquiero', 'solo quiero')\n",
    "    text = text.replace('every day', 'every day')\n",
    "    text = text.replace('gbbo', 'the great british bake off')\n",
    "    text = text.replace('misfortunebut', 'misfortune but')\n",
    "    text = text.replace('53inch 300w', '')\n",
    "    text = text.replace('4X4', '')\n",
    "    text = text.replace('six meter', 'six meter')\n",
    "    text = text.replace('six meter', 'six meter')\n",
    "    text = text.replace('16yr','sixteen years')\n",
    "    text = text.replace('fvck','fuck')\n",
    "    text = text.replace('p.m.','post meridiem')\n",
    "    text = text.replace(' pm ',' post meridiem ')\n",
    "    text = text.replace('a.m.','ante meridiem')\n",
    "    text = text.replace(' a.m ',' ante meridiem ')\n",
    "    text = text.replace(' rn ',' right now ')\n",
    "    text = text.replace(' da ',' the ')\n",
    "    text = text.replace(' min ',' minutes ')\n",
    "    return text\n",
    "\n",
    "def eliminar_palabras_especiales(text):\n",
    "    text = eliminar_palabras_con(text, 'http')\n",
    "    text = eliminar_palabras_con(text, '@')\n",
    "    \n",
    "    '''\n",
    "    text = eliminar_palabras_con(text, '#')\n",
    "    text = eliminar_palabras_con(text, 'mmm')\n",
    "    text = eliminar_palabras_con(text, 'mhm')\n",
    "    text = eliminar_palabras_con(text, 'ww')\n",
    "    text = eliminar_palabras_con(text, 'jsj')\n",
    "    text = eliminar_palabras_con(text, 'haha')\n",
    "    text = eliminar_palabras_con(text, 'hah')\n",
    "    text = eliminar_palabras_con(text, 'ooh')\n",
    "    text = eliminar_palabras_con(text, 'hhh')\n",
    "    text = eliminar_palabras_con(text, 'ahh')   \n",
    "    '''\n",
    "    return text\n",
    "\n",
    "def reemplazar_chars_segunda(text):\n",
    "    text = text.replace('\\'s', '')\n",
    "    text = text.replace('10:00','ten hours')\n",
    "    text = text.replace('i\\'m','i am')     \n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('i\\'ve','i have')\n",
    "    text = text.replace('there\\'s ','there is ')\n",
    "    text = text.replace('i\\'ll ','i will ')\n",
    "    text = text.replace('doesn\\'t','does not')\n",
    "    text = text.replace('i\\'d','i had')\n",
    "    text = text.replace('didn\\'t','did not')\n",
    "    text = text.replace('we\\'re','we are')\n",
    "    text = text.replace('they\\'re','they are')\n",
    "    text = text.replace('isn\\'t','is not')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('let\\'s','let us')\n",
    "    text = text.replace('ain\\'t','am not')\n",
    "    text = text.replace('that\\'s','that is')\n",
    "    text = text.replace('won\\'t','will not')\n",
    "    text = text.replace('wasn\\'t','was not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('haven\\'t','have not')\n",
    "    text = text.replace('here s','here is')\n",
    "    text = text.replace('aren\\'t','are not')\n",
    "    text = text.replace('you\\'ll','you will')\n",
    "    text = text.replace('it\\'s','it is')\n",
    "    text = text.replace('you\\'re','you are')\n",
    "    text = text.replace('shouldn\\'t','should not')\n",
    "    text = text.replace('should\\'ve','should have')\n",
    "\n",
    "    text = text.replace('here\\'s','here is it')\n",
    "    text = text.replace('she\\'s','she is')\n",
    "    text = text.replace('we\\'ve','we have')\n",
    "    text = text.replace('you\\'ve','you have')\n",
    "    text = text.replace('who\\'s','who is')\n",
    "    text = text.replace('y\\'all','you all')\n",
    "    text = text.replace('wouldn\\'t','would not')\n",
    "    text = text.replace('they\\'ve','they have')\n",
    "    text = text.replace('weren\\'t','were not')\n",
    "    text = text.replace('would\\'ve','would have')\n",
    "    text = text.replace('you\\'d','you would')\n",
    "    text = text.replace('they\\'ll','they will')\n",
    "    text = text.replace('we\\'d','we would')\n",
    "    text = text.replace('they\\'d','they would')\n",
    "    text = text.replace('we\\'ll','we shall')\n",
    "    text = text.replace('it\\'ll','it will')\n",
    "    text = text.replace('what\\'s','what is')\n",
    "    text = text.replace('can\\'t','can not')\n",
    "    text = text.replace('can t ','can not ')\n",
    "    text = text.replace('he\\'s','hi is')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('must\\'ve','must have')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace('hasn\\'t','has not')\n",
    "    text = text.replace(' u ',' you ')\n",
    "    text = text.replace('prebreak','pre break')\n",
    "    text = text.replace('soudelor','hanna')\n",
    "    text = text.replace('bayelsa','bayelsa state nigerian')\n",
    "    text = text.replace('marians','CNMI')\n",
    "    text = text.replace('udhampur','indian union territory of jammu and kashmir')\n",
    "    text = text.replace('utc20150805','utc 2015-08-05')\n",
    "    text = text.replace('time20150806','time 2015-08-06')\n",
    "    text = text.replace('utc20150806','utc 2015-08-06')\n",
    "    text = text.replace('time20150805','time 2015-08-05')\n",
    "    text = text.replace('8615','2015-08-06')\n",
    "    text = text.replace('trfc','traffic')\n",
    "    text = text.replace('beyhive','beyonce fan')\n",
    "    text = text.replace('o784','')\n",
    "    text = text.replace('abstorm','absolute storm')\n",
    "    text = text.replace('animalrescue','animal rescue')\n",
    "    text = text.replace('icemoon','ice moon')\n",
    "    text = text.replace('runion','reunion')\n",
    "    text = text.replace('hiroshimanagasaki','hiroshima nagasaki')\n",
    "    text = text.replace('ww1 2 ','world war ')\n",
    "    text = text.replace('measuresarrestpastornganga','measures arrest pastor nganga')\n",
    "    text = text.replace('warmbodies','warm bodies')\n",
    "   # text = text.replace('full rea','full read')\n",
    "   # text = text.replace('full re','full read')\n",
    "    text = text.replace('linkury','linkury malware')\n",
    "    text = text.replace('neileastwood77','neil eastwood')\n",
    "    text = text.replace('mhtw4fnet','')\n",
    "    text = text.replace('kisii','kisii kenya')\n",
    "    text = text.replace('yazidishingalgenocide','yazidi shingal genocide')\n",
    "    text = text.replace('sinjar','sinjar irak')\n",
    "    text = text.replace('okwx','oklahoma weather')\n",
    "    text = text.replace('rohingya','rohingya myanmar')    \n",
    "    text = text.replace('chicagoarea','chicago area')\n",
    "    text = text.replace('socialnews','social news')\n",
    "    text = text.replace('wheavenly','heavenly') \n",
    "    text = text.replace(' www ',' ')\n",
    "    text = text.replace('it\\'d','it would')\n",
    "    text = text.replace('buildings\\'we','building\\'we')\n",
    "    text = text.replace('couldn\\'t','could not')\n",
    "    text = text.replace('r\\'lyeh','fictional lost city')\n",
    "    text = text.replace('could\\'ve','could have')\n",
    "    text = text.replace('don\\'t','do not')\n",
    "    text = text.replace('that\\'d','that would')\n",
    "    text = text.replace('u\\'d','you would')\n",
    "    text = text.replace('he\\'d','he had')\n",
    "    text = text.replace('he\\'ll','he will')\n",
    "    text = text.replace('o\\'clock','hours')\n",
    "    text = text.replace('cont\\'d','continued')\n",
    "    return text\n",
    "\n",
    "def limpiar_char_especiales(text):\n",
    "    text = re.sub(r'[^\\'\\sA-Za-z0-9]', '', text)\n",
    "    return text\n",
    "\n",
    "def limpiar_comillas_simples(text):\n",
    "    cadena = []\n",
    "    end_char = '\\''\n",
    "    for word in text.split():\n",
    "        if word.startswith(end_char):\n",
    "            word = word[len(end_char):]\n",
    "        if word.endswith(end_char):\n",
    "            word = word[:len(word) - len(end_char)]\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "\n",
    "def eliminar_cortas(text):\n",
    "    excepto = ['i', 'a', '1', '2', '3', '4', '5',\n",
    "              '6', '7', '8', '9', '0']\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if (len(word) == 1) & (word not in excepto):\n",
    "            word = ''\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def numero_a_texto(text):\n",
    "    cadena = []\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            word = num2words(word)\n",
    "        cadena.append(word)\n",
    "    return  ' '.join(cadena)\n",
    "\n",
    "def split_hashtag(text):\n",
    "    cadena = []\n",
    "    for w in text.split():\n",
    "        p = w.replace('#', '')\n",
    "        cond = ((not p.isupper()) & (not p.islower()))\n",
    "        if (w.startswith('#') & cond):\n",
    "            p = ' '.join(re.findall('[A-Z][^A-Z]*', p))\n",
    "        cadena.append(p)\n",
    "    return ' '.join(cadena)\n",
    "    \n",
    "def reemplazo_inicial(text):\n",
    "    text = text.replace('%20', ' ')\n",
    "    text = text.replace('Rly', 'really')\n",
    "    text = text.replace('RT ', 'rt ')\n",
    "    text = text.replace(' RT ', ' rt ')\n",
    "    text = text.replace('Û', '\\'')\n",
    "    return text\n",
    "\n",
    " #Algunos tweets como por ejmplo los que contienen la palabra funtenna que no está vectorizado\n",
    " #se repiten con tendencia a ser falsos. 19/5 por ahí conviene entrenarlos con target 0. Ver otros...\n",
    "def cambiar_valor_target(df):\n",
    "    #Tweet casi siempre falso 14/19\n",
    "    cond = (df.text_clean == 'hot funtenna hijacking computers to send data as sound waves black hat 2015 pre break best')\n",
    "    df.loc[cond,'target'] = 0\n",
    "    \n",
    "def formato_inicial(df):\n",
    "    df['location'].fillna(value='noloc', inplace=True)\n",
    "    df['keyword'].fillna(value='nokey', inplace=True)\n",
    "    \n",
    "    #Minúsculas\n",
    "    df['keyword_clean'] = df.keyword.str.replace('%20', ' ')\n",
    "    df['location_clean'] = df.location.str.replace('%20', ' ')\n",
    "    df['text_clean'] = df.text.apply(reemplazo_inicial)\n",
    "    \n",
    "    df['keyword'] = df.keyword_clean.str.lower()\n",
    "    #Antes de poner en minúsculas al texto hago split de los hashtags\n",
    "    df['text_clean'] = df.text_clean.apply(split_hashtag)   \n",
    "    df['text_clean'] = df.text_clean.str.lower()\n",
    "    df['location_clean'] = df.location_clean.str.lower()\n",
    "\n",
    "    \n",
    "    #Sobre location\n",
    "    df['location_clean'] = df.location_clean.str.lower()\n",
    "    df['location_clean'] = df.location_clean.apply(convertir_location_code)\n",
    "    df['location_clean'] = df.location_clean.apply(limpiar_char_especiales)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_punct)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_palabras_con_numeros)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_cortas)\n",
    "    df['location_clean'] = df.location_clean.apply(eliminar_palabras_especiales)\n",
    "\n",
    "\n",
    "    \n",
    "    #Sobre text_clean\n",
    "    df['text_clean'] = df.text_clean.apply(eliminar_palabras_especiales) #Links, @user, etc\n",
    "    df['text_clean'] = df.text_clean.apply(reemplazar_chars_primer) #1st por first, etc.\n",
    "    df['text_clean'] = df.text_clean.apply(limpiar_char_especiales)\n",
    "    df['text_clean'] = df.text_clean.apply(reemplazar_chars_segunda) #Luego de eliminar puntos-giones\n",
    "    df['text_clean'] = df.text_clean.apply(limpiar_comillas_simples)\n",
    "    df['text_clean'] = df.text_clean.str.replace('\\'', ' ') #Algunas palabras están separadas por\n",
    "\n",
    "    \n",
    "    #Hasta acá  \n",
    "   # df['text_clean'] = df.text_clean.apply(numero_a_texto)\n",
    "   # df['text_clean'] = df.text_clean.str.replace('-', ' ') # Esto porque lo anterior enumera: twenty-five\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_punct)\n",
    "    #df['text_clean'] = df.text_clean.apply(eliminar_palabras_con_numeros)\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_sw) \n",
    "    #df['text_clean'] = df.text_clean.apply(lematizar) \n",
    "    df['text_clean'] = df.text_clean.apply(eliminar_espacios_multiples)\n",
    "   # df['text_clean'] = df.text_clean.apply(eliminar_cortas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_df[train_df.target == 0]['location'].str.split(expand=True).stack().value_counts().to_csv('data/sample0.csv')\n",
    "#train_df[train_df.target == 1]['location'].value_counts().to_csv('data/sample1.csv')\n",
    "#cols= ['word_lenght']\n",
    "#df = pd.get_dummies(train_df, columns=cols, drop_first=True)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('data/test.csv', encoding='utf-8')\n",
    "#train_df = train_df.sample(frac=1)\n",
    "formato_inicial(train_df)\n",
    "cambiar_valor_target(train_df)\n",
    "formato_inicial(test_df)\n",
    "train_df.to_csv('data/train_fttd.csv')\n",
    "test_df.to_csv('data/test_fttd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>location_clean</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2890</td>\n",
       "      <td>damage</td>\n",
       "      <td>noloc</td>\n",
       "      <td>Devil May Cry 4 Special Edition Vergil Vs Agnus [Window] Mission 6 - DMD - No Damage By LeedStraiF\\nhttps://t.co/ZhRTcVU0Ff</td>\n",
       "      <td>0</td>\n",
       "      <td>damage</td>\n",
       "      <td>noloc</td>\n",
       "      <td>devil may cry 4 special edition vergil vs agnus window mission 6 dmd no damage by leedstraif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id keyword location  \\\n",
       "2014  2890  damage    noloc   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "2014  Devil May Cry 4 Special Edition Vergil Vs Agnus [Window] Mission 6 - DMD - No Damage By LeedStraiF\\nhttps://t.co/ZhRTcVU0Ff   \n",
       "\n",
       "      target keyword_clean location_clean  \\\n",
       "2014       0        damage          noloc   \n",
       "\n",
       "                                                                                        text_clean  \n",
       "2014  devil may cry 4 special edition vergil vs agnus window mission 6 dmd no damage by leedstraif  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df[train_df.text_clean.str.contains('\\'')]['text_clean'].str.split(expand=True).stack().value_counts().to_csv('data/sample.csv')\n",
    "#train_df.sample(2)\n",
    "#train_df[train_df.location_clean.str.contains('')]['location_clean'].value_counts().to_csv('data/location.csv')\n",
    "#train_df[train_df.text_clean.str.contains('')]['text_clean'].str.split(expand=True).stack().value_counts().to_csv('data/textss.csv')\n",
    "train_df[train_df.text.str.contains('')].sample()\n",
    "#test_df[test_df.text.str.contains('enna')]\n",
    "\n",
    "#train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grafico_distr(train_df, 'cant_palabras', 'Gráfico de distr. de cantidad de palabras en text',  'Cantidad', '')\n",
    "#grafico_distr(train_df, 'cant_caracteres','Gráfico de dist. de cantidad de caracteres en text', 'Cantidad', '')\n",
    "#grafico_distr(train_df, 'cant_stopwords_pct','Gráfico de distr. porcentual de stopwords en text', 'Porcentaje', '')\n",
    "#grafico_distr(train_df, 'cant_mayus_pct', 'Gráfico de distr. porcentual de mayúsculas en text','Porcentaje', '')\n",
    "#grafico_pie(train_df, 'Porcentaje de tweets que contienen link', 'ref_noticias', 'target')\n",
    "#grafico_distr(train_df, 'cant_numeros', 'Gráfico de distr. porcentual de mayúsculas en text','Cantidad', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de la matriz de pesos(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens únicos(blending):  15106\n"
     ]
    }
   ],
   "source": [
    "# Cantidad máxima de palabras en text_clean\n",
    "MAX_SEQUENCE_LENGTH = train_df['text_clean'].str.split().str.len().max() \n",
    "\n",
    "#Maximo de palabras\n",
    "MAX_NUM_WORDS = 50000\n",
    "\n",
    "# Matriz de salida\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "VALIDATION_SPLIT = 0.1  \n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "t = test_df['text_clean']\n",
    "\n",
    "#Cantidad de toquens\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Más una columna para el index\n",
    "print('Tokens únicos(blending): ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_datos_blending(X, y):\n",
    "    #Para entrenar los resultados de los clasificadores\n",
    "    train, test = train_test_split(train_df, test_size=0.1)\n",
    "    X = train['text_clean']\n",
    "    y = train['target']\n",
    "    \n",
    "    X_t = test['text_clean']\n",
    "    y_t = test['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = carga_datos_train(X, y)\n",
    "    \n",
    "    #Tokenizo X_train\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X.values)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Más una columna para el index\n",
    "    print('Tokens únicos(blending): ', vocab_size)\n",
    "\n",
    "    #Texo a sentencias\n",
    "    X_t = tokenizer.texts_to_sequences(X_t.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_t = pad_sequences(X_t, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # 0/1\n",
    "    y_t = y_t.values\n",
    "    return X_train, X_test, y_train, y_test, X_t, y_t\n",
    "\n",
    "def carga_datos_test(X_train, y_train, X_test):\n",
    "\n",
    "    #Tokenizo X_train\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train.values)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Más una columna para el index\n",
    "    print('Tokens únicos(test): ', vocab_size)\n",
    "\n",
    "    #Texo a sentencias\n",
    "    X_train = tokenizer.texts_to_sequences(X_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # [0 1]\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def carga_datos_train(X, y):\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state=12)\n",
    "        \n",
    "    #Tokenizo X_train\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train.values)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Más una columna para el index\n",
    "    print('Tokens únicos(train): ', vocab_size)\n",
    "\n",
    "    #Texo a sentencias\n",
    "    X_train = tokenizer.texts_to_sequences(X_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test.values)\n",
    "    \n",
    "    #Padding\n",
    "    X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # [0 1]\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    y_test = pd.get_dummies(y_test).values\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://nlp.stanford.edu/projects/glove/\n",
    "#https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "#https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "\n",
    "#EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin'    \n",
    "#EMBEDDING_FILE = 'data/wiki-news-300d-1M.vec' \n",
    "EMBEDDING_FILE = 'data/glove.840B.300d.txt' #Best\n",
    "#EMBEDDING_FILE = 'data/glove.6B.300d.txt' \n",
    "#EMBEDDING_FILE = 'data/glove.twitter.27B.200d.txt'\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def embeddings_matrix(w2vec=False):\n",
    "    not_found_vect = []\n",
    "    if w2vec == True:\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True) #para word2vec\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    # Matriz de pesos\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            if w2vec == True:\n",
    "                embedding_vector = embeddings_index.wv[word]\n",
    "            else:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[index] = embedding_vector\n",
    "        except:\n",
    "            not_found_vect.append(word)\n",
    "    return embedding_matrix, embeddings_index, not_found_vect\n",
    "            \n",
    "#Create embedding matrix\n",
    "embedding_matrix, embeddings_index, not_found_vect = embeddings_matrix(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras que no se encuentran en los vectores(ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pct palabras cubiertas: 0.8551002846362613\n",
      "Pct texto cubierto: 0.9756142859796074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15106, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c = porcentaje_cobertura(X,embeddings_index)\n",
    "print('Pct palabras cubiertas:', b)\n",
    "print('Pct texto cubierto:', c)\n",
    "#print('Palabras no cubiertas:', a)\n",
    "serie = pd.Series(a, index=a.keys())\n",
    "palabras_df = serie.to_frame().reset_index()\n",
    "palabras_df.sort_values(by=0, ascending=False).to_csv('data/palabras_no_tw.csv')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El mejor algoritmo(submiteado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens únicos(test):  15106\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 33, 300)           4531800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_26 (Spatia (None, 33, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 33, 64)            85248     \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,683,354\n",
      "Trainable params: 151,554\n",
      "Non-trainable params: 4,531,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "138/138 [==============================] - 9s 63ms/step - loss: 0.5095 - accuracy: 0.7567 - val_loss: 0.4178 - val_accuracy: 0.8018\n",
      "Epoch 2/100\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.4283 - accuracy: 0.8107 - val_loss: 0.4188 - val_accuracy: 0.8031\n",
      "Epoch 3/100\n",
      "138/138 [==============================] - 14s 100ms/step - loss: 0.4111 - accuracy: 0.8193 - val_loss: 0.4144 - val_accuracy: 0.8071\n",
      "Epoch 4/100\n",
      "138/138 [==============================] - 14s 105ms/step - loss: 0.4074 - accuracy: 0.8272 - val_loss: 0.4073 - val_accuracy: 0.8150\n",
      "Epoch 5/100\n",
      "138/138 [==============================] - 14s 98ms/step - loss: 0.3862 - accuracy: 0.8354 - val_loss: 0.4248 - val_accuracy: 0.8018\n",
      "Epoch 6/100\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.3743 - accuracy: 0.8390 - val_loss: 0.4218 - val_accuracy: 0.8123\n",
      "Epoch 7/100\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.3557 - accuracy: 0.8518 - val_loss: 0.4339 - val_accuracy: 0.8110\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                  weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100 #best 9\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm_bi = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_lstm_bi, axis=1)\n",
    "resultados(pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "tokenized = train_df['text_clean'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 55)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 55)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail..\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(padded).to(DEVICE)  \n",
    "attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "print('fail..')\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783
         ],
         "y": [
          10.145262580247529,
          16.16058756606974,
          20.362284217698402,
          24.310416632574135,
          27.542982204071897,
          30.497377344664507,
          33.2174532205926,
          35.581401465674205,
          37.766623687190794,
          39.78526542825121,
          41.658863767083226,
          43.31025490576434,
          44.91245741796727,
          46.41171037579649,
          47.8611461590275,
          49.20301625837991,
          50.51075287145185,
          51.78804179621567,
          53.02897866812702,
          54.13316566256292,
          55.12691520620412,
          56.07928787329084,
          57.00475157381514,
          57.910562472789564,
          58.8020499429136,
          59.651874979917935,
          60.46518836095717,
          61.24877040193545,
          61.98521802714639,
          62.706244881481794,
          63.3904294137915,
          64.04198155360871,
          64.66118700967255,
          65.26500749386501,
          65.84771571981766,
          66.4138756619901,
          66.96154691605456,
          67.4861354842387,
          68.00026744194706,
          68.5043358675648,
          68.99811536625862,
          69.47755470039651,
          69.94568050715138,
          70.40390809821588,
          70.84716124185604,
          71.27171882525262,
          71.67818457977717,
          72.07906076238226,
          72.46993982087159,
          72.84687359084046,
          73.22039011875094,
          73.58494469586124,
          73.94075809743559,
          74.28230486533612,
          74.62216559023707,
          74.9517476848779,
          75.27675392650914,
          75.59004544618493,
          75.89973175307934,
          76.20257370885128,
          76.50319429234482,
          76.7945508307292,
          77.0769312014211,
          77.35857111151562,
          77.63497346969712,
          77.90519969109903,
          78.16987394203684,
          78.4273446576187,
          78.67618626245176,
          78.92306545267776,
          79.16509233800082,
          79.40516967370262,
          79.64221571392426,
          79.87294376360215,
          80.09858564413203,
          80.3225706189669,
          80.54287834168683,
          80.759500404725,
          80.97415922171979,
          81.18076210515069,
          81.38429539805821,
          81.5864105451947,
          81.78747505596812,
          81.98193924862137,
          82.17430931256177,
          82.35956164604518,
          82.54455805293748,
          82.72506347073316,
          82.90382545795143,
          83.0781397731351,
          83.2508882964294,
          83.42173474033832,
          83.58887661553287,
          83.75376097881511,
          83.91775459462525,
          84.08111264581098,
          84.24073260527945,
          84.39753137670556,
          84.55199394414736,
          84.70617224764432,
          84.8585871305644,
          85.00796647870258,
          85.15444819692922,
          85.299662636731,
          85.4421779272493,
          85.58268816850187,
          85.7216312961087,
          85.85792372913788,
          85.99274773685025,
          86.12630459607271,
          86.25711937954058,
          86.38599281478389,
          86.5131147331947,
          86.63981343437284,
          86.76367123631029,
          86.88585471486387,
          87.00596273865125,
          87.12514912260724,
          87.24254968951242,
          87.35889367599981,
          87.47406507860322,
          87.58600195890962,
          87.69694540683024,
          87.80765239530974,
          87.91626865345116,
          88.02410483100523,
          88.13035314522871,
          88.23467575155439,
          88.337364200861,
          88.43921342746849,
          88.54043073818157,
          88.63963500173564,
          88.73785724225144,
          88.83398770081914,
          88.92918960564582,
          89.0233388379942,
          89.11712638597747,
          89.20899722888453,
          89.29984589341629,
          89.38948131628787,
          89.47866518020905,
          89.56721536990344,
          89.6540632625918,
          89.74043518943621,
          89.82613833332407,
          89.91138680714683,
          89.99417213288777,
          90.07678807367006,
          90.15733691906598,
          90.23766643868345,
          90.31731016214533,
          90.39570833973713,
          90.47306601895298,
          90.54882052801908,
          90.62422134127961,
          90.69890516235495,
          90.77291879393543,
          90.84678860688821,
          90.9190857487924,
          90.99112171650131,
          91.06272252422401,
          91.13391960077534,
          91.20365106709737,
          91.27277458863624,
          91.34089161386842,
          91.40812767807316,
          91.4749700666141,
          91.54083841577956,
          91.60616307042191,
          91.67094275923642,
          91.7351315676551,
          91.7984460042604,
          91.86128211374722,
          91.92399325936196,
          91.98511120530837,
          92.04592953086257,
          92.1061312690859,
          92.16586060307415,
          92.22536602087943,
          92.28366463503478,
          92.34126067434407,
          92.39848719042969,
          92.4553512294183,
          92.51207592723941,
          92.56809343198839,
          92.6235865155741,
          92.67837744610206,
          92.73295174991611,
          92.78670206352369,
          92.83998988746147,
          92.89270478319035,
          92.94507293656066,
          92.99734379529824,
          93.04908402741056,
          93.09991705533153,
          93.14992452593441,
          93.19971807021383,
          93.24878102154213,
          93.29720286447515,
          93.34523122589633,
          93.39295405143156,
          93.44005047052107,
          93.48684788285496,
          93.53343559854001,
          93.57951161867275,
          93.62543013239178,
          93.67054370465067,
          93.71515269939319,
          93.75931396897722,
          93.80307519566324,
          93.84643401952057,
          93.88952965177992,
          93.93232091335305,
          93.9749644719347,
          94.01714918493735,
          94.05865791281987,
          94.09971105429814,
          94.14033658284123,
          94.18087441860011,
          94.22077691150693,
          94.26034586730886,
          94.29969051302707,
          94.33861147100647,
          94.3774065654491,
          94.41598897659482,
          94.4542986755161,
          94.49242695542186,
          94.53010719665357,
          94.56756092635356,
          94.60472359961952,
          94.64148047725023,
          94.6779914756475,
          94.71402679000713,
          94.74975044687328,
          94.78515930610031,
          94.82050212120839,
          94.85568334877851,
          94.89074637703979,
          94.92540164561161,
          94.95974130109657,
          94.99383492666311,
          95.02775346996614,
          95.0615252819174,
          95.09483795975397,
          95.12779539501018,
          95.16049995403245,
          95.19274572894253,
          95.22490511544116,
          95.25686177503934,
          95.28850506169178,
          95.32002226349078,
          95.35139291155346,
          95.38230496845664,
          95.41304144930682,
          95.44355581666474,
          95.47389384321964,
          95.50410449262331,
          95.53414285789925,
          95.56359161754341,
          95.59301641711954,
          95.62214494310791,
          95.6509528186212,
          95.679565477976,
          95.7078681731444,
          95.73592500419703,
          95.76389450484736,
          95.79168323079861,
          95.81899283030592,
          95.84624760706618,
          95.87335119245975,
          95.90023260500523,
          95.92700234276043,
          95.95356438910599,
          95.97984852092543,
          96.00603981393013,
          96.03201519646205,
          96.05771976490523,
          96.08338109747078,
          96.10887813176562,
          96.13424496240687,
          96.15942821690172,
          96.18452095610508,
          96.20939514084637,
          96.23386808528142,
          96.25801264877416,
          96.28203107359855,
          96.30598118160042,
          96.32975281122938,
          96.35348828295021,
          96.37696623668414,
          96.40037836848937,
          96.42363061841544,
          96.446761446473,
          96.4696129139995,
          96.492257867036,
          96.51483158984057,
          96.53716373774311,
          96.55943560360136,
          96.58143863033119,
          96.60333085464707,
          96.62494596771262,
          96.64653051620148,
          96.66793886774663,
          96.68925963440574,
          96.71037448469372,
          96.73139256307313,
          96.75233682905815,
          96.7730628230376,
          96.79371549729578,
          96.8142808780989,
          96.83462854055229,
          96.85493997420656,
          96.87504521579501,
          96.89500863572961,
          96.9148698416368,
          96.9345508364493,
          96.95396719299384,
          96.97332608083464,
          96.99256786318166,
          97.01173489999327,
          97.03084352806702,
          97.04982018768969,
          97.0685595691232,
          97.08721918136239,
          97.10578212596491,
          97.1243075861197,
          97.14261766149059,
          97.16083873633912,
          97.17892508840298,
          97.19696477986591,
          97.21493925067406,
          97.23278127248825,
          97.25046749338924,
          97.26811850054474,
          97.28568254233286,
          97.30304366405754,
          97.32032867736132,
          97.33750923534843,
          97.35461175773659,
          97.37157990889628,
          97.3883728388297,
          97.40508381797753,
          97.42157709422922,
          97.43802209790897,
          97.4544264952754,
          97.47077124600051,
          97.48700933501506,
          97.50311542938935,
          97.51917473714394,
          97.53515749759539,
          97.5510233639419,
          97.56679505370876,
          97.5824883018426,
          97.59809882896235,
          97.61355206921915,
          97.62887636885539,
          97.64406699665314,
          97.65922026107454,
          97.67420851725265,
          97.68910304314211,
          97.70398258156848,
          97.71883758013614,
          97.73350401360652,
          97.7480708004026,
          97.76256684378738,
          97.77703714042647,
          97.79143646137597,
          97.80578373401538,
          97.81989511909376,
          97.83391878938058,
          97.84791506233769,
          97.86185828827,
          97.87570984856852,
          97.88952814367961,
          97.90327022733263,
          97.91682800912635,
          97.93029297459285,
          97.9436839940453,
          97.95705856959744,
          97.97035401353361,
          97.98360613680626,
          97.99676217031003,
          98.0097715881888,
          98.02275205369654,
          98.03561940924514,
          98.04843370589677,
          98.06108262543806,
          98.07363870868824,
          98.08616991391034,
          98.09856412928488,
          98.11094016148293,
          98.12325449072044,
          98.13546814545982,
          98.14761697713048,
          98.15972155515279,
          98.17180669139104,
          98.18384458411481,
          98.19576331784042,
          98.2075857536098,
          98.2192968204525,
          98.23098365894506,
          98.24263588929432,
          98.25418507626621,
          98.26569851305595,
          98.27713936245192,
          98.28849921223554,
          98.29978450201372,
          98.31096533412556,
          98.32208628917094,
          98.33317479034945,
          98.34424292736351,
          98.35520040535613,
          98.36611082260131,
          98.37697721779983,
          98.38781386081688,
          98.39855158446103,
          98.40922489641406,
          98.41986258544817,
          98.43040096246214,
          98.4409056227304,
          98.45133469417101,
          98.46174900143333,
          98.4720936975478,
          98.48238361576057,
          98.49262650703251,
          98.50280371798964,
          98.51290117668906,
          98.52297926372486,
          98.53298000963692,
          98.54291660575204,
          98.55278961253342,
          98.56257285999334,
          98.5723479544889,
          98.58205017939758,
          98.59172074597085,
          98.60131146455782,
          98.61086715697449,
          98.6203663924164,
          98.62980621180911,
          98.63918786884794,
          98.64855465976653,
          98.65780373678582,
          98.66700208299679,
          98.67618365061642,
          98.68535848144752,
          98.69448867417408,
          98.70361018708803,
          98.7126709365998,
          98.7217126504547,
          98.7306830108269,
          98.73955502602176,
          98.74840578386356,
          98.75719957798103,
          98.76593114279252,
          98.77460114001532,
          98.7832331543222,
          98.79181290171677,
          98.80030185888921,
          98.8087642816585,
          98.81719407602257,
          98.82560348980297,
          98.83391125274547,
          98.84214846969084,
          98.85032240092796,
          98.8584522949445,
          98.86654040301997,
          98.87461524249632,
          98.88266079279865,
          98.89063899391296,
          98.89860962758985,
          98.9065608407664,
          98.91449060141433,
          98.92235455906862,
          98.9301670112855,
          98.93792270813769,
          98.94562394757772,
          98.95329589743558,
          98.96088715400718,
          98.96842859232336,
          98.9759431657459,
          98.9834372580964,
          98.99089913608279,
          98.99832928151797,
          99.00573761542877,
          99.0130668826207,
          99.02032612287435,
          99.02757086069934,
          99.03474535598961,
          99.04189328474757,
          99.04903216432524,
          99.05610825917982,
          99.06314806078791,
          99.07016997472469,
          99.07714504512343,
          99.08409651325604,
          99.09099293673349,
          99.09784878654358,
          99.10469092902248,
          99.11150580752093,
          99.11828410040286,
          99.12499688652582,
          99.13168382161282,
          99.13835627177278,
          99.14494945338441,
          99.15152540149111,
          99.1580603331124,
          99.16455130780834,
          99.17098525582001,
          99.17738518306217,
          99.18373077079879,
          99.1900554629992,
          99.19635913870442,
          99.20261576771912,
          99.20885257187784,
          99.2150144874812,
          99.22115679571421,
          99.22724453693617,
          99.23331402767535,
          99.23936830721132,
          99.24536677190453,
          99.25133477794185,
          99.25726889353882,
          99.2631596554394,
          99.26898199854978,
          99.27475848937718,
          99.2805207298602,
          99.28624343867024,
          99.29193509769826,
          99.29761625529375,
          99.30326914535786,
          99.30885721739291,
          99.31442534365564,
          99.31996320937803,
          99.32544142141758,
          99.33091214516406,
          99.33635989454673,
          99.34176691235884,
          99.34716276731643,
          99.35252685777802,
          99.3578883275157,
          99.36323584819455,
          99.36855183230973,
          99.37384927392281,
          99.37910696313668,
          99.38433076579469,
          99.38950099565798,
          99.39465099651628,
          99.39977210560176,
          99.40486835576664,
          99.40995072676535,
          99.41497930892103,
          99.41997026248627,
          99.42494320114085,
          99.42988976047268,
          99.43481498865832,
          99.43970772869147,
          99.44455645495967,
          99.44939418132635,
          99.45418420797851,
          99.45894077904059,
          99.4636847407592,
          99.46841465065839,
          99.47312067507221,
          99.47778774175,
          99.48241906769542,
          99.48704292719361,
          99.49164087276439,
          99.49620814215228,
          99.50075016414013,
          99.50525600298812,
          99.50973967576171,
          99.51416576948179,
          99.51856628295974,
          99.52294705449167,
          99.52731257676051,
          99.53166775618784,
          99.5359882580815,
          99.54028232717847,
          99.54457121273985,
          99.54883425262823,
          99.5530771587829,
          99.55729514419656,
          99.56150584711574,
          99.56566587300483,
          99.56979040134077,
          99.57391164879931,
          99.57800097441863,
          99.58208283998414,
          99.5861169971463,
          99.59013964897862,
          99.59415233849867,
          99.59814842268287,
          99.60212556853112,
          99.60604737932663,
          99.60995225165848,
          99.61382069148289,
          99.61766985081731,
          99.6215012940465,
          99.62529910548413,
          99.62909459293913,
          99.6328523793437,
          99.63658679587402,
          99.64030284675071,
          99.64400671354322,
          99.64769361994914,
          99.65132825961855,
          99.65494743263294,
          99.6585499653266,
          99.66213476259742,
          99.66570536361661,
          99.66923747803794,
          99.67274970632286,
          99.67623366721563,
          99.67968033370082,
          99.68312479279683,
          99.68653314702273,
          99.6899261569497,
          99.693310895674,
          99.69667929336413,
          99.7000128358169,
          99.70333481446833,
          99.70662500830245,
          99.70991043709896,
          99.71317079162557,
          99.7164215036115,
          99.71965249684636,
          99.72286543468203,
          99.72605095832267,
          99.72919836035132,
          99.73234026354888,
          99.73547271340937,
          99.73858754236178,
          99.74164973889174,
          99.74469434407729,
          99.74772904884408,
          99.7507409170646,
          99.75373903002685,
          99.75671982843346,
          99.75967407150766,
          99.76261853269712,
          99.76554006098148,
          99.76845150937423,
          99.77133954151942,
          99.7742181503919,
          99.77707626088655,
          99.77992431171546,
          99.78275014944057,
          99.78554857865652,
          99.7883432055596,
          99.79111412653442,
          99.79387575162347,
          99.79662195624674,
          99.79935243549227,
          99.80205020611872,
          99.80471941586025,
          99.80736974396225,
          99.81001526380355,
          99.81264290598465,
          99.81526000682906,
          99.81786340681055,
          99.82044859904892,
          99.82301370238568,
          99.82556638107255,
          99.82810225290831,
          99.83062295270307,
          99.83313129615259,
          99.83563145331104,
          99.83808949910484,
          99.84054184385464,
          99.84296771350141,
          99.8453705975681,
          99.84775602084893,
          99.85013542750917,
          99.85248862681492,
          99.8548268860025,
          99.85714965287914,
          99.85944758788942,
          99.86174066303803,
          99.86402281969724,
          99.86628885053254,
          99.8685326806073,
          99.8707729046697,
          99.87299819785184,
          99.8752139955459,
          99.87741889996005,
          99.87960271453544,
          99.88177666518465,
          99.8839239577943,
          99.88605890034053,
          99.88818264288888,
          99.89026862146842,
          99.89234444733965,
          99.89440116253314,
          99.89642918321272,
          99.89844470801178,
          99.90043205495834,
          99.9024150665652,
          99.90439025511894,
          99.90635662148749,
          99.90830585309676,
          99.91024271093553,
          99.9121622014042,
          99.91404880017353,
          99.91591969138973,
          99.91777326343342,
          99.91961445812036,
          99.9214313668514,
          99.92323728298007,
          99.92502595291903,
          99.92680222100405,
          99.92857065502068,
          99.93032429016647,
          99.93206770268158,
          99.93379921763253,
          99.93551802317258,
          99.93721046703511,
          99.93886947986442,
          99.94052633759716,
          99.94216176338998,
          99.94378844869853,
          99.94540577858756,
          99.94700526479758,
          99.94859603594782,
          99.95017138656698,
          99.95172879183723,
          99.95327855391488,
          99.95481264627755,
          99.95632183700843,
          99.95782050458088,
          99.95930176140034,
          99.96077690753884,
          99.96224039309548,
          99.9636994756172,
          99.96512685856341,
          99.966538659348,
          99.96793712952443,
          99.96931054761818,
          99.97067691011327,
          99.97202208878302,
          99.97334737396699,
          99.97465807250452,
          99.97595643416292,
          99.97723988072279,
          99.9785126246956,
          99.97975747559084,
          99.98099477142412,
          99.98221408150363,
          99.98342001605086,
          99.98461558176248,
          99.98579641085914,
          99.9869581189147,
          99.98809624488162,
          99.98920421808013,
          99.99030087723403,
          99.99139469626107,
          99.9924577812493,
          99.99348958856125,
          99.99446789277114,
          99.99538738343179,
          99.99621834308668,
          99.99697069202158,
          99.99769829440295,
          99.99836242900378,
          99.9990020269554,
          99.99954561202104,
          99.9998487904302,
          100.00000000000003,
          100.00000000000004
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"22090433-a02f-4eee-9dfc-721e565e71e6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"22090433-a02f-4eee-9dfc-721e565e71e6\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '22090433-a02f-4eee-9dfc-721e565e71e6',\n",
       "                        [{\"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783], \"y\": [10.145262580247529, 16.16058756606974, 20.362284217698402, 24.310416632574135, 27.542982204071897, 30.497377344664507, 33.2174532205926, 35.581401465674205, 37.766623687190794, 39.78526542825121, 41.658863767083226, 43.31025490576434, 44.91245741796727, 46.41171037579649, 47.8611461590275, 49.20301625837991, 50.51075287145185, 51.78804179621567, 53.02897866812702, 54.13316566256292, 55.12691520620412, 56.07928787329084, 57.00475157381514, 57.910562472789564, 58.8020499429136, 59.651874979917935, 60.46518836095717, 61.24877040193545, 61.98521802714639, 62.706244881481794, 63.3904294137915, 64.04198155360871, 64.66118700967255, 65.26500749386501, 65.84771571981766, 66.4138756619901, 66.96154691605456, 67.4861354842387, 68.00026744194706, 68.5043358675648, 68.99811536625862, 69.47755470039651, 69.94568050715138, 70.40390809821588, 70.84716124185604, 71.27171882525262, 71.67818457977717, 72.07906076238226, 72.46993982087159, 72.84687359084046, 73.22039011875094, 73.58494469586124, 73.94075809743559, 74.28230486533612, 74.62216559023707, 74.9517476848779, 75.27675392650914, 75.59004544618493, 75.89973175307934, 76.20257370885128, 76.50319429234482, 76.7945508307292, 77.0769312014211, 77.35857111151562, 77.63497346969712, 77.90519969109903, 78.16987394203684, 78.4273446576187, 78.67618626245176, 78.92306545267776, 79.16509233800082, 79.40516967370262, 79.64221571392426, 79.87294376360215, 80.09858564413203, 80.3225706189669, 80.54287834168683, 80.759500404725, 80.97415922171979, 81.18076210515069, 81.38429539805821, 81.5864105451947, 81.78747505596812, 81.98193924862137, 82.17430931256177, 82.35956164604518, 82.54455805293748, 82.72506347073316, 82.90382545795143, 83.0781397731351, 83.2508882964294, 83.42173474033832, 83.58887661553287, 83.75376097881511, 83.91775459462525, 84.08111264581098, 84.24073260527945, 84.39753137670556, 84.55199394414736, 84.70617224764432, 84.8585871305644, 85.00796647870258, 85.15444819692922, 85.299662636731, 85.4421779272493, 85.58268816850187, 85.7216312961087, 85.85792372913788, 85.99274773685025, 86.12630459607271, 86.25711937954058, 86.38599281478389, 86.5131147331947, 86.63981343437284, 86.76367123631029, 86.88585471486387, 87.00596273865125, 87.12514912260724, 87.24254968951242, 87.35889367599981, 87.47406507860322, 87.58600195890962, 87.69694540683024, 87.80765239530974, 87.91626865345116, 88.02410483100523, 88.13035314522871, 88.23467575155439, 88.337364200861, 88.43921342746849, 88.54043073818157, 88.63963500173564, 88.73785724225144, 88.83398770081914, 88.92918960564582, 89.0233388379942, 89.11712638597747, 89.20899722888453, 89.29984589341629, 89.38948131628787, 89.47866518020905, 89.56721536990344, 89.6540632625918, 89.74043518943621, 89.82613833332407, 89.91138680714683, 89.99417213288777, 90.07678807367006, 90.15733691906598, 90.23766643868345, 90.31731016214533, 90.39570833973713, 90.47306601895298, 90.54882052801908, 90.62422134127961, 90.69890516235495, 90.77291879393543, 90.84678860688821, 90.9190857487924, 90.99112171650131, 91.06272252422401, 91.13391960077534, 91.20365106709737, 91.27277458863624, 91.34089161386842, 91.40812767807316, 91.4749700666141, 91.54083841577956, 91.60616307042191, 91.67094275923642, 91.7351315676551, 91.7984460042604, 91.86128211374722, 91.92399325936196, 91.98511120530837, 92.04592953086257, 92.1061312690859, 92.16586060307415, 92.22536602087943, 92.28366463503478, 92.34126067434407, 92.39848719042969, 92.4553512294183, 92.51207592723941, 92.56809343198839, 92.6235865155741, 92.67837744610206, 92.73295174991611, 92.78670206352369, 92.83998988746147, 92.89270478319035, 92.94507293656066, 92.99734379529824, 93.04908402741056, 93.09991705533153, 93.14992452593441, 93.19971807021383, 93.24878102154213, 93.29720286447515, 93.34523122589633, 93.39295405143156, 93.44005047052107, 93.48684788285496, 93.53343559854001, 93.57951161867275, 93.62543013239178, 93.67054370465067, 93.71515269939319, 93.75931396897722, 93.80307519566324, 93.84643401952057, 93.88952965177992, 93.93232091335305, 93.9749644719347, 94.01714918493735, 94.05865791281987, 94.09971105429814, 94.14033658284123, 94.18087441860011, 94.22077691150693, 94.26034586730886, 94.29969051302707, 94.33861147100647, 94.3774065654491, 94.41598897659482, 94.4542986755161, 94.49242695542186, 94.53010719665357, 94.56756092635356, 94.60472359961952, 94.64148047725023, 94.6779914756475, 94.71402679000713, 94.74975044687328, 94.78515930610031, 94.82050212120839, 94.85568334877851, 94.89074637703979, 94.92540164561161, 94.95974130109657, 94.99383492666311, 95.02775346996614, 95.0615252819174, 95.09483795975397, 95.12779539501018, 95.16049995403245, 95.19274572894253, 95.22490511544116, 95.25686177503934, 95.28850506169178, 95.32002226349078, 95.35139291155346, 95.38230496845664, 95.41304144930682, 95.44355581666474, 95.47389384321964, 95.50410449262331, 95.53414285789925, 95.56359161754341, 95.59301641711954, 95.62214494310791, 95.6509528186212, 95.679565477976, 95.7078681731444, 95.73592500419703, 95.76389450484736, 95.79168323079861, 95.81899283030592, 95.84624760706618, 95.87335119245975, 95.90023260500523, 95.92700234276043, 95.95356438910599, 95.97984852092543, 96.00603981393013, 96.03201519646205, 96.05771976490523, 96.08338109747078, 96.10887813176562, 96.13424496240687, 96.15942821690172, 96.18452095610508, 96.20939514084637, 96.23386808528142, 96.25801264877416, 96.28203107359855, 96.30598118160042, 96.32975281122938, 96.35348828295021, 96.37696623668414, 96.40037836848937, 96.42363061841544, 96.446761446473, 96.4696129139995, 96.492257867036, 96.51483158984057, 96.53716373774311, 96.55943560360136, 96.58143863033119, 96.60333085464707, 96.62494596771262, 96.64653051620148, 96.66793886774663, 96.68925963440574, 96.71037448469372, 96.73139256307313, 96.75233682905815, 96.7730628230376, 96.79371549729578, 96.8142808780989, 96.83462854055229, 96.85493997420656, 96.87504521579501, 96.89500863572961, 96.9148698416368, 96.9345508364493, 96.95396719299384, 96.97332608083464, 96.99256786318166, 97.01173489999327, 97.03084352806702, 97.04982018768969, 97.0685595691232, 97.08721918136239, 97.10578212596491, 97.1243075861197, 97.14261766149059, 97.16083873633912, 97.17892508840298, 97.19696477986591, 97.21493925067406, 97.23278127248825, 97.25046749338924, 97.26811850054474, 97.28568254233286, 97.30304366405754, 97.32032867736132, 97.33750923534843, 97.35461175773659, 97.37157990889628, 97.3883728388297, 97.40508381797753, 97.42157709422922, 97.43802209790897, 97.4544264952754, 97.47077124600051, 97.48700933501506, 97.50311542938935, 97.51917473714394, 97.53515749759539, 97.5510233639419, 97.56679505370876, 97.5824883018426, 97.59809882896235, 97.61355206921915, 97.62887636885539, 97.64406699665314, 97.65922026107454, 97.67420851725265, 97.68910304314211, 97.70398258156848, 97.71883758013614, 97.73350401360652, 97.7480708004026, 97.76256684378738, 97.77703714042647, 97.79143646137597, 97.80578373401538, 97.81989511909376, 97.83391878938058, 97.84791506233769, 97.86185828827, 97.87570984856852, 97.88952814367961, 97.90327022733263, 97.91682800912635, 97.93029297459285, 97.9436839940453, 97.95705856959744, 97.97035401353361, 97.98360613680626, 97.99676217031003, 98.0097715881888, 98.02275205369654, 98.03561940924514, 98.04843370589677, 98.06108262543806, 98.07363870868824, 98.08616991391034, 98.09856412928488, 98.11094016148293, 98.12325449072044, 98.13546814545982, 98.14761697713048, 98.15972155515279, 98.17180669139104, 98.18384458411481, 98.19576331784042, 98.2075857536098, 98.2192968204525, 98.23098365894506, 98.24263588929432, 98.25418507626621, 98.26569851305595, 98.27713936245192, 98.28849921223554, 98.29978450201372, 98.31096533412556, 98.32208628917094, 98.33317479034945, 98.34424292736351, 98.35520040535613, 98.36611082260131, 98.37697721779983, 98.38781386081688, 98.39855158446103, 98.40922489641406, 98.41986258544817, 98.43040096246214, 98.4409056227304, 98.45133469417101, 98.46174900143333, 98.4720936975478, 98.48238361576057, 98.49262650703251, 98.50280371798964, 98.51290117668906, 98.52297926372486, 98.53298000963692, 98.54291660575204, 98.55278961253342, 98.56257285999334, 98.5723479544889, 98.58205017939758, 98.59172074597085, 98.60131146455782, 98.61086715697449, 98.6203663924164, 98.62980621180911, 98.63918786884794, 98.64855465976653, 98.65780373678582, 98.66700208299679, 98.67618365061642, 98.68535848144752, 98.69448867417408, 98.70361018708803, 98.7126709365998, 98.7217126504547, 98.7306830108269, 98.73955502602176, 98.74840578386356, 98.75719957798103, 98.76593114279252, 98.77460114001532, 98.7832331543222, 98.79181290171677, 98.80030185888921, 98.8087642816585, 98.81719407602257, 98.82560348980297, 98.83391125274547, 98.84214846969084, 98.85032240092796, 98.8584522949445, 98.86654040301997, 98.87461524249632, 98.88266079279865, 98.89063899391296, 98.89860962758985, 98.9065608407664, 98.91449060141433, 98.92235455906862, 98.9301670112855, 98.93792270813769, 98.94562394757772, 98.95329589743558, 98.96088715400718, 98.96842859232336, 98.9759431657459, 98.9834372580964, 98.99089913608279, 98.99832928151797, 99.00573761542877, 99.0130668826207, 99.02032612287435, 99.02757086069934, 99.03474535598961, 99.04189328474757, 99.04903216432524, 99.05610825917982, 99.06314806078791, 99.07016997472469, 99.07714504512343, 99.08409651325604, 99.09099293673349, 99.09784878654358, 99.10469092902248, 99.11150580752093, 99.11828410040286, 99.12499688652582, 99.13168382161282, 99.13835627177278, 99.14494945338441, 99.15152540149111, 99.1580603331124, 99.16455130780834, 99.17098525582001, 99.17738518306217, 99.18373077079879, 99.1900554629992, 99.19635913870442, 99.20261576771912, 99.20885257187784, 99.2150144874812, 99.22115679571421, 99.22724453693617, 99.23331402767535, 99.23936830721132, 99.24536677190453, 99.25133477794185, 99.25726889353882, 99.2631596554394, 99.26898199854978, 99.27475848937718, 99.2805207298602, 99.28624343867024, 99.29193509769826, 99.29761625529375, 99.30326914535786, 99.30885721739291, 99.31442534365564, 99.31996320937803, 99.32544142141758, 99.33091214516406, 99.33635989454673, 99.34176691235884, 99.34716276731643, 99.35252685777802, 99.3578883275157, 99.36323584819455, 99.36855183230973, 99.37384927392281, 99.37910696313668, 99.38433076579469, 99.38950099565798, 99.39465099651628, 99.39977210560176, 99.40486835576664, 99.40995072676535, 99.41497930892103, 99.41997026248627, 99.42494320114085, 99.42988976047268, 99.43481498865832, 99.43970772869147, 99.44455645495967, 99.44939418132635, 99.45418420797851, 99.45894077904059, 99.4636847407592, 99.46841465065839, 99.47312067507221, 99.47778774175, 99.48241906769542, 99.48704292719361, 99.49164087276439, 99.49620814215228, 99.50075016414013, 99.50525600298812, 99.50973967576171, 99.51416576948179, 99.51856628295974, 99.52294705449167, 99.52731257676051, 99.53166775618784, 99.5359882580815, 99.54028232717847, 99.54457121273985, 99.54883425262823, 99.5530771587829, 99.55729514419656, 99.56150584711574, 99.56566587300483, 99.56979040134077, 99.57391164879931, 99.57800097441863, 99.58208283998414, 99.5861169971463, 99.59013964897862, 99.59415233849867, 99.59814842268287, 99.60212556853112, 99.60604737932663, 99.60995225165848, 99.61382069148289, 99.61766985081731, 99.6215012940465, 99.62529910548413, 99.62909459293913, 99.6328523793437, 99.63658679587402, 99.64030284675071, 99.64400671354322, 99.64769361994914, 99.65132825961855, 99.65494743263294, 99.6585499653266, 99.66213476259742, 99.66570536361661, 99.66923747803794, 99.67274970632286, 99.67623366721563, 99.67968033370082, 99.68312479279683, 99.68653314702273, 99.6899261569497, 99.693310895674, 99.69667929336413, 99.7000128358169, 99.70333481446833, 99.70662500830245, 99.70991043709896, 99.71317079162557, 99.7164215036115, 99.71965249684636, 99.72286543468203, 99.72605095832267, 99.72919836035132, 99.73234026354888, 99.73547271340937, 99.73858754236178, 99.74164973889174, 99.74469434407729, 99.74772904884408, 99.7507409170646, 99.75373903002685, 99.75671982843346, 99.75967407150766, 99.76261853269712, 99.76554006098148, 99.76845150937423, 99.77133954151942, 99.7742181503919, 99.77707626088655, 99.77992431171546, 99.78275014944057, 99.78554857865652, 99.7883432055596, 99.79111412653442, 99.79387575162347, 99.79662195624674, 99.79935243549227, 99.80205020611872, 99.80471941586025, 99.80736974396225, 99.81001526380355, 99.81264290598465, 99.81526000682906, 99.81786340681055, 99.82044859904892, 99.82301370238568, 99.82556638107255, 99.82810225290831, 99.83062295270307, 99.83313129615259, 99.83563145331104, 99.83808949910484, 99.84054184385464, 99.84296771350141, 99.8453705975681, 99.84775602084893, 99.85013542750917, 99.85248862681492, 99.8548268860025, 99.85714965287914, 99.85944758788942, 99.86174066303803, 99.86402281969724, 99.86628885053254, 99.8685326806073, 99.8707729046697, 99.87299819785184, 99.8752139955459, 99.87741889996005, 99.87960271453544, 99.88177666518465, 99.8839239577943, 99.88605890034053, 99.88818264288888, 99.89026862146842, 99.89234444733965, 99.89440116253314, 99.89642918321272, 99.89844470801178, 99.90043205495834, 99.9024150665652, 99.90439025511894, 99.90635662148749, 99.90830585309676, 99.91024271093553, 99.9121622014042, 99.91404880017353, 99.91591969138973, 99.91777326343342, 99.91961445812036, 99.9214313668514, 99.92323728298007, 99.92502595291903, 99.92680222100405, 99.92857065502068, 99.93032429016647, 99.93206770268158, 99.93379921763253, 99.93551802317258, 99.93721046703511, 99.93886947986442, 99.94052633759716, 99.94216176338998, 99.94378844869853, 99.94540577858756, 99.94700526479758, 99.94859603594782, 99.95017138656698, 99.95172879183723, 99.95327855391488, 99.95481264627755, 99.95632183700843, 99.95782050458088, 99.95930176140034, 99.96077690753884, 99.96224039309548, 99.9636994756172, 99.96512685856341, 99.966538659348, 99.96793712952443, 99.96931054761818, 99.97067691011327, 99.97202208878302, 99.97334737396699, 99.97465807250452, 99.97595643416292, 99.97723988072279, 99.9785126246956, 99.97975747559084, 99.98099477142412, 99.98221408150363, 99.98342001605086, 99.98461558176248, 99.98579641085914, 99.9869581189147, 99.98809624488162, 99.98920421808013, 99.99030087723403, 99.99139469626107, 99.9924577812493, 99.99348958856125, 99.99446789277114, 99.99538738343179, 99.99621834308668, 99.99697069202158, 99.99769829440295, 99.99836242900378, 99.9990020269554, 99.99954561202104, 99.9998487904302, 100.00000000000003, 100.00000000000004]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('22090433-a02f-4eee-9dfc-721e565e71e6');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=list(range(784)), y=cum_var_exp))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 500)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_mean = np.mean(features, axis = 1)\n",
    "A_demeaned = A - A_mean.reshape(-1, 1)\n",
    "U, S, V = scipy_svd(A_demeaned, full_matrices = False)\n",
    "pca = PCA(500)  # project from 768 to 500 dimensions\n",
    "features = pca.fit_transform(A_demeaned)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "LR = LogisticRegression(penalty='l2',  C=0.22)\n",
    "LR.fit(X_train, y_train)\n",
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "scatter",
         "x": [
          0.01,
          0.02,
          0.03,
          0.04,
          0.05,
          0.060000000000000005,
          0.06999999999999999,
          0.08,
          0.09,
          0.09999999999999999,
          0.11,
          0.12,
          0.13,
          0.14,
          0.15000000000000002,
          0.16,
          0.17,
          0.18000000000000002,
          0.19,
          0.2,
          0.21000000000000002,
          0.22,
          0.23,
          0.24000000000000002,
          0.25,
          0.26,
          0.27,
          0.28,
          0.29000000000000004,
          0.3,
          0.31,
          0.32,
          0.33,
          0.34,
          0.35000000000000003,
          0.36000000000000004,
          0.37,
          0.38,
          0.39,
          0.4,
          0.41000000000000003,
          0.42000000000000004,
          0.43,
          0.44,
          0.45,
          0.46,
          0.47000000000000003,
          0.48000000000000004,
          0.49,
          0.5,
          0.51,
          0.52,
          0.53,
          0.54,
          0.55,
          0.56,
          0.5700000000000001,
          0.5800000000000001,
          0.59,
          0.6,
          0.61,
          0.62,
          0.63,
          0.64,
          0.65,
          0.66,
          0.67,
          0.68,
          0.6900000000000001,
          0.7000000000000001,
          0.7100000000000001,
          0.72,
          0.73,
          0.74,
          0.75,
          0.76,
          0.77,
          0.78,
          0.79,
          0.8,
          0.81,
          0.8200000000000001,
          0.8300000000000001,
          0.8400000000000001,
          0.85,
          0.86,
          0.87,
          0.88,
          0.89,
          0.9,
          0.91,
          0.92,
          0.93,
          0.9400000000000001,
          0.9500000000000001,
          0.9600000000000001,
          0.97,
          0.98,
          0.99,
          1,
          1.01,
          1.02,
          1.03,
          1.04,
          1.05,
          1.06,
          1.07,
          1.08,
          1.09,
          1.1,
          1.11,
          1.12,
          1.1300000000000001,
          1.1400000000000001,
          1.1500000000000001,
          1.1600000000000001,
          1.17,
          1.18,
          1.19,
          1.2,
          1.21,
          1.22,
          1.23,
          1.24,
          1.25,
          1.26,
          1.27,
          1.28,
          1.29,
          1.3,
          1.31,
          1.32,
          1.33,
          1.34,
          1.35,
          1.36,
          1.37,
          1.3800000000000001,
          1.3900000000000001,
          1.4000000000000001,
          1.4100000000000001,
          1.42,
          1.43,
          1.44,
          1.45,
          1.46,
          1.47,
          1.48,
          1.49,
          1.5,
          1.51,
          1.52,
          1.53,
          1.54,
          1.55,
          1.56,
          1.57,
          1.58,
          1.59,
          1.6,
          1.61,
          1.62,
          1.6300000000000001,
          1.6400000000000001,
          1.6500000000000001,
          1.6600000000000001,
          1.6700000000000002,
          1.68,
          1.69,
          1.7,
          1.71,
          1.72,
          1.73,
          1.74,
          1.75,
          1.76,
          1.77,
          1.78,
          1.79,
          1.8,
          1.81,
          1.82,
          1.83,
          1.84,
          1.85,
          1.86,
          1.87,
          1.8800000000000001,
          1.8900000000000001,
          1.9000000000000001,
          1.9100000000000001,
          1.9200000000000002,
          1.93,
          1.94,
          1.95,
          1.96,
          1.97,
          1.98,
          1.99,
          2,
          2.01,
          2.02,
          2.03,
          2.04,
          2.05,
          2.0599999999999996,
          2.07,
          2.0799999999999996,
          2.09,
          2.0999999999999996,
          2.11,
          2.1199999999999997,
          2.13,
          2.1399999999999997,
          2.15,
          2.1599999999999997,
          2.17,
          2.1799999999999997,
          2.19,
          2.1999999999999997,
          2.21,
          2.2199999999999998,
          2.23,
          2.2399999999999998,
          2.25,
          2.26,
          2.27,
          2.28,
          2.29,
          2.3,
          2.31,
          2.32,
          2.3299999999999996,
          2.34,
          2.3499999999999996,
          2.36,
          2.3699999999999997,
          2.38,
          2.3899999999999997,
          2.4,
          2.4099999999999997,
          2.42,
          2.4299999999999997,
          2.44,
          2.4499999999999997,
          2.46,
          2.4699999999999998,
          2.48,
          2.4899999999999998,
          2.5,
          2.51,
          2.52,
          2.53,
          2.54,
          2.55,
          2.56,
          2.57,
          2.5799999999999996,
          2.59,
          2.5999999999999996,
          2.61,
          2.6199999999999997,
          2.63,
          2.6399999999999997,
          2.65,
          2.6599999999999997,
          2.67,
          2.6799999999999997,
          2.69,
          2.6999999999999997,
          2.71,
          2.7199999999999998,
          2.73,
          2.7399999999999998,
          2.75,
          2.76,
          2.77,
          2.78,
          2.79,
          2.8,
          2.81,
          2.82,
          2.8299999999999996,
          2.84,
          2.8499999999999996,
          2.86,
          2.8699999999999997,
          2.88,
          2.8899999999999997,
          2.9,
          2.9099999999999997,
          2.92,
          2.9299999999999997,
          2.94,
          2.9499999999999997,
          2.96,
          2.9699999999999998,
          2.98,
          2.9899999999999998
         ],
         "y": [
          0.8121212121212121,
          0.8121212121212121,
          0.8131313131313131,
          0.8161616161616162,
          0.8161616161616162,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8151515151515152,
          0.8141414141414142,
          0.8141414141414142,
          0.8131313131313131,
          0.8141414141414142,
          0.8151515151515152,
          0.8141414141414142,
          0.8161616161616162,
          0.8181818181818182,
          0.8191919191919191,
          0.8191919191919191,
          0.8202020202020202,
          0.8202020202020202,
          0.8212121212121212,
          0.8191919191919191,
          0.8191919191919191,
          0.8202020202020202,
          0.8202020202020202,
          0.8191919191919191,
          0.8191919191919191,
          0.8191919191919191,
          0.8191919191919191,
          0.8181818181818182,
          0.8191919191919191,
          0.8181818181818182,
          0.8181818181818182,
          0.8171717171717172,
          0.8181818181818182,
          0.8171717171717172,
          0.8171717171717172,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8181818181818182,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8171717171717172,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8141414141414142,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8161616161616162,
          0.8161616161616162,
          0.8161616161616162,
          0.8161616161616162,
          0.8161616161616162,
          0.8161616161616162,
          0.8161616161616162,
          0.8151515151515152,
          0.8161616161616162,
          0.8141414141414142,
          0.8141414141414142,
          0.8151515151515152,
          0.8151515151515152,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8161616161616162,
          0.8161616161616162,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8151515151515152,
          0.8161616161616162,
          0.8151515151515152,
          0.8151515151515152,
          0.8141414141414142,
          0.8141414141414142,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8151515151515152,
          0.8141414141414142,
          0.8151515151515152,
          0.8141414141414142,
          0.8141414141414142,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8141414141414142,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8141414141414142,
          0.8131313131313131,
          0.8141414141414142,
          0.8141414141414142,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8121212121212121,
          0.8121212121212121,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8131313131313131,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8121212121212121,
          0.8111111111111111,
          0.8111111111111111,
          0.8101010101010101,
          0.8111111111111111,
          0.8111111111111111,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8101010101010101,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111,
          0.8111111111111111
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"bc169e61-f385-4017-90f6-7b72fd672fbc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"bc169e61-f385-4017-90f6-7b72fd672fbc\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'bc169e61-f385-4017-90f6-7b72fd672fbc',\n",
       "                        [{\"type\": \"scatter\", \"x\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.060000000000000005, 0.06999999999999999, 0.08, 0.09, 0.09999999999999999, 0.11, 0.12, 0.13, 0.14, 0.15000000000000002, 0.16, 0.17, 0.18000000000000002, 0.19, 0.2, 0.21000000000000002, 0.22, 0.23, 0.24000000000000002, 0.25, 0.26, 0.27, 0.28, 0.29000000000000004, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36000000000000004, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42000000000000004, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48000000000000004, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.5800000000000001, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.7100000000000001, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.8400000000000001, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.9600000000000001, 0.97, 0.98, 0.99, 1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1, 1.11, 1.12, 1.1300000000000001, 1.1400000000000001, 1.1500000000000001, 1.1600000000000001, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.3800000000000001, 1.3900000000000001, 1.4000000000000001, 1.4100000000000001, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5, 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6, 1.61, 1.62, 1.6300000000000001, 1.6400000000000001, 1.6500000000000001, 1.6600000000000001, 1.6700000000000002, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 1.77, 1.78, 1.79, 1.8, 1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.8800000000000001, 1.8900000000000001, 1.9000000000000001, 1.9100000000000001, 1.9200000000000002, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, 1.99, 2.0, 2.01, 2.02, 2.03, 2.04, 2.05, 2.0599999999999996, 2.07, 2.0799999999999996, 2.09, 2.0999999999999996, 2.11, 2.1199999999999997, 2.13, 2.1399999999999997, 2.15, 2.1599999999999997, 2.17, 2.1799999999999997, 2.19, 2.1999999999999997, 2.21, 2.2199999999999998, 2.23, 2.2399999999999998, 2.25, 2.26, 2.27, 2.28, 2.29, 2.3, 2.31, 2.32, 2.3299999999999996, 2.34, 2.3499999999999996, 2.36, 2.3699999999999997, 2.38, 2.3899999999999997, 2.4, 2.4099999999999997, 2.42, 2.4299999999999997, 2.44, 2.4499999999999997, 2.46, 2.4699999999999998, 2.48, 2.4899999999999998, 2.5, 2.51, 2.52, 2.53, 2.54, 2.55, 2.56, 2.57, 2.5799999999999996, 2.59, 2.5999999999999996, 2.61, 2.6199999999999997, 2.63, 2.6399999999999997, 2.65, 2.6599999999999997, 2.67, 2.6799999999999997, 2.69, 2.6999999999999997, 2.71, 2.7199999999999998, 2.73, 2.7399999999999998, 2.75, 2.76, 2.77, 2.78, 2.79, 2.8, 2.81, 2.82, 2.8299999999999996, 2.84, 2.8499999999999996, 2.86, 2.8699999999999997, 2.88, 2.8899999999999997, 2.9, 2.9099999999999997, 2.92, 2.9299999999999997, 2.94, 2.9499999999999997, 2.96, 2.9699999999999998, 2.98, 2.9899999999999998], \"y\": [0.8121212121212121, 0.8121212121212121, 0.8131313131313131, 0.8161616161616162, 0.8161616161616162, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8151515151515152, 0.8141414141414142, 0.8141414141414142, 0.8131313131313131, 0.8141414141414142, 0.8151515151515152, 0.8141414141414142, 0.8161616161616162, 0.8181818181818182, 0.8191919191919191, 0.8191919191919191, 0.8202020202020202, 0.8202020202020202, 0.8212121212121212, 0.8191919191919191, 0.8191919191919191, 0.8202020202020202, 0.8202020202020202, 0.8191919191919191, 0.8191919191919191, 0.8191919191919191, 0.8191919191919191, 0.8181818181818182, 0.8191919191919191, 0.8181818181818182, 0.8181818181818182, 0.8171717171717172, 0.8181818181818182, 0.8171717171717172, 0.8171717171717172, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8181818181818182, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8171717171717172, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8141414141414142, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8161616161616162, 0.8161616161616162, 0.8161616161616162, 0.8161616161616162, 0.8161616161616162, 0.8161616161616162, 0.8161616161616162, 0.8151515151515152, 0.8161616161616162, 0.8141414141414142, 0.8141414141414142, 0.8151515151515152, 0.8151515151515152, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8161616161616162, 0.8161616161616162, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8151515151515152, 0.8161616161616162, 0.8151515151515152, 0.8151515151515152, 0.8141414141414142, 0.8141414141414142, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8151515151515152, 0.8141414141414142, 0.8151515151515152, 0.8141414141414142, 0.8141414141414142, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8141414141414142, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8141414141414142, 0.8131313131313131, 0.8141414141414142, 0.8141414141414142, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8121212121212121, 0.8121212121212121, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8131313131313131, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8121212121212121, 0.8111111111111111, 0.8111111111111111, 0.8101010101010101, 0.8111111111111111, 0.8111111111111111, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8101010101010101, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111, 0.8111111111111111]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bc169e61-f385-4017-90f6-7b72fd672fbc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8212121212121212"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "cs = []\n",
    "for i in np.arange(0.01, 3, 0.01):\n",
    "    LR = LogisticRegression(penalty='l2',  C=i)\n",
    "    LR.fit(X_train, y_train)\n",
    "    scores.append(LR.score(X_test, y_test))\n",
    "    cs.append(i)\n",
    "    \n",
    "fig = go.Figure(data=go.Scatter(x=cs, y=scores))\n",
    "fig.show()\n",
    "max(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_t, y_t = carga_datos_blending(X, y)\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 1 - BI-LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100 #best 9\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm_bi = model.predict(X_t)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 2 - LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm = model.predict(X_t)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 3 - CONV-1D\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df = pd.DataFrame(pred_lstm)\n",
    "lstm_bi_df = pd.DataFrame(pred_lstm_bi)\n",
    "conv_df = pd.DataFrame(pred_conv)\n",
    "\n",
    "\n",
    "val_df = pd.concat([lstm_df, lstm_bi_df, conv_df],axis=1)\n",
    "\n",
    "\n",
    "#Entrenamiento\n",
    "LR = LogisticRegression(C=1.2, n_jobs=-1)\n",
    "LR.fit(val_df, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y, X_test = carga_datos_test(X, y, t)\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 1 - BI-LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100 #best 9\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm_bi = model.predict(X_test)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 2 - LSTM\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_lstm = model.predict(X_test)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# 3 - CONV-1D\n",
    "#-------------------------------------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending - Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados de las predicciones\n",
    "lstm_df = pd.DataFrame(pred_lstm)\n",
    "lstm_bi_df = pd.DataFrame(pred_lstm_bi)\n",
    "conv_df = pd.DataFrame(pred_conv)\n",
    "\n",
    "val_df = pd.concat([lstm_df, lstm_bi_df, conv_df],axis=1)\n",
    "\n",
    "#Métricas\n",
    "y_pred = LR.predict(val_df)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "resultados(y_pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = carga_datos_train(X,y)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM -> TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "pred_lstm = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_lstm, axis=1)\n",
    "resultados(pred, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED CONVOLUCIONAL 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = carga_datos_test(X, y, t)\n",
    "\n",
    "\n",
    "# Agrego unua red convolucional 1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(64,  return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=VALIDATION_SPLIT,callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                    patience=3, min_delta=0.0001)])\n",
    "\n",
    "pred_conv = model.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "pred = np.argmax(pred_conv, axis=1)\n",
    "resultados(pred, test_df)\n",
    "\n",
    "'''\n",
    "\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "MLP = MLPClassifier(activation='relu', solver='adam',random_state=1)\n",
    "\n",
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "#Predicciones\n",
    "y_pred = MLP.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu: 0.771 (0.016831)\n",
      "Reporte de clasificación: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1109\n",
      "           1       0.79      0.68      0.73       795\n",
      "\n",
      "    accuracy                           0.79      1904\n",
      "   macro avg       0.79      0.78      0.78      1904\n",
      "weighted avg       0.79      0.79      0.79      1904\n",
      "\n",
      "Matriz de confusión: \n",
      " [[970 139]\n",
      " [256 539]]\n",
      "ROC: \n",
      " 0.7763246394564768\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "LGBM = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(LGBM, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accu: %.3f (%.6f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "#Fit\n",
    "LGBM = LGBMClassifier()\n",
    "LGBM.fit(X_train, y_train)\n",
    "\n",
    "#Predicciones\n",
    "y_pred = LGBM.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "#Entrenamiento\n",
    "alg = XGBClassifier(learning_rate=0.1, n_estimators=20, max_depth=5,\n",
    "                    min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,\n",
    "                    objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "alg.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "#Predicciones\n",
    "y_pred = alg.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Random Forest Classifier - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# transform  to a normalized tf-idf representation \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train\n",
    "RFC = RandomForestClassifier(n_estimators=500, max_depth=131, class_weight = 'balanced',\n",
    "                             criterion='entropy', max_features='auto', n_jobs=-1)\n",
    "\n",
    "\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = RFC.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n",
    "#Features importances\n",
    "#features = np.array(X.columns)\n",
    "importances = RFC.feature_importances_\n",
    "\n",
    "'''\n",
    "\n",
    "#Buscando parámetros 'buenos'\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 400],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,5,7,9,11, 13],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Grid Search train\n",
    "GSCV = GridSearchCV(estimator=RFC, param_grid=param_grid, cv= 5)\n",
    "GSCV.fit(X_train, y_train)\n",
    "\n",
    "# Best paramns\n",
    "print(GSCV.best_params_)\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regresión Logística - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "#Entrenamiento\n",
    "\n",
    "LR = LogisticRegression(C=2, n_jobs=-1)\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Métricas\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "# Secuencia [0, 1]\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "resultados(y_pred, test_df)\n",
    "\n",
    "#Feature estimator\n",
    "eli5.show_weights(estimator=LR,feature_names= list(count_vect.get_feature_names()),top=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KNN - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "#Train\n",
    "KNN = KNeighborsClassifier(n_neighbors = 79, metric='minkowski')\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "#Métricas\n",
    "y_pred = KNN.predict(X_test)\n",
    "mostrar_metricas(y_test, y_pred)\n",
    "\n",
    "\n",
    "#Buscando un K 'bueno'\n",
    "k_range = range(1,100, 3)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for k in k_range:\n",
    "    KNN = KNeighborsClassifier(n_neighbors = k, metric='minkowski')\n",
    "    KNN.fit(X_train, y_train)\n",
    "    scores.append(KNN.score(X_test, y_test))\n",
    "   # accuracy = metrics.accuracy_score(y_test, y_pred) #Accuracy\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multinomial NB - TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text_clean']\n",
    "y = train_df['target']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Doc vs Term\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(X_train)\n",
    "X_test = count_vect.transform(X_test)\n",
    "\n",
    "# TF-IDF \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train = tfidf_transformer.fit_transform(X_train)\n",
    "X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = MNB.predict(X_test)\n",
    "\n",
    "#Métricas\n",
    "mostrar_metricas(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
