{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import SpatialDropout1D, Dropout, Conv1D, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D,GlobalMaxPool1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.optimizers import Adam, Adamax, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirdata = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>location_clean</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target keyword_clean location_clean  \\\n",
       "0       1                          NaN   \n",
       "1       1                          NaN   \n",
       "2       1                          NaN   \n",
       "3       1                          NaN   \n",
       "4       1                          NaN   \n",
       "\n",
       "                                          text_clean  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train data\n",
    "train_df = pd.read_csv(dirdata + 'train_fttd.csv')\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>location_clean</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon hanna kills 28 in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0                                  Just happened a terrible car crash   \n",
       "1   2                   Heard about #earthquake is different cities, s...   \n",
       "2   3                   there is a forest fire at spot pond, geese are...   \n",
       "3   9                            Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11                       Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "  keyword_clean location_clean  \\\n",
       "0                          NaN   \n",
       "1                          NaN   \n",
       "2                          NaN   \n",
       "3                          NaN   \n",
       "4                          NaN   \n",
       "\n",
       "                                          text_clean  \n",
       "0                 just happened a terrible car crash  \n",
       "1  heard about earthquake is different cities sta...  \n",
       "2  there is a forest fire at spot pond geese are ...  \n",
       "3              apocalypse lighting spokane wildfires  \n",
       "4         typhoon hanna kills 28 in china and taiwan  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data \n",
    "test_df = pd.read_csv(dirdata + 'test_fttd.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El valor máximo es 34, probé con 32 y 64. No usé más para no agregar ruido.\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones(Tokenizar, encodear, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultados(pred, test_df, algoritmo):\n",
    "    res_df=pd.DataFrame(test_df['id'])\n",
    "    res_df['target']=pred\n",
    "    res_df.to_csv('data/submission' + algoritmo + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bert_encode(texts, tokenizer, max_len = 128):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agragando más capas tarda muuuh o cambio de nada casi.\n",
    "def build_model(bert_layer, max_len = 128):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    x = clf_output\n",
    "    print('Shape: ',clf_output.shape)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    #optimizer = SGD(learning_rate=1e-5)\n",
    "    optimizer = Adamax(learning_rate=LR)\n",
    "    #optimizer = Adam(learning_rate=LR)\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=x)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Intente agregar más capas pero el modelo empieza a empeorar, overtittea.\\nx = Flatten()(x)\\nx = Dense(256, activation = 'relu')(x)\\nx = Dropout(0.5)(x)\\nx = Dense (256, activation = 'relu')(x)\\nx = Dropout(0.5)(x)\\nx = Dense(1, activation = 'sigmoid')(x)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Intente agregar más capas pero el modelo empieza a empeorar, overtittea.\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense (256, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation = 'sigmoid')(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se probaron varios Embbedings pero algunos no se terminaron debido al excesivo consumo de memoria. Con 5 apochs y 10 de batch_size anda bien(37 min).\n",
    "url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1' # Mas lento pero mejor..\n",
    "#url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "#url = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\n",
    "#url = 'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/2'\n",
    "bert_layer = hub.KerasLayer(url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "print('do_lower_case:', do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_df.text_clean.values, tokenizer, max_len=MAX_LEN)\n",
    "test_input = bert_encode(test_df.text_clean.values, tokenizer, max_len=MAX_LEN)\n",
    "train_labels = train_df.target.values\n",
    "#train_labels = pd.get_dummies(train_df.target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la GPU menos 20 de batch_size anda bien. Aumentando a 20 nos quedamos sin memoria. Otra opción es aumentar los Epochs.\n",
    "# Con 3 anda bien. Probé hasta 50 pero no se consigue buenos resultados. Converge rápidamente.\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "#submission_df['target'] = np.argmax(test_pred, axis=1)\n",
    "y_pred = np.round(test_pred).astype(int)\n",
    "\n",
    "#Salida\n",
    "resultados(y_pred, test_df, 'BERT-LOCAL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
